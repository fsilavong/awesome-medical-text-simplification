{
    "results": [
        {
            "title": "Are Mobile Applications in Laryngology Designed for All Patients?",
            "abstract": {
                "Objectives:": "Mobile applications (apps) are multiplying in laryngology, with little standardization of content, functionality, or accessibility. The purpose of this study is to evaluate the quality, functionality, health literacy, readability, accessibility and inclusivity of laryngology mobile applications.",
                "Methods:": "Of the 3,230 apps identified from the Apple and Google Play stores, 28 patient-facing apps met inclusion criteria. Apps were evaluated using validated scales assessing quality and functionality: the Mobile App Rating Scale (MARS) and Institute for Healthcare Informatics App Functionality Scale. The CDC Clear Communication Index, Institute of Medicine Strategies for Creating Health Literate Mobile Applications, and Patient Education Materials Assessment Tool (PEMAT) were used to evaluate apps health literacy level. Readability was assessed using established readability formulas. Apps were evaluated for language, accessibility features and representation of a diverse population.",
                "Results:": "Twenty-six apps (92%) had adequate quality (MARS score >3). The mean PEMAT score was 89% for actionability and 86% for understandability. On average apps utilized 25/33 health literate strategies. Twenty-two apps (79%) did not pass the CDC index threshold of 90% for health literacy. Twenty-four app descriptions (86%) were above an 8th grade reading level. Only 4 apps (14%) showed diverse representation, 3 (11%) had non-English language functions, and 2 (7%) offered subtitles. Inter-rater reliability for MARS was adequate (CA-ICC=0.715).",
                "Conclusion:": "While most apps scored well in quality and functionality, many laryngology apps did not meet standards for health literacy. Most apps were written at a reading level above the national average, lacked accessibility features and did not represent diverse populations."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1002/lary.30465",
                "pmid": "36317789",
                "pmc": "10149562",
                "pub_year": "2022",
                "url": "https://pubmed.ncbi.nlm.nih.gov/36317789/"
            }
        },
        {
            "title": "Development of a Patient Activation Toolkit for Hepatitis C Virus Testing",
            "abstract": {
                "title_content_0": "We evaluated the acceptability of a patient activation toolkit for hepatitis C virus (HCV) testing amidst universal adult guidelines. We developed a patient-facing toolkit that included a letter to the patient from their healthcare provider, HCV factsheet, and question prompt list, which contained questions for their provider about HCV infection and testing. We conducted qualitative interviews with patients ages 18\u201378 (n = 17), using a semi-structured interview guide based on learner verification. We assessed attraction, comprehension, cultural-linguistic acceptability, self-efficacy, and persuasiveness of toolkit materials using direct content analysis. Participants reported materials were attractive, offering suggestions to improve readability. They reported some understanding of materials but requested use of less medical jargon, particularly for the factsheet. Participants discussed cultural acceptability and suggested ways to improve language inclusiveness and comfort with content, given stigma surrounding HCV risk factors. Participants reported that including a letter, factsheet, and QPL improved the persuasiveness of materials, and they conveyed their motivation to be tested for HCV. Results indicate preliminary acceptability for use of the patient activation toolkit, which will be refined based on participants\u2019 recommendations. Overall, this patient activation toolkit holds promise for increasing HCV testing rates."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1007/s13187-022-02209-0",
                "pmid": "35971055",
                "pmc": "10187067",
                "pub_year": "2023",
                "url": "https://pubmed.ncbi.nlm.nih.gov/35971055/"
            }
        },
        {
            "title": "Evaluation of Available Online Information Regarding Treatment for Vitreous Floaters",
            "abstract": {
                "Objective:": "To assess the quality, content, and readability of information available online on vitreous floater information.",
                "Design:": "Cross-sectional study.",
                "Participants:": "Not applicable",
                "Methods:": "Websites were generated using a Google search of \u201cvitreous floaters treatment\u201d and \u201c[State]\u201d and were analyzed using a standardized checklist of 22 questions. Readability was assessed using the Flesch Reading Ease score. Websites met qualification criteria if they represented U.S. based institutions, if they provided clinical care and addressed vitreous floater treatment on their website.",
                "Results:": "Of the 1,065 websites screened, 456 were included. Of these, 406 (89%) were private institutions, 24 (5.3%) were academic, and 26 (5.7%) were a combination of private and academic. The average readability score correlated to a 10th-12th grade reading level. Vitreous floater treatment was discussed on 283 (62.1%) websites and 63 (21.8%) websites discussed potential side effects. Google rank was inversely correlated with depth of explanation (r= \u22120.114, p=.016). Observation was the main treatment recommended (55.8%, n=158), followed by laser treatment (27.6%, n=78), no specific treatment recommendation (11.3%, n=32), and vitrectomy (5.3%, n=15). Centers with vitreoretinal surgeons were 16.43 times more likely to recommend vitrectomy than those without vitreoretinal surgeons (p<0.001).",
                "Conclusions:": "Online information about vitreous floater treatment is variable, and the material is at a higher than recommended reading level for health information. While treatment was discussed by nearly two thirds of websites, less than a quarter mentioned possible complications, and treatment recommendations varied significantly depending on physician training."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1080/08820538.2021.1887898",
                "pmid": "33599190",
                "pmc": "8026748",
                "pub_year": "2021",
                "url": "https://pubmed.ncbi.nlm.nih.gov/33599190/"
            }
        },
        {
            "title": "Machine Scoring of Medical Students\u2019 Written Clinical Reasoning: Initial Validity Evidence",
            "abstract": {
                "Purpose": "Developing medical students\u2019 clinical reasoning requires a structured longitudinal curriculum with frequent targeted assessment and feedback. Performance-based assessments, which have the strongest validity evidence, are currently not feasible for this purpose because they are time-intensive to score. This study explored the potential of using machine learning technologies to score one such assessment\u2014the diagnostic justification essay.",
                "Method": "In May to September 2018, machine scoring algorithms were trained to score a sample of 700 diagnostic justification essays written by 414 third-year medical students from the Southern Illinois University School of Medicine classes of 2012\u20132017. The algorithms applied semantically based natural language processing metrics (e.g., coherence and readability) to assess essay quality on 4 criteria (differential diagnosis, recognition and use of findings, workup, and thought process); the scores for these criteria were summed to create overall scores. Three sources of validity evidence (response process, internal structure, and association with other variables) were examined.",
                "Results": "Machine scores correlated more strongly with faculty ratings than faculty ratings did with each other (machine: .28\u2013.53, faculty: .13\u2013.33) and were less case-specific. Machine scores and faculty ratings were similarly correlated with medical knowledge, clinical cognition, and prior diagnostic justification. Machine scores were more strongly associated with clinical communication than were faculty ratings (.43 vs .31).",
                "Conclusions": "Machine learning technologies may be useful for assessing medical students\u2019 long-form written clinical reasoning. Semantically based machine scoring may capture the communicative aspects of clinical reasoning better than faculty ratings, offering the potential for automated assessment that generalizes to the workplace. These results underscore the potential of machine scoring to capture an aspect of clinical reasoning performance that is difficult to assess with traditional analytic scoring methods. Additional research should investigate machine scoring generalizability and examine its acceptability to trainees and educators."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1097/ACM.0000000000004010",
                "pmid": "33637657",
                "pmc": "8243833",
                "pub_year": "2021",
                "url": "https://pubmed.ncbi.nlm.nih.gov/33637657/"
            }
        },
        {
            "title": "Improved readability and functions needed for mHealth apps targeting patients with heart failure: An app store review",
            "abstract": {
                "title_content_0": "To maintain their quality of life and avoid hospitalization and early mortality, patients with heart failure must recognize and respond to symptoms of exacerbation. A promising method for engaging patients in their self-care is through mobile health applications (mHealth apps). However, for mHealth to have its greatest chance for improving patient outcomes, the app content must be readable, provide useful functions and be based in evidence. The study aimed to determine: (1) readability, (2) types of functions, and (3) linkage to authoritative sources of evidence for self-care focused mHealth apps targeting heart failure patients that are available in the Apple and Google Play Stores. We systematically searched for mHealth apps targeting patients with heart failure in the Apple and Google Play Stores and applied selection criteria. Readability of randomly selected informational paragraphs were determined using Flesch\u2013Kincaid grade level test tool in Microsoft Word. Ten mHealth apps met our criteria. Only one had a reading grade level at or below the recommended 6th grade reading level (average 9.35). The most common functions were tracking, clinical data feedback, and non-data-based reminders and alerts. Only three had statements that clearly linked the mHealth app content to trustworthy, evidence-based sources. Only two had interoperability with the electronic health record and only one had a communication feature with clinicians. Future mHealth designs that are tailored to patients\u2019 literacy level and have advanced functions may hold greater potential for improving patient outcomes."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1002/nur.22078",
                "pmid": "33107056",
                "pmc": "8270757",
                "pub_year": "2021",
                "url": "https://pubmed.ncbi.nlm.nih.gov/33107056/"
            }
        },
        {
            "title": "Assessing the Quality, Content, and Readability of Freely Available Online Information for Patients Regarding Age-Related Macular Degeneration",
            "abstract": {
                "Importance:": "One of the top ten causes of disability in the United States is vision loss, primarily due to age-related eye diseases such as age-related macular degeneration. With an aging population, the number of people affected by this condition is expected to rise. Patients increasingly turn to the internet for health-related information, but no standard exists across published websites.",
                "Objective:": "To assess the quality, content, accountability and readability of information found online for age-related macular degeneration.",
                "Design:": "This cross-sectional study analyzed 12 freely available medical sites with information on age-related macular degeneration and used PubMed as a gold standard for comparison. Thirty-four questions were composed to include information most relevant to patients and each website was independently evaluated by one vitreoretinal surgeon, two vitreoretinal fellows and one ophthalmology resident. Readability was analyzed using an online readability tool. The JAMA benchmarks were used to evaluate the accountability of each site.",
                "Setting:": "Freely available online information was used in this study.",
                "Results:": "The average questionnaire score for all websites was 90.23 (SD 17.56, CI 95% \u00b19.55) out of 136 possible points. There was a significant difference between the content quality of the websites (P=0.01). The mean reading grade for all websites was 11.44 (SD 1.75, CI 95% \u00b10.99). No significant correlation was found between content accuracy and the mean reading grade or Google rank (r=0.392, P=0.207) and r=0.133, P=0.732 respectively). Without including PubMed, only one website achieved the full 4 JAMA benchmarks. There was no correlation between the accuracy of the content of the website and JAMA benchmarks (r=0.344, P=0.273). The interobserver reproducibility was similar among 3 out of 4 observers (r=0.747 between JS and NT, r=0.643 between JS and NP, r=0.686 between NP and NT, r=0.581 between JS and NY; P\u22640.05",
                "Conclusion and Relevance:": "The freely available information online on age-related macular degeneration varies by source but is generally of low quality. The material presented is difficult to interpret and exceeds the recommended reading level for health information. Most websites reviewed did not provide sufficient information using the grading scheme we used to support the patient in making medical decisions."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1080/08820538.2021.1893761",
                "pmid": "33646928",
                "pmc": "8328874",
                "pub_year": "2021",
                "url": "https://pubmed.ncbi.nlm.nih.gov/33646928/"
            }
        },
        {
            "title": "Biennial Analysis of Medication Guide Length and Estimated Readability for New Molecular Entity Drugs, 2011 \u2013 2017",
            "abstract": {
                "Background:": "A medication guide (MG) is a form of FDA-approved labeling that provides patients with information about certain prescribed drugs so that patients can use these drugs safely and effectively. Given ongoing efforts by FDA and industry to continuously improve MG content and format, we hypothesized that more recently approved MGs for new molecular entities (NMEs) would be shorter and more readable compared to NME MGs approved earlier.",
                "Methods:": "We analyzed 53 NMEs MGs that were either approved in 2011 (n = 16), 2013 (n = 9), 2015 (n = 12), or 2017 (n = 16) to determine whether MG page length, word count, and readability scores differed by year. Readability was estimated by Flesch Reading Ease, Flesch-Kincaid Grade Level (FKGL), Fry graph (FRY), and Gunning\u2019s Fog Index (FOG) scores.",
                "Results:": "Mean page length was significantly lower in 2017 than in 2011 and 2013 (ps < .0001). Mean FKGL scores reflected sentences and words found in 8th grade textbooks, but mean FOG and FRY scores were consistent with sentences and words found in 10th and 11th grade textbooks.",
                "Conclusions:": "Although more recent NME MGs were shorter than older NME MGs, additional research is warranted to determine whether shorter MGs lead to improved readability. Developers choosing to estimate MG readability with equations should consider using multiple readability formulas and weigh the strengths and weaknesses of this approach. Using validated tools to more comprehensively assess MG readability should also be considered."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1007/s43441-021-00270-3",
                "pmid": "33970464",
                "pmc": "8338801",
                "pub_year": "2021",
                "url": "https://pubmed.ncbi.nlm.nih.gov/33970464/"
            }
        },
        {
            "title": "The Development and Usability of the AMPREDICT Decision Support Tool: A Mixed Methods Study",
            "abstract": {
                "Objective:": "Amputation level decision making in patients with chronic limb threatening ischaemia is challenging. Currently, evidence relies on published average population risks rather than individual patient risks. The result is significant variation in the distribution of amputation levels across health systems, geographical regions, and time. Clinical decision support has been shown to enhance decision making, especially complex decision making. The goal of this study was to translate the previously validated AMPREDICT prediction models by developing and testing the usability of the AMPREDICT Decision Support Tool (DST), a novel, web based, clinical DST that calculates individual one year post-operative risk of death, re-amputation, and probability of achieving independent mobility by amputation level.",
                "Methods:": "A mixed methods approach was used. Previously validated prediction models were translated into a web based DST with additional content and format developed by an expert panel. Tool usability was assessed using the Post-Study System Usability Questionnaire (PSSUQ; a 16 item scale with scores ranging from 1 to 7, where lower scores indicate greater usability) by 10 clinician end users from diverse specialties, sex, geography, and clinical experience. Think aloud, semi-structured, qualitative interviews evaluated the AMPREDICT DST\u2019s look and feel, user friendliness, readability, functionality, and potential implementation challenges.",
                "Results:": "The PSSUQ overall and subscale scores were favourable, with a mean overall total score of 1.57 (standard deviation [SD] 0.69) and a range from 1.00 to 3.21. The potential clinical utility of the DST included (1) assistance in counselling patients on amputation level decisions, (2) setting outcome expectations, and (3) use as a tool in the academic environment to facilitate understanding of factors that contribute to various outcome risks.",
                "Conclusion:": "After extensive iterative development and testing, the AMPREDICT DST was found to demonstrate strong usability characteristics and clinical relevance. Further evaluation will benefit from integration into an electronic health record with assessment of its impact on physician and patient shared amputation level decision making."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1016/j.ejvs.2021.03.031",
                "pmid": "34088615",
                "pmc": "8376076",
                "pub_year": "2021",
                "url": "https://pubmed.ncbi.nlm.nih.gov/34088615/"
            }
        },
        {
            "title": "What\u2019s in Between the Lines: Assessing the Readability, Understandability, and Actionability in Breast Cancer Survivorship Print Materials",
            "abstract": {
                "title_content_0": "Educational print materials for young women breast cancer survivors (YBCS) are supplemental tools used in patient teaching. However, the readability of the text coupled with how well YBCS understand or act upon the material are rarely explored. The purpose of this study was to assess the readability, understandability, and actionability of commonly distributed breast cancer survivorship print materials. We used an environmental scan approach to obtain a sample of breast cancer survivorship print materials available in outpatient oncology clinics in the central region of a largely rural Southern state. The readability analyses were completed using the Flesch-Kincaid (F-K), Fry Graph Readability Formula (Fry), and Simple Measure of Gobbledygook (SMOG). Understandability and actionability were analyzed using Patient Education Materials Assessment Tool for Printable Materials (PEMAT-P). The environmental scan resulted in a final sample of 14 materials. The mean readability of the majority of survivorship materials was \u201cdifficult,\u201d but the majority scored above the recommended 70% in both understandability and actionability. The importance of understandability and actionability may outweigh readability results in cancer education survivorship material. While reading grade level cannot be dismissed all together, we surmise that patient behavior may hinge more on other factors such as understandability and actionability. Personalized teaching accompanying print material may help YBCS comprehend key messages and promote acting upon specific tasks."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1007/s13187-021-02003-4",
                "pmid": "33822316",
                "pmc": "8492775",
                "pub_year": "2021",
                "url": "https://pubmed.ncbi.nlm.nih.gov/33822316/"
            }
        },
        {
            "title": "SEE-Diabetes, a Patient-Centered Diabetes Self-Management Education and Support for Older Adults: Findings and Information Needs from Patients\u2019 Perspectives",
            "abstract": {
                "Aims:": "This study identified the information needs of people with diabetes aged 65 and older through surveys and focus groups to inform the development of a patient-centered educational decision aid for diabetes care, SEE-Diabetes (Support-Engage-Empower-Diabetes).",
                "Methods:": "We conducted survey (N=37) and three focus groups (N=9). The survey collected demographics, diabetes duration, insulin usage, and clinic notes accessibility through a patient portal. In focus groups, participants evaluated the Assessment and Plan section of three selected deidentified clinic notes to assess readability and helpfulness for diabetes care.",
                "Results:": "The mean age of participants was 66 (24\u201382, SD=12), and 22 were female (60%). The mean diabetes duration was 20.9 years (1\u201363, SD=15). Most participants (80%) read their clinical notes via patient portal. In the focus groups, the readability of clinic notes was noted as a primary concern because of medical abbreviations and poor formatting. Participants found the helpfulness of clinic notes was negatively impacted by vague or insufficient self-care information.",
                "Conclusions:": "We found the high use of patient portal for reading clinic notes, which offers a use case opportunity for the proposed SEE-Diabetes educational aid. Feedback about the readability and helpfulness of clinic notes will be considered during the design process."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1016/j.pcd.2022.02.008",
                "pmid": "35227635",
                "pmc": "9133060",
                "pub_year": "2022",
                "url": "https://pubmed.ncbi.nlm.nih.gov/35227635/"
            }
        },
        {
            "title": "Paragraph-level Simplification of Medical Texts",
            "abstract": {
                "title_content_0": "We consider the problem of learning to simplify medical texts. This is important because most reliable, up-to-date information in biomedicine is dense with jargon and thus practically inaccessible to the lay audience. Furthermore, manual simplification does not scale to the rapidly growing body of biomedical literature, motivating the need for automated approaches. Unfortunately, there are no large-scale resources available for this task. In this work we introduce a new corpus of parallel texts in English comprising technical and lay summaries of all published evidence pertaining to different clinical topics. We then propose a new metric based on likelihood scores from a masked language model pretrained on scientific texts. We show that this automated measure better differentiates between technical and lay summaries than existing heuristics. We introduce and evaluate baseline encoder-decoder Transformer models for simplification and propose a novel augmentation to these in which we explicitly penalize the decoder for producing \u2018jargon\u2019 terms; we find that this yields improvements over baselines in terms of readability."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.18653/v1/2021.naacl-main.395",
                "pmid": "35663507",
                "pmc": "9161242",
                "pub_year": "2022",
                "url": "https://pubmed.ncbi.nlm.nih.gov/35663507/"
            }
        },
        {
            "title": "Evaluating Factuality in Text Simplification",
            "abstract": {
                "title_content_0": "Automated simplification models aim to make input texts more readable. Such methods have the potential to make complex information accessible to a wider audience, e.g., providing access to recent medical literature which might otherwise be impenetrable for a lay reader. However, such models risk introducing errors into automatically simplified texts, for instance by inserting statements unsupported by the corresponding original text, or by omitting key information. Providing more readable but inaccurate versions of texts may in many cases be worse than providing no such access at all. The problem of factual accuracy (and the lack thereof) has received heightened attention in the context of summarization models, but the factuality of automatically simplified texts has not been investigated. We introduce a taxonomy of errors that we use to analyze both references drawn from standard simplification datasets and state-of-the-art model outputs. We find that errors often appear in both that are not captured by existing evaluation metrics, motivating a need for research into ensuring the factual accuracy of automated simplification models."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.18653/v1/2022.acl-long.506",
                "pmid": "36404800",
                "pmc": "9671157",
                "pub_year": "2022",
                "url": "https://pubmed.ncbi.nlm.nih.gov/36404800/"
            }
        },
        {
            "title": "Physicians\u2019 use of plain language during discussions of prostate cancer clinical trials with patients",
            "abstract": {
                "Objective:": "This study described physicians\u2019 use of plain language during patient-physician cancer clinical trial discussions.",
                "Methods:": "Video-recorded clinical interactions and accompanying transcripts were taken from a larger study of communication and clinical trials (PACCT). Interactions (n = 25) were selected if they included invitations to participate in a clinical trial. We used descriptive, qualitative discourse analysis, a method that identifies language patterns at or above the sentence level. We first excerpted discussions of clinical trials, then identified instances of plain language within those discussions. Finally, we inductively coded those instances to describe physicians\u2019 plain language practices.",
                "Results:": "The analysis identified four plain language practices. Lexical simplification replaced medical terminology with simpler words. Patient-centered definition named, categorized, and explained complex medical terminology. Metaphor explained medical terminology by comparing it with known concepts. Finally, experience-focused description replaced medical terminology with descriptions of patients\u2019 potential physical experiences.",
                "Conclusion:": "These plain language practices hold promise as part of effective information exchange in discussions of cancer clinical trials. Testing is needed to identify patient preferences and the extent to which these practices address patient health literacy needs.",
                "Practice Implications:": "Pending further testing, these plain language practices may be integrated into physician clinical trial and other communication training."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1016/j.pec.2022.09.002",
                "pmid": "36085183",
                "pmc": "9675686",
                "pub_year": "2022",
                "url": "https://pubmed.ncbi.nlm.nih.gov/36085183/"
            }
        },
        {
            "title": "Readability and Quality of English and Spanish Online Health Information about Cochlear Implants",
            "abstract": {
                "OBJECTIVE:": "According to the American Medical Association, internet website health information should be written at or below a 6th grade reading level. The purpose of this study was to evaluate the readability and quality of cochlear implant website health information.",
                "STUDY DESIGN:": "Cross-sectional website analysis",
                "SETTING:": "4 internet search engines involving the top 200 websites (English and Spanish)",
                "INTERVENTION/METHODS:": "\u201cCochlear implant\u201d was queried in 4 internet search engines and the top 200 English and Spanish websites were aggregated. After removing duplicates, the websites were evaluated for readability by using the following validated online readability calculators: Flesch Reading Ease Score (FRES) for English websites and the Fernandez-Huerta Formula (FHF) for Spanish websites. Information quality was assessed using the validated DISCERN quality criteria and the presence of Health on the Net Code of Conduct (HONcode) certification.",
                "RESULTS:": "A total of 80 non-industry sponsored (43 English and 37 Spanish) and 11 industry sponsored (4 English and 7 Spanish) cochlear implant health information websites were included in the study. English websites were written at a higher reading level (mean = 50.88, SD = 11.98) compared to Spanish websites (mean = 59.79, SD = 6.04) (p<0.01). For both English and Spanish websites, these scores correlate to the reading level of the average 10th to 12th grade student. Only 12% of Spanish websites and 27% of English websites were HONcode certified. The average DISCERN quality score was 41.67 for English websites and 43.46 for Spanish indicating significant concerns for quality. There was no association found between readability and quality of the websites analyzed.",
                "CONCLUSIONS:": "Patient-directed English and Spanish websites regarding cochlear implantation were written at reading levels that significantly exceed those recommended by the AMA. Furthermore, these websites have significant quality shortcomings. Patients would benefit from more rigorous editing to improve readability and quality of content."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1097/MAO.0000000000003791",
                "pmid": "36728625",
                "pmc": "9928885",
                "pub_year": "2022",
                "url": "https://pubmed.ncbi.nlm.nih.gov/36728625/"
            }
        },
        {
            "title": "Readability of Online Hand and Upper Extremity Patient Resources",
            "abstract": {
                "title_content_0": "Background",
                "title_content_1": "Online patient resources regarding hand and upper extremity topics published by professional societies are written at a level that exceeds that of the average reader.",
                "title_content_2": "Methodology",
                "title_content_3": "Online patient resources focused on hand and upper extremity topics published by the American Society for Surgery of the Hand (ASSH), the American Association for Hand Surgery (AAHS), and the American Academy of Orthopaedic Surgeons (AAOS) were reviewed.\u00a0The reading material from each topic page was analyzed using the Flesch-Kincaid Grade Level (FKGL) and Flesch Reading Ease (FRE) formulas. The reading level (FKGL) of each topic page was compared against an eighth-grade reading level, which corresponds to the average US reading level.",
                "title_content_4": "Results",
                "title_content_5": "A total of 170 online patient resources were reviewed, including 84 from the ASSH, 74 from the AAOS, and 12 from the AAHS. Overall, the mean FKGL was 9.1, and the mean FRE was 57.3. Overall, 50% of all hand and upper extremity online resources were written at or below an eighth-grade reading level. Pairwise testing revealed topic pages written by the ASSH had lower FKGL compared to those written by the AAHS (p = 0.046).",
                "title_content_6": "Conclusions",
                "title_content_7": "Online patient resources focused on hand and upper extremity topics are, on average, written at a level that exceeds the ability of the average reader. Comparisons between organizations showed a statistical, but not clinical, difference in readability measures. An emphasis on improving readability should be maintained as professional organizations continue to develop their online patient resources."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.7759/cureus.36031",
                "pmid": null,
                "pmc": "PMC10085876",
                "pub_year": "2023",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC10085876/"
            }
        },
        {
            "title": "Readability of online health information pertaining to migraine and headache in the UK",
            "abstract": {
                "title_content_0": "An estimated 46% of the worldwide adult population live with an active headache disorder, and it is thought that there is a proportion of headache and migraine sufferers who do not attend for medical care, instead choosing to manage their symptoms at home. The internet continues to act as a source of online health information for self-management, however, it is important that this information can be understood by the user. Research indicates that most health information online is written at a level too difficult for much of the UK population to understand. The aim of this study was to investigate the readability of online health information pertaining to headache and migraine for a UK-based internet user accessing the top four search engines. Searches for \u2018headache\u2019 and \u2018migraine\u2019 were performed on each search engine and results from the first page were selected for analysis. Five validated readability tests were used to analyse readability; Flesch-Kincaid Grade Level, Flesch Reading Ease, Gunning Fog Index, Coleman-Liau Index and Simple Measure of Gobbledygook Index. We found that the majority of online health information about migraine and headache is too difficult for the UK adult population to read. Findings highlight work is required to ensure that information from a wider variety of sources is easier to comprehend for much of the population in order for individuals to make informed decisions about health seeking and self-management of headache and migraine. Health information providers should weave readability analysis into their content design process, incorporating shorter sentences and simpler words in their description of conditions and treatment."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1177/20494637221134461",
                "pmid": null,
                "pmc": "PMC10088424",
                "pub_year": "2022",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC10088424/"
            }
        },
        {
            "title": "Content, Readability, and Accountability of Online Health Information for Patients Regarding Blue Light and Impact on Ocular Health",
            "abstract": {
                "title_content_0": "Objective",
                "title_content_1": "To evaluate the quality and readability of online health content regarding the ocular health effects of blue light.",
                "title_content_2": "Methods",
                "title_content_3": "Five commercial and five non-commercial websites with content regarding the ocular effect of blue light were examined. Quality evaluations were conducted using a 14-question assessment composed by the authors and the 16-question DISCERN instrument. Website accountability was evaluated via the Journal of the American Medical Association (JAMA) benchmarks. Readability was determined using an online tool (Readable). Correlational and comparative analyses were conducted where appropriate.",
                "title_content_4": "Results",
                "title_content_5": "The average questionnaire score was 84 (standard deviation [SD] \u00b1 17.89, 95% confidence interval [CI] 77.32-90.68) out of 136 points (61.8%). Significant differences in quality were identified between websites (p = 0.02), with Healthline achieving the highest score. Compared to commercial websites, non-commercial websites trended toward having significantly higher median questionnaire scores (p = 0.06). Zero websites achieved all four JAMA benchmarks. The average reading grade level of content was 10.43 (SD \u00b1 1.15, 95% CI 9.60 - 11.25), with differences between websites trending toward significance (p = 0.09). There was no correlation between resource readability and quality (\u03c1 = 0.28; p = 0.43) or accountability (\u03c1 = 0.47; p = 0.17).",
                "title_content_6": "Conclusions",
                "title_content_7": "There remain substantial deficiencies in the quality, accountability, and readability of online content concerning the effect of blue light on ocular health. Clinicians and patients must recognize such issues when recommending and consuming these resources."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.7759/cureus.38715",
                "pmid": null,
                "pmc": "PMC10249644",
                "pub_year": "2023",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC10249644/"
            }
        },
        {
            "title": "Google Trends\u2122 and Quality of Information Analyses of Google\u2122 Searches Pertaining to Concussion",
            "abstract": {
                "title_content_0": "Sports-related concussions occur with high incidence in the United States. Google Trends\u2122 (GT) analyses indicate changes of public interest in a topic over time, and can be correlated with incidence of health events such as concussion. Internet searches represent a primary means of patient education for many health topics, including concussion; however, the quality of medical information yielded by internet searches is variable and frequently of an inappropriate reading level. This study therefore aims to describe GT over time and evaluate the quality and readability of information produced by Google\u2122 searches of the term \u201cconcussion.\u201d We identified a strong negative correlation from 2009 to 2016 between GT scores and total number of American high school football participants (R2\u2009=\u20090.8553) and participants per school (R2\u2009=\u20090.9533). Between 2004 and 2020, the monthly GT popularity score were variable (p\u2009=\u20093.193E-08), with September having the greatest scores, correlating with the height of American tackle football season. Applying five validated quality assessment scoring systems at two time points, it was confirmed that different sources yielded varying quality of information. Academic and non-profit healthcare sources demonstrated the highest quality metrics across two time points. There was significant variability of scores among the different scoring systems, however. The majority of searches at both time points yielded information that was rated as \u201cfair\u201d to \u201cpoor\u201d in quality. Applying six readability tests, we revealed that only a single commercial website offered information written at or below the American Medical Association\u2013 recommended 6th-grade level for healthcare information. In summary, GT data analyses suggest that searches correlate with the American tackle football season and increased between 2009 and 2016, given that public interest in concussion increased and annual participation in football decreased. The quality of information yielded by Google\u2122 searches and readability are inadequate, indicating the need for significant improvement."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1089/neur.2022.0084",
                "pmid": null,
                "pmc": "PMC10039270",
                "pub_year": "2023",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC10039270/"
            }
        },
        {
            "title": "Application of artificial intelligence in diagnosis and treatment of colorectal cancer: A novel Prospect",
            "abstract": {
                "title_content_0": "In the past few decades, according to the rapid development of information technology, artificial intelligence (AI) has also made significant progress in the medical field. Colorectal cancer (CRC) is the third most diagnosed cancer worldwide, and its incidence and mortality rates are increasing yearly, especially in developing countries. This article reviews the latest progress in AI in diagnosing and treating CRC based on a systematic collection of previous literature. Most CRCs transform from polyp mutations. The computer-aided detection systems can significantly improve the polyp and adenoma detection rate by early colonoscopy screening, thereby lowering the possibility of mutating into CRC. Machine learning and bioinformatics analysis can help screen and identify more CRC biomarkers to provide the basis for non-invasive screening. The Convolutional neural networks can assist in reading histopathologic tissue images, reducing the experience difference among doctors. Various studies have shown that AI-based high-level auxiliary diagnostic systems can significantly improve the readability of medical images and help clinicians make more accurate diagnostic and therapeutic decisions. Moreover, Robotic surgery systems such as da Vinci have been more and more commonly used to treat CRC patients, according to their precise operating performance. The application of AI in neoadjuvant chemoradiotherapy has further improved the treatment and efficacy evaluation of CRC. In addition, AI represented by deep learning in gene sequencing research offers a new treatment option. All of these things have seen that AI has a promising prospect in the era of precision medicine."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.3389/fmed.2023.1128084",
                "pmid": null,
                "pmc": "PMC10030915",
                "pub_year": "2023",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC10030915/"
            }
        },
        {
            "title": "Determination of the Readability Level of Consent Forms Used in the Gynecology and Obstetrics Clinic at Suleyman Demirel University",
            "abstract": {
                "title_content_0": "Background",
                "title_content_1": "This study aimed to evaluate the readability level of consent forms used for interventional procedures in the obstetrics and gynecology clinic and to determine the readability of the texts according to the education level of the patient.",
                "title_content_2": "Methodology",
                "title_content_3": "This study determined the readability of patient consent forms used before interventional procedures in the gynecology and obstetrics clinic at the Suleyman Demirel University Hospital, Isparta.\u00a0The consent forms were divided into two main groups according to their use in obstetrics and gynecology procedures. The readability level of consent forms was assessed using two readability formulas developed by Ate\u015fman and Bezirci-Y\u0131lmaz, which determine the readability level of Turkish texts in the literature.",
                "title_content_4": "Results",
                "title_content_5": "When the consent forms were analyzed according to Atesman\u2019s readability formula, they were found to be readable with more than 15 years of education at the undergraduate level, while according to Bezirci-Y\u0131lmaz\u2019s readability formula, they were found to be readable with 17 years of education at the postgraduate level.",
                "title_content_6": "Conclusions",
                "title_content_7": "Easy-to-read consent forms will ensure that patients are more informed about interventional procedures and participate more effectively in the treatment process. There is a need to develop readable consent forms suitable for the general education level."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.7759/cureus.37147",
                "pmid": null,
                "pmc": "PMC10074016",
                "pub_year": "2023",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC10074016/"
            }
        },
        {
            "title": "The Readiness of ChatGPT to Write Scientific Case Reports Independently: A Comparative Evaluation Between Human and Artificial Intelligence",
            "abstract": {
                "title_content_0": "The use of artificial intelligence (AI) in scientific publishing has gained increased attention, and one AI tool that has been the subject of much discussion is\u00a0ChatGPT. It is a large language\u00a0model\u00a0(LLM) built on the OpenAI platform that aims to emulate human-like writing and continually improves through user interactions. In this paper, ChatGPT's performance was assessed in medical publishing by comparing it to a case report written by oral and maxillofacial radiologists. ChatGPT was tasked with writing a case report based on a drafted report written by the authors in five different prompts.\u00a0The\u00a0findings of this study highlight issues related to the accuracy, completeness, and readability of the generated text. These results have significant implications for the future use of AI in scientific publishing and suggest\u00a0that in the current iteration of ChatGPT, scientific information must be revised by an expert."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.7759/cureus.39386",
                "pmid": null,
                "pmc": "PMC10292135",
                "pub_year": "2023",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC10292135/"
            }
        },
        {
            "title": "Comprehensiveness of online sources for patient education on hereditary hearing impairment",
            "abstract": {
                "Introduction": "The present study aimed at investigating the readability of online sources on hereditary hearing impairment (HHI).",
                "Methods": "In August 2022, the search terms \u201chereditary hearing impairment\u201d, \u201cgenetic deafness\u201d, hereditary hearing loss\u201d, and \u201csensorineural hearing loss of genetic origin\u201d were entered into the Google search engine and educational materials were determined. The first 50 websites were determined for each search. The double hits were removed and websites with only graphics or tables were excluded. Websites were categorized into either a professional society, a clinical practice or a general health information website. The readability tests to evaluate the websites included: Flesch Reading Ease, Flesch\u2013Kincaid grade level, Gunning\u2013Fog Index, Simple Measure of Gobbledygook, Coleman\u2013Liau Index, Automated Readability Index.",
                "Results": "Twentynine websites were included and categorized as from 4 professional societies, 11 from clinical practices and 14 providing general information. All analyzed websites required higher reading levels than sixth grade. On average 12\u201316 years of education is required to read and understand the websites focused on HHI. Although general health information websites have better readability, the difference was not statistically significant.",
                "Discussion": "The readability scores of every type of online educational materials on HHI are above the recommended level indicating that not all patients and parents can comprehend the information they seek for on these websites."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.3389/fped.2023.1147207",
                "pmid": null,
                "pmc": "PMC10315533",
                "pub_year": "2023",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC10315533/"
            }
        },
        {
            "title": "Assessing Readability of FDA-Required Labeling for Breast Implants",
            "abstract": {
                "title": "Abstract",
                "Goals/Purpose": "In 2021, the United States (US) Food and Drug Administration (FDA) introduced regulations to improve the delivery of breast implant patient education, including new labeling requirements. The new regulations called for a patient decision checklist to be reviewed with a physician ahead of surgery. To ensure educational materials are accessible to most patients, the National Institutes of Health recommend they be written at a sixth to seventh grade level. This study aims to assess the readability of patient-facing FDA labeling for breast implants.",
                "Methods/Technique": "Eleven patient information brochures and seven patient decision checklists were obtained from four breast implant company websites. An example checklist was also obtained from the FDA website. Plain text was extracted from each of the documents. Figures, tables, references, and indices were omitted. The text files were analyzed using four measures of readability validated in clinical materials: Flesch-Kincaid Grade Level, Coleman-Liau Index, Simplified Measure of Gobbledygook (SMOG), and Automated Readability Index (ARI).",
                "Results/Complications": "On average, patient information brochures were 73 pages long (range: 48-92) and included 8 figures or graphics, while patient decision checklists were 5.7 pages long (range: 4-7) and included 0 figures or graphics. Mean readability scores of brochures were a Flesch-Kincaid of 12.7, Coleman-Liau Index of 13.0, SMOG of 14.0, and ARI of 12.7, with an overall mean grade level of 13.1 (13th grade). For decision checklists, results showed a Flesch-Kincaid of 14.2, Coleman-Liau Index of 13.3, SMOG of 15.6, and ARI of 14.8, with an overall grade level of 14.5 (14-15th grade). Comparatively, the sample decision checklist published by the FDA had a Flesch-Kincaid of 13.9, Coleman-Liau Index of 13.0, SMOG of 15.5, and ARI of 14.4, with an overall grade level of 14.2 (14th grade).",
                "Conclusion": "Although patient decision checklists were introduced to improve accessibility of breast implant educational materials, the level at which they are written exceeds the recommendation for the average US adult by several grade levels, suggesting that patients may not fully understand current materials. While shorter than the comprehensive information brochures available, decision checklists were more difficult to read across all readability scales assessed in this study, and neither brochures nor checklists met recommended reading levels on any of the assessed scales. In order to encourage patient understanding of surgical and implant risks and promote the positive health outcomes, the FDA and breast implant companies may consider revising labeling materials."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1093/asjof/ojad027.009",
                "pmid": null,
                "pmc": "PMC10316408",
                "pub_year": "2023",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC10316408/"
            }
        },
        {
            "title": "Using the H2O Automatic Machine Learning Algorithms to Identify Predictors of Web-Based Medical Record Nonuse Among Patients in a Data-Rich Environment: Mixed Methods Study",
            "abstract": {
                "Background": "With the advent of electronic storage of medical records and the internet, patients can access web-based medical records. This has facilitated doctor-patient communication and built trust between them. However, many patients avoid using web-based medical records despite their greater availability and readability.",
                "Objective": "On the basis of demographic and individual behavioral characteristics, this study explores the predictors of web-based medical record nonuse among patients.",
                "Methods": "Data were collected from the National Cancer Institute 2019 to 2020 Health Information National Trends Survey. First, based on the data-rich environment, the chi-square test (categorical variables) and 2-tailed t tests (continuous variables) were performed on the response variables and the variables in the questionnaire. According to the test results, the variables were initially screened, and those that passed the test were selected for subsequent analysis. Second, participants were excluded from the study if any of the initially screened variables were missing. Third, the data obtained were modeled using 5 machine learning algorithms, namely, logistic regression, automatic generalized linear model, automatic random forest, automatic deep neural network, and automatic gradient boosting machine, to identify and investigate factors affecting web-based medical record nonuse. The aforementioned automatic machine learning algorithms were based on the R interface (R Foundation for Statistical Computing) of the H2O (H2O.ai) scalable machine learning platform. Finally, 5-fold cross-validation was adopted for 80% of the data set, which was used as the training data to determine hyperparameters of 5 algorithms, and 20% of the data set was used as the test data for model comparison.",
                "Results": "Among the 9072 respondents, 5409 (59.62%) had no experience using web-based medical records. Using the 5 algorithms, 29 variables were identified as crucial predictors of nonuse of web-based medical records. These 29 variables comprised 6 (21%) sociodemographic variables (age, BMI, race, marital status, education, and income) and 23 (79%) variables related to individual lifestyles and behavioral habits (such as electronic and internet use, individuals\u2019 health status and their level of health concern, etc). H2O\u2019s automatic machine learning methods have a high model accuracy. On the basis of the performance of the validation data set, the optimal model was the automatic random forest with the highest area under the curve in the validation set (88.52%) and the test set (82.87%).",
                "Conclusions": "When monitoring web-based medical record use trends, research should focus on social factors such as age, education, BMI, and marital status, as well as personal lifestyle and behavioral habits, including smoking, use of electronic devices and the internet, patients\u2019 personal health status, and their level of health concern. The use of electronic medical records can be targeted to specific patient groups, allowing more people to benefit from their usefulness."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.2196/41576",
                "pmid": "37335616",
                "pmc": "PMC10337515",
                "pub_year": "2023",
                "url": "https://pubmed.ncbi.nlm.nih.gov/37335616/"
            }
        },
        {
            "title": "ChatGPT's Ability to Assess Quality and Readability of Online Medical Information: Evidence From a Cross-Sectional Study",
            "abstract": {
                "title_content_0": "Introduction",
                "title_content_1": "Artificial Intelligence (AI) platforms have gained widespread attention for their distinct ability to generate automated responses to various prompts. However, its role in assessing the quality and readability of a provided text remains unclear. Thus, the purpose of this study is to evaluate the proficiency of the conversational generative pre-trained transformer (ChatGPT) in utilizing the DISCERN tool to evaluate the quality of online content regarding shock wave therapy for erectile dysfunction.",
                "title_content_2": "Methods",
                "title_content_3": "Websites were generated using a Google search of \u201cshock wave therapy for erectile dysfunction\u201d with location filters disabled. Readability was analyzed using Readable software (Readable.com, Horsham, United Kingdom). Quality was assessed independently by three reviewers using the DISCERN tool. The same plain text files collected were inputted into ChatGPT to determine whether they produced comparable metrics for readability and quality.",
                "title_content_4": "Results",
                "title_content_5": "The study results revealed a notable disparity between ChatGPT's readability assessment and that obtained from a reliable tool, Readable.com (p<0.05). This indicates a lack of alignment between ChatGPT's algorithm and that of established tools, such as Readable.com. Similarly, the DISCERN score generated by ChatGPT differed significantly from the scores generated manually by human evaluators (p<0.05), suggesting that ChatGPT may not be capable of accurately identifying poor-quality information sources regarding shock wave therapy as a treatment for erectile dysfunction.",
                "title_content_6": "Conclusion",
                "title_content_7": "ChatGPT\u2019s evaluation of the quality and readability of online text regarding shockwave therapy for erectile dysfunction differs from that of human raters and trusted tools. Therefore, ChatGPT's current capabilities were not sufficient for reliably assessing the quality and readability of textual content. Further research is needed to elucidate the role of AI in the objective evaluation of online medical content in other fields. Continued development in AI and incorporation of tools such as DISCERN into AI software may enhance the way patients navigate the web in search of high-quality medical content in the future."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.7759/cureus.42214",
                "pmid": null,
                "pmc": "PMC10362474",
                "pub_year": "2023",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC10362474/"
            }
        },
        {
            "title": "Deriving Vocal Fold Oscillation Information from Recorded Voice Signals Using Models of Phonation",
            "abstract": {
                "title_content_0": "During phonation, the vocal folds exhibit a self-sustained oscillatory motion, which is influenced by the physical properties of the speaker\u2019s vocal folds and driven by the balance of bio-mechanical and aerodynamic forces across the glottis. Subtle changes in the speaker\u2019s physical state can affect voice production and alter these oscillatory patterns. Measuring these can be valuable in developing computational tools that analyze voice to infer the speaker\u2019s state. Traditionally, vocal fold oscillations (VFOs) are measured directly using physical devices in clinical settings. In this paper, we propose a novel analysis-by-synthesis approach that allows us to infer the VFOs directly from recorded speech signals on an individualized, speaker-by-speaker basis. The approach, called the ADLES-VFT algorithm, is proposed in the context of a joint model that combines a phonation model (with a glottal flow waveform as the output) and a vocal tract acoustic wave propagation model such that the output of the joint model is an estimated waveform. The ADLES-VFT algorithm is a forward-backward algorithm which minimizes the error between the recorded waveform and the output of this joint model to estimate its parameters. Once estimated, these parameter values are used in conjunction with a phonation model to obtain its solutions. Since the parameters correlate with the physical properties of the vocal folds of the speaker, model solutions obtained using them represent the individualized VFOs for each speaker. The approach is flexible and can be applied to various phonation models. In addition to presenting the methodology, we show how the VFOs can be quantified from a dynamical systems perspective for classification purposes. Mathematical derivations are provided in an appendix for better readability."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.3390/e25071039",
                "pmid": null,
                "pmc": "PMC10378572",
                "pub_year": "2023",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC10378572/"
            }
        },
        {
            "title": "Factors Influencing the Effectiveness of E-Learning in Healthcare: A Fuzzy ANP Study",
            "abstract": {
                "title_content_0": "E-learning has transformed the healthcare education system by providing healthcare professionals with training and development opportunities, regardless of their location. However, healthcare professionals in remote or rural areas face challenges such as limited access to educational resources, lack of reliable internet connectivity, geographical isolation, and limited availability of specialized training programs and instructors. These challenges hinder their access to e-learning opportunities and impede their professional development. To address this issue, a study was conducted to identify the factors that influence the effectiveness of e-learning in healthcare. A literature review was conducted, and two questionnaires were distributed to e-learning experts to assess primary variables and identify the most significant factor. The Fuzzy Analytic Network Process (Fuzzy ANP) was used to identify the importance of selected factors. The study found that success, satisfaction, availability, effectiveness, readability, and engagement are the main components ranked in order of importance. Success was identified as the most significant factor. The study results highlight the benefits of e-learning in healthcare, including increased accessibility, interactivity, flexibility, knowledge management, and cost efficiency. E-learning offers a solution to the challenges of professional development faced by healthcare professionals in remote or rural areas. The study provides insights into the factors that influence the effectiveness of e-learning in healthcare and can guide the development of future e-learning programs."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.3390/healthcare11142035",
                "pmid": null,
                "pmc": "PMC10379776",
                "pub_year": "2023",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC10379776/"
            }
        },
        {
            "title": "Health Information From Web Search Engines and Virtual Assistants About Pre-Exposure Prophylaxis for HIV Prevention in Adolescents and Young Adults: Content Analysis",
            "abstract": {
                "Background": "Adolescents and young adults are disproportionately affected by HIV, suggesting that HIV prevention methods such as pre-exposure prophylaxis (PrEP) should focus on this group as a priority. As digital natives, youth likely turn to internet resources regarding health topics they may not feel comfortable discussing with their medical providers. To optimize informed decision-making by adolescents and young adults most impacted by HIV, the information from internet searches should be educational, accurate, and readable.",
                "Objective": "The aims of this study were to compare the accuracy of web-based PrEP information found using web search engines and virtual assistants, and to assess the readability of the resulting information.",
                "Methods": "Adolescent HIV prevention clinical experts developed a list of 23 prevention-related questions that were posed to search engines (Ask.com, Bing, Google, and Yahoo) and virtual assistants (Amazon Alexa, Microsoft Cortana, Google Assistant, and Apple Siri). The first three results from search engines and virtual assistant web references, as well as virtual assistant verbal responses, were recorded and coded using a six-tier scale to assess the quality of information produced. The results were also entered in a web-based tool determining readability using the Flesch-Kincaid Grade Level scale.",
                "Results": "Google web search engine and Google Assistant more frequently produced PrEP information of higher quality than the other search engines and virtual assistants with scores ranging from 3.4 to 3.7 and 2.8 to 3.3, respectively. Additionally, the resulting information generally was presented in language at a seventh and 10th grade reading level according to the Flesch-Kincaid Grade Level scale.",
                "Conclusions": "Adolescents and young adults are large consumers of technology and may experience discomfort discussing their sexual health with providers. It is important that efforts are made to ensure the information they receive about HIV prevention methods, and PrEP in particular, is comprehensive, comprehensible, and widely available."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.2196/41806",
                "pmid": "37463044",
                "pmc": "PMC10394601",
                "pub_year": "2023",
                "url": "https://pubmed.ncbi.nlm.nih.gov/37463044/"
            }
        },
        {
            "title": "ChatGPT vs Google for Queries Related to Dementia and Other Cognitive Decline: Comparison of Results",
            "abstract": {
                "Background": "People living with dementia or other cognitive decline and their caregivers (PLWD) increasingly rely on the web to find information about their condition and available resources and services. The recent advancements in large language models (LLMs), such as ChatGPT, provide a new alternative to the more traditional web search engines, such as Google.",
                "Objective": "This study compared the quality of the results of ChatGPT and Google for a collection of PLWD-related queries.",
                "Methods": "A set of 30 informational and 30 service delivery (transactional) PLWD-related queries were selected and submitted to both Google and ChatGPT. Three domain experts assessed the results for their currency of information, reliability of the source, objectivity, relevance to the query, and similarity of their response. The readability of the results was also analyzed. Interrater reliability coefficients were calculated for all outcomes.",
                "Results": "Google had superior currency and higher reliability. ChatGPT results were evaluated as more objective. ChatGPT had a significantly higher response relevance, while Google often drew upon sources that were referral services for dementia care or service providers themselves. The readability was low for both platforms, especially for ChatGPT (mean grade level 12.17, SD 1.94) compared to Google (mean grade level 9.86, SD 3.47). The similarity between the content of ChatGPT and Google responses was rated as high for 13 (21.7%) responses, medium for 16 (26.7%) responses, and low for 31 (51.6%) responses.",
                "Conclusions": "Both Google and ChatGPT have strengths and weaknesses. ChatGPT rarely includes the source of a result. Google more often provides a date for and a known reliable source of the response compared to ChatGPT, whereas ChatGPT supplies more relevant responses to queries. The results of ChatGPT may be out of date and often do not specify a validity time stamp. Google sometimes returns results based on commercial entities. The readability scores for both indicate that responses are often not appropriate for persons with low health literacy skills. In the future, the addition of both the source and the date of health-related information and availability in other languages may increase the value of these platforms for both nonmedical and medical professionals."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.2196/48966",
                "pmid": "37490317",
                "pmc": "PMC10410383",
                "pub_year": "2023",
                "url": "https://pubmed.ncbi.nlm.nih.gov/37490317/"
            }
        },
        {
            "title": "Advance Care Planning Website for People With Dementia and Their Family Caregivers: Protocol for a Development and Usability Study",
            "abstract": {
                "Background": "Web-based tools for people with dementia and their family caregivers have considerably increased over the years and offer promising solutions to several unmet needs such as supporting self-care in daily life, facilitating treatment delivery, or ensuring their ability to communicate. The use of web-based tools in the field of advance care planning (ACP) for people with dementia and their family caregivers has yet to be explored and requires careful consideration, given the sensitive topic and the specific needs of people with dementia and their families.",
                "Objective": "This paper reports the protocol for a study aiming to develop and simultaneously test the usability of an ACP website designed for, and with, people with dementia and their families.",
                "Methods": "The development of the website is based on a process map for the development of web-based decision support interventions and on the Medical Research Council framework for complex intervention development and evaluation. Additionally, we apply a user-centered approach in combination with patient and public involvement (PPI) throughout the development process. We describe our iterative development approach to the website. Participants and a PPI group give feedback on 4 prototypes of the ACP website. For each iteration, we aim to include 12 participants (3 people with dementia, 3 family caregivers, and 3 dyads) in usability testing. In the first 3 iterations, usability testing includes (1) a think-aloud exercise, (2) researcher observations, and (3) the System Usability Scale questionnaire. The last iteration of usability testing is composed of a semistructured interview assessing the layout, content, face validity, and readability of the website. Qualitative data from the think-aloud exercises and interviews are analyzed using thematic analysis. Mean scores are calculated for the System Usability Scale questionnaire.",
                "Results": "This study received approval from the Ethical Review Board of Brussels University Hospital of the Vrije Universiteit Brussel. Recruitment began in October 2021. The target date for paper submission of the results of the development and usability testing will be in 2023.",
                "Conclusions": "The methods in this protocol describe a feasible and inclusive approach to the development of an ACP website together with people with dementia, their family caregivers, and other stakeholders. We provide a clear overview of how to combine PPI input and user-centered development methods, leading to a transparent and reliable development process. This protocol might stimulate the active participation of people with dementia, their caregivers, and regional stakeholders in future studies on web-based technologies. The results of this study will be used to refine the design and create a relevant and user-friendly ACP website that is ready to be tested in a larger evaluation study.",
                "International Registered Report Identifier (IRRID)": "DERR1-10.2196/46935"
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.2196/46935",
                "pmid": "37494084",
                "pmc": "PMC10413243",
                "pub_year": "2023",
                "url": "https://pubmed.ncbi.nlm.nih.gov/37494084/"
            }
        },
        {
            "title": "Challenges and Solutions in Implementing eSource Technology for Real-World Studies in China: Qualitative Study Among Different Stakeholders",
            "abstract": {
                "Background": "eSources consist of data that were initially documented in an electronic structure. Typically, an eSource encompasses the direct acquisition, compilation, and retention of electronic information (such as electronic health records [EHRs] or wearable devices), which serves to streamline clinical research. eSources have the potential to enhance the accuracy of data, promote patient safety, and minimize expenses associated with clinical trials. An opinion study published in September 2020 by TransCelerate outlined a road map for the future application of eSource technology and identified 5 key areas of challenges. The background of this study concerns the use of eSource technology in clinical research.",
                "Objective": "The aim of this study was to present challenges and possible solutions for the implementation of eSource technology in real-world studies by summarizing team experiences and lessons learned from an eSource record (ESR) project.",
                "Methods": "After initially developing a simple prototype of the ESR software that can be demonstrated systematically, the researchers conducted in-depth interviews and interacted with different stakeholders to obtain guidance and suggestions. The researchers selected 5 different roles for interviewees: regulatory authorities, pharmaceutical company representatives, hospital information department employees, medical system providers, and clinicians.",
                "Results": "After screening all consultants, the researchers concluded that there were 25 representative consultants. The hospital information department needs to implement many demands from various stakeholders, which makes the existing EHR system unable to meet all the demands of eSources. The emergence of an ESR is intended to divert the burden of the hospital information department from the enormous functional requirements of the outdated EHR system. The entire research process emphasizes multidisciplinary and multibackground expert opinions and considers the complexity of the knowledge backgrounds of personnel involved in the chain of clinical source data collection, processing, quality control, and application in real-world scenarios. To increase the readability of the results, the researchers classified the main results in accordance with the paragraph titles in \u201cUse of Electronic Health Record Data in Clinical Investigations,\u201d a guide released by the US Food and Drug Administration.",
                "Conclusions": "This study introduces the requirement dependencies of different stakeholders and the challenges and recommendations for designing ESR software when implementing eSource technology in China. Experiences based on ESR projects will provide new insights into the disciplines that advance the eSource research field. Future studies should engage patients directly to understand their experiences, concerns, and preferences regarding the implementation of eSource technology. Moreover, involving additional stakeholders, including community health care providers and social workers, will provide valuable insights into the challenges and potential solutions across various health care settings."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.2196/48363",
                "pmid": "37561551",
                "pmc": "PMC10450541",
                "pub_year": "2023",
                "url": "https://pubmed.ncbi.nlm.nih.gov/37561551/"
            }
        },
        {
            "title": "Patient perceptions of the readability and helpfulness of bilingual clinical forms: a survey study",
            "abstract": {
                "Background": "Patients with limited English proficiency (LEP) are rarely provided with translated clinical materials. Typically, healthcare clinics cite high costs of translation and lack of professional translators as barriers to this service. The purpose of the current study was to investigate the perceptions of LEP dental patients regarding the readability, understanding, and helpfulness of translated clinical forms produced by dental student doctor translators.",
                "Methods": "We used a survey design and convenience sampling to recruit LEP patients from a dental school clinic. Participants completed a 9-question (8 Likert-type items and 1 open-ended item) paper survey about translated forms. The bilingual survey was a combination of English and 8 other languages (Arabic, Dari, Pashto, Russian, Spanish, Ukrainian, Urdu, or Vietnamese) and assessed the type of form received; self-reported literacy; design, readability, and helpfulness of the form; overall understanding of the form; understanding of medical and dental terms; helpfulness for patient-provider communication; and comfort level with dental care after receiving the form. Demographic characteristics of participants were collected from the clinic\u2019s electronic health record. Survey responses were analyzed descriptively, and Spearman\u2019s correlation was used to examine the relationship between outcomes.",
                "Results": "Ninety-seven LEP patients (61.9% [60] female, 78.4% [70] Spanish speakers) completed 140 surveys for various translated forms in Dari, Pashto, Spanish, Urdu, or Vietnamese. Participants positively rated translated clinical forms: range, 50.4% (70) for design of the form to 80.0% (112) for comfort level with dental care after receiving the form. For the open-ended item, participants indicated the translations were good, and no improvements were needed. They also thought providing the form was evidence of good customer service. When examining relationships between outcomes, positive correlations were found between self-reported literacy and readability (Spearman r\u2009=\u2009.57, P\u2009<\u2009.001), overall understanding and understanding of medical and dental terms (Spearman r\u2009=\u2009.58, P\u2009<\u2009.001), and type of form and helpfulness for patient-provider communication (Spearman r\u2009=\u2009.26, P\u2009=\u2009.005).",
                "Conclusions": "Study results suggested the translated clinical forms were perceived as helpful and beneficial by LEP dental patients. Similar approaches should be considered to reduce language barriers in healthcare."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1186/s12909-023-04519-3",
                "pmid": "37626348",
                "pmc": "PMC10464213",
                "pub_year": "2023",
                "url": "https://pubmed.ncbi.nlm.nih.gov/37626348/"
            }
        },
        {
            "title": "Assessing Racial and Ethnic Bias in Text Generation for Healthcare-Related Tasks by ChatGPT1",
            "abstract": {
                "title_content_0": "Large Language Models (LLM) are AI tools that can respond human-like to voice or free-text commands without training on specific tasks. However, concerns have been raised about their potential racial bias in healthcare tasks. In this study, ChatGPT was used to generate healthcare-related text for patients with HIV, analyzing data from 100 deidentified electronic health record encounters. Each patient\u2019s data were fed four times with all information remaining the same except for race/ethnicity (African American, Asian, Hispanic White, Non-Hispanic White). The text output was analyzed for sentiment, subjectivity, reading ease, and most used words by race/ethnicity and insurance type. Results showed that instructions for African American, Asian, Hispanic White, and Non-Hispanic White patients had an average polarity of 0.14, 0.14, 0.15, and 0.14, respectively, with an average subjectivity of 0.46 for all races/ethnicities. The differences in polarity and subjectivity across races/ethnicities were not statistically significant. However, there was a statistically significant difference in word frequency across races/ethnicities and a statistically significant difference in subjectivity across insurance types with commercial insurance eliciting the most subjective responses and Medicare and other payer types the lowest. The study suggests that ChatGPT is relatively invariant to race/ethnicity and insurance type in terms of linguistic and readability measures. Further studies are needed to validate these results and assess their implications."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1101/2023.08.28.23294730",
                "pmid": "37693388",
                "pmc": "PMC10491360",
                "pub_year": "2023",
                "url": "https://pubmed.ncbi.nlm.nih.gov/37693388/"
            }
        },
        {
            "title": "Assessing the Readability of Online Patient Education Materials in Obstetrics and Gynecology Using Traditional Measures: Comparative Analysis and Limitations",
            "abstract": {
                "Background": "Patient education materials (PEMs) can be vital sources of information for the general population. However, despite American Medical Association (AMA) and National Institutes of Health (NIH) recommendations to make PEMs easier to read for patients with low health literacy, they often do not adhere to these recommendations. The readability of online PEMs in the obstetrics and gynecology (OB/GYN) field, in particular, has not been thoroughly investigated.",
                "Objective": "The study sampled online OB/GYN PEMs and aimed to examine (1) agreeability across traditional readability measures (TRMs), (2) adherence of online PEMs to AMA and NIH recommendations, and (3) whether the readability level of online PEMs varied by web-based source and medical topic. This study is not a scoping review, rather, it focused on scoring the readability of OB/GYN PEMs using the traditional measures to add empirical evidence to the literature.",
                "Methods": "A total of 1576 online OB/GYN PEMs were collected via 3 major search engines. In total 93 were excluded due to shorter content (less than 100 words), yielding 1483 PEMs for analysis. Each PEM was scored by 4 TRMs, including Flesch-Kincaid grade level, Gunning fog index, Simple Measure of Gobbledygook, and the Dale-Chall. The PEMs were categorized based on publication source and medical topic by 2 research team members. The readability scores of the categories were compared statistically.",
                "Results": "Results indicated that the 4 TRMs did not agree with each other, leading to the use of an averaged readability (composite) score for comparison. The composite scores across all online PEMs were not normally distributed and had a median at the 11th grade. Governmental PEMs were the easiest to read amongst source categorizations and PEMs about menstruation were the most difficult to read. However, the differences in the readability scores among the sources and the topics were small.",
                "Conclusions": "This study found that online OB/GYN PEMs did not meet the AMA and NIH readability recommendations and would be difficult to read and comprehend for patients with low health literacy. Both findings connected well to the literature. This study highlights the need to improve the readability of OB/GYN PEMs to help patients make informed decisions. Research has been done to create more sophisticated readability measures for medical and health documents. Once validated, these tools need to be used by web-based content creators of health education materials."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.2196/46346",
                "pmid": "37647115",
                "pmc": "PMC10500363",
                "pub_year": "2023",
                "url": "https://pubmed.ncbi.nlm.nih.gov/37647115/"
            }
        },
        {
            "title": "From quality to clarity: evaluating the effectiveness of online \u0131nformation related to septic arthritis",
            "abstract": {
                "Background": "The aim of this study was to assess the content, readability, and quality of online resources on septic arthritis, a crucial orthopedic condition necessitating immediate diagnosis and treatment to avert serious complications, with a particular focus on the relevance to individuals from the general public.",
                "Methods": "Two search terms (\u201cseptic arthritis\u201d and \u201cjoint infection\u201d) were input into three different search engines on the Internet (Google, Yahoo, and Bing) and 60 websites were evaluated, with the top 20 results in each search engine. The websites underwent categorization based on their type, and their content and quality were assessed utilizing the DISCERN score, the Journal of the American Medical Association (JAMA) benchmark, the Global Quality Score (GQS), and the Information Value Score (IVS). The readability of the text was assessed through the utilization of the Flesch Kincaid Grade Level (FKGL) and the Flesch Reading Ease Score (FKRS). The presence or absence of the Health on Net (HON) code was evaluated on each website.",
                "Results": "The DISCERN, JAMA, GQS, FKGL, and IVS scores of the academic category were found to be substantially greater when compared with the physician, medical, and commercial categories. But at the same time, academic sites had high readability scores. Websites with HON code had significantly higher average FKGL, FCRS, DISCERN, JAMA, GQS, and IVS scores than those without.",
                "Conclusion": "The quality of websites giving information on septic arthritis was variable and not optimal. Although the content of the academic group was of higher quality, it could be difficult to understand. One of the key responsibilities of healthcare professionals should be to provide high quality and comprehensible information concerning joint infections on reputable academic platforms, thereby facilitating patients in attaining a fundamental level of health literacy."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1186/s13018-023-04181-x",
                "pmid": "37715176",
                "pmc": "PMC10503092",
                "pub_year": "2023",
                "url": "https://pubmed.ncbi.nlm.nih.gov/37715176/"
            }
        },
        {
            "title": "Accessibility and readability of online patient education on cutaneous lymphomas",
            "abstract": {
                "Background": "Patients facing a cutaneous lymphoma diagnosis frequently turn to the internet for information but finding patient-accessible education may be a challenge.",
                "Objective": "To investigate accessibility and readability of patient-oriented online education on cutaneous lymphomas, including cutaneous T-cell and B-cell lymphoma subtypes.",
                "Methods": "This study queried a search engine for 11 cutaneous lymphoma terms, resulting in 1083 webpages. Webpages were screened using defined inclusion/exclusion criteria; literature directed to physicians and scientists was excluded. Webpages were stratified by academic/nonacademic and dermatology/nondermatology hosts and assessed by order of appearance. Readability, including text complexity, was analyzed for grade level understanding using 5 established calculators. Overall readability was assessed by Flesch\u2013Kincaid Reading Ease.",
                "Results": "Academic webpages had earlier order of appearance. There was a dearth in dermatology-hosted webpages. Rarer cutaneous lymphomas yielded fewer patient-accessible results. Search term readability significantly exceeded the American Medical Association\u2013recommended sixth grade level (P\u00a0<\u00a0.001\u2217), with higher grade levels for cutaneous T-cell lymphoma subtype webpages than cutaneous B-cell lymphoma subtypes.",
                "Limitations": "Webpage quality, accuracy, and language were not assessed.",
                "Conclusion": "Current online education for cutaneous lymphomas exceeds the American Medical Association\u2019s maximum readability recommendation. There is a need for more patient-accessible education amidst predominance of scientific literature, greater dermatology host websites, and enhanced readability of existing online education."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1016/j.jdin.2023.07.010",
                "pmid": null,
                "pmc": "PMC10505972",
                "pub_year": "2023",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC10505972/"
            }
        },
        {
            "title": "Does splitting make sentence easier?",
            "abstract": {
                "title_content_0": "In this study, we focus on sentence splitting, a subfield of text simplification, motivated largely by an unproven idea that if you divide a sentence in pieces, it should become easier to understand. Our primary goal in this study is to find out whether this is true. In particular, we ask, does it matter whether we break a sentence into two, three, or more? We report on our findings based on Amazon Mechanical Turk. More specifically, we introduce a Bayesian modeling framework to further investigate to what degree a particular way of splitting the complex sentence affects readability, along with a number of other parameters adopted from diverse perspectives, including clinical linguistics, and cognitive linguistics. The Bayesian modeling experiment provides clear evidence that bisecting the sentence leads to enhanced readability to a degree greater than when we create simplification with more splits."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.3389/frai.2023.1208451",
                "pmid": null,
                "pmc": "PMC10544970",
                "pub_year": "2023",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC10544970/"
            }
        },
        {
            "title": "Assessment of Arabic Web-Based Knowledge About Clear Aligners: An Infodemiologic Study",
            "abstract": {
                "title_content_0": "Background: Orthodontic treatments aim to enhance dental aesthetics, functionality, and long-term oral health. Clear aligners have gained popularity as an aesthetic and convenient option for patients seeking orthodontic correction. However, the quality and readability of online Arabic patient-centered information regarding clear aligners has not been studied yet. The aim of our study is to investigate the quality and readability of Arabic patient-centered information about clear aligners.",
                "title_content_1": "Methods: We conducted an extensive evaluation of Arabic web-based content pertaining to clear aligners using three prominent search engines. Eligible websites were categorized based on specialization, organizational affiliation, material type, and presentation style. We assessed website quality and reliability using the DISCERN instrument, Journal of American Medical Association (JAMA)\u00a0benchmarks, and Health on the Net (HON) code. In addition, we measured readability using the Flesch Reading Ease Score (FRES), Simplified Measure of Gobbledygook (SMOG), and Flesch-Kincaid Grade Level (FKGL).",
                "title_content_2": "Results: Out of 600 search results, 195 websites met the inclusion criteria. None of the websites were HON-code accredited. DISCERN assessments revealed low content quality, with none of the websites achieving high-quality status. The JAMA benchmarks showed limited compliance with the four items, with currency being the most frequently achieved. Readability assessments indicated generally high readability, with FKGL scores suggesting easy comprehension for the average readers.",
                "title_content_3": "Conclusion: While Arabic web-based information on clear aligners is highly readable, its credibility and quality require significant improvement. Websites should adhere to medical information standards, subject content to rigorous assessments, and seek accreditation to ensure reliability. Enhancing the accessibility and comprehensibility of health-related content will empower individuals to make informed health decisions. Addressing limitations, such as social media and video content evaluation, and conducting comparisons with English websites in future research will provide a more comprehensive understanding of the landscape of online orthodontic information."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.7759/cureus.46879",
                "pmid": null,
                "pmc": "PMC10570755",
                "pub_year": "2023",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC10570755/"
            }
        },
        {
            "title": "An mHealth Text Messaging Program Providing Symptom Detection Training and Psychoeducation to Improve Hypoglycemia Self-Management: Intervention Development Study",
            "abstract": {
                "Background": "Hypoglycemia remains a challenge for roughly 25% of people with type 1 diabetes (T1D) despite using advanced technologies such as continuous glucose monitors (CGMs) or automated insulin delivery systems. Factors impacting hypoglycemia self-management behaviors (including reduced ability to detect hypoglycemia symptoms and unhelpful hypoglycemia beliefs) can lead to hypoglycemia development in people with T1D who use advanced diabetes technology.",
                "Objective": "This study aims to develop a scalable, personalized mobile health (mHealth) behavioral intervention program to improve hypoglycemia self-management and ultimately reduce hypoglycemia in people with T1D who use advanced diabetes technology.",
                "Methods": "We (a multidisciplinary team, including clinical and health psychologists, diabetes care and education specialists, endocrinologists, mHealth interventionists and computer engineers, qualitative researchers, and patient partners) jointly developed an mHealth text messaging hypoglycemia behavioral intervention program based on user-centered design principles. The following five iterative steps were taken: (1) conceptualization of hypoglycemia self-management processes and relevant interventions; (2) identification of text message themes and message content development; (3) message revision; (4) patient partner assessments for message readability, language acceptability, and trustworthiness; and (5) message finalization and integration with a CGM data\u2013connected mHealth SMS text message delivery platform. An mHealth web-based SMS text message delivery platform that communicates with a CGM glucose information-sharing platform was also developed.",
                "Results": "The mHealth SMS text messaging hypoglycemia behavioral intervention program HypoPals, directed by patients\u2019 own CGM data, delivers personalized intervention messages to (1) improve hypoglycemia symptom detection and (2) elicit self-reflection, provide fact-based education, and suggest practical health behaviors to address unhelpful hypoglycemia beliefs and promote hypoglycemia self-management. The program is designed to message patients up to 4 times per day over a 10-week period.",
                "Conclusions": "A rigorous conceptual framework, a multidisciplinary team (including patient partners), and behavior change techniques were incorporated to create a scalable, personalized mHealth SMS text messaging behavioral intervention. This program was systematically developed to improve hypoglycemia self-management in advanced diabetes technology users with T1D. A clinical trial is needed to evaluate the program\u2019s efficacy for future clinical implementation."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.2196/50374",
                "pmid": "37788058",
                "pmc": "PMC10582820",
                "pub_year": "2023",
                "url": "https://pubmed.ncbi.nlm.nih.gov/37788058/"
            }
        },
        {
            "title": "Web-Based Information on the Treatment of Dental Hypomineralization",
            "abstract": {
                "title_content_0": "Purpose: To categorise and evaluate the quality and readability of the web-based information about the treatment of the variety of forms of dental hypomineralization.",
                "title_content_1": "Methods: An internet search using two different search terms regarding treating dental hypomineralization was conducted using the Google search engine. The first 100 websites from each search were analysed. Data recorded included DISCERN instrument scores, the Journal of the American Medical Association (JAMA) benchmarks, and the Health on the Net seal (HON). Flesch Reading Ease Scores (FRES), Flesch-Kincaid Grade Level (FKGL), the Simplified Measure of Gobbledygook Index (SMOG), and the Coleman-Liau index were calculated to assess readability.",
                "title_content_2": "Results: A search for \"Treatment of hypomineralized teeth\" on Google yielded 48,500 results. After excluding irrelevant websites, only 25 were evaluated based on affiliation with universities/medical centers, non-profit organizations, commercial entities, or government agencies. The majority of the content was medical facts presented as text and visuals such as images and videos. The study found that the scores for questions about the benefits and risks of treatment were low, while alternative treatments had high scores. Only one website met the HON code criteria, and a minority of websites achieved JAMA benchmarks. The readability ratings varied across different tests used in the study.",
                "title_content_3": "Conclusion: Most websites had university or medical center affiliation but only partially related to the specialty. Two-thirds of websites used images. The online information was inaccurate, poor quality, and hard to read for the average person. Dental professionals should be aware of this information's quality and work to improve it."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.7759/cureus.45840",
                "pmid": "37881377",
                "pmc": "PMC10594141",
                "pub_year": "2023",
                "url": "https://pubmed.ncbi.nlm.nih.gov/37881377/"
            }
        },
        {
            "title": "Quality and readability of web-based information on dental caries in Arabic: an infodemiological study",
            "abstract": {
                "Background": "Web-based information on dental caries in Arabic remains poorly understood. This study aimed to assess the quality and readability of web-based information about dental caries in Arabic.",
                "Methods": "The first 100 websites in Arabic about dental caries were retrieved from Google and Bing using common terms. The websites were classified and evaluated for quality based on the Journal of the American Medical Association (JAMA) benchmark criteria, the DISCERN tool, and the presence of the Health on the Net Foundation Code of Conduct (HONcode). Readability was assessed using online readability indexes.",
                "Results": "A total of 102 Arabic websites were included. The JAMA benchmark score was low (m\u2009=\u20090.36, SD\u2009=\u20090.56), with 67.7% failing to meet any of the JAMA criteria. The DISCERN total score mean was 37.68 (SD\u2009=\u20097.99), with a majority (67.65%) of moderate quality. None of the websites had the HONcode. Readability was generally good, with 52.94% of websites having a Flesch\u2013Kincaid Grade Level (FKGL)\u2009<\u20097, 91.18% having a Simple Measure of Gobbledygook (SMOG)\u2009<\u20097, and 85.29% having a Flesch reading ease (FRE) score\u2009\u2265\u200980. There was a positive correlation between JAMA and DISCERN scores (p\u2009<\u20090.001). DISCERN scores were positively correlated with the number of words (p\u2009<\u20090.001) and sentences (p\u2009=\u20090.004) on the websites. However, JAMA or DISCERN scores were not correlated with FKGL, SMOG, or FRE scores (p\u2009>\u20090.05).",
                "Conclusions": "The quality of Arabic dental caries websites was found to be low, despite their readability. Efforts are needed to introduce more reliable sources for discussing dental caries and treatment options on sites aimed at Arabic populations.",
                "Supplementary Information": "The online version contains supplementary material available at 10.1186/s12903-023-03547-1."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1186/s12903-023-03547-1",
                "pmid": "37880640",
                "pmc": "PMC10601140",
                "pub_year": "2023",
                "url": "https://pubmed.ncbi.nlm.nih.gov/37880640/"
            }
        },
        {
            "title": "Evaluating understandability and actionability of online education materials for home-care patients with COVID-19 in Japan",
            "abstract": {
                "Objective": "In Japan, educational materials on the home care of coronavirus disease 2019 (COVID-19) were developed owing to limited access to medical care during the pandemic. This study quantitatively evaluated the understandability, actionability, natural flow, and readability of 87 materials published by local governments in Japan for patients with COVID-19. Their understandability and actionability were rated using the Japanese version of the Patient Education Material Evaluation Tool for Printed Materials (PEMAT-P). Natural flow and readability were rated using Global Quality Score (GQS) and jReadability, respectively.",
                "Results": "Of the 87 materials, 55 (62.1%) were understandable and 33 (37.9%) were actionable according to the PEMAT-P. Regarding understandability, the materials used medical terms without providing definitions and lacked summaries. Regarding actionability, the materials did not demonstrate explicit steps or utilize visual aids to help the readers take action. The mean (SD) of GQS was 3.44 (0.98), indicating a moderate level of naturalness and comprehensiveness of the materials. The mean (SD) score for readability was 2.4 (0.6), indicating a \u201clower advanced\u201d level. However, challenges regarding the materials\u2019 plain language remained, such as defining medical terms, summarizing the content for understandability, and using charts and tables that encourage patients to act.",
                "Supplementary Information": "The online version contains supplementary material available at 10.1186/s13104-023-06570-1."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1186/s13104-023-06570-1",
                "pmid": "37880802",
                "pmc": "PMC10601193",
                "pub_year": "2023",
                "url": "https://pubmed.ncbi.nlm.nih.gov/37880802/"
            }
        },
        {
            "title": "Evaluating the Quality, Readability, and Activity of Online Information on Brain Arteriovenous Malformations",
            "abstract": {
                "title_content_0": "Introduction",
                "title_content_1": "Brain arteriovenous malformations (AVMs) are vascular deformities created by improper connections between arteries and veins, most commonly in the brain and spinal cord. The management is complex and patient-dependent; further understanding of patient education activities is imperative. Internet access has become more ubiquitous, allowing patients to utilize a large database of medical information online. Using Google Trends (GT) (Google LLC, Mountain View, CA, USA), one can see the public interest in a particular topic over time. Further, when presented with numerous search results, patients may not be able to identify the highest-yielding resources, making objective measures of information quality and readability imperative.",
                "title_content_2": "Methods",
                "title_content_3": "A GT analysis was conducted for \u201chereditary hemorrhagic telangiectasia,\u201d \u201ccerebral aneurysm,\u201d and \u201carteriovenous malformation\u201d.\u00a0These relative search volumes (RSV) were compared with the\u00a02017 to 2019 annual USA AVM diagnosis quantity\u00a0for correlation. These RSVs were also compared with the\u00a02017 to 2019 annual USA deaths due to cerebral hemorrhagic conditions.\u00a0One search was conducted for \u201cbrain arteriovenous malformation\u201d. Since most users looking for health information online use only the first page of sources, the quality and readability analyses were limited to the first page of results on Google search. Five quality tools and six readability formulas were used.",
                "title_content_4": "Results",
                "title_content_5": "Pearson\u2019s correlation coefficients showed positive correlations between USA AVM RSVs and annual AVM deaths per capita from 2017 to 2019 (R2=0.932). The AVM annual diagnosis quantity and AVM RSVs showed a strong positive correlation as well (R2=0.998). Hereditary\u00a0hemorrhagic telangiectasia\u00a0and cerebral aneurysms had strong positive correlations between their RSVs and their corresponding annual diagnoses in the 2017 to 2019 time period (R2=0.982, R2=0.709). One-way ANOVA, for USA\u2019s 2004 to 2021 AVM RSVs and 2004 to 2019 deaths per capita, displayed no month-specific statistically significant repeating pattern (all p>0.483). The DISCERN tool had four websites that qualified as \u201cpoor\u201d and five as \u201cgood.\u201d The average score for the tool was \u201cgood.\u201d The Journal of the American Medical Association (JAMA) benchmark scores were very low on average, as four websites achieved zero points. There was a wide variance in the currency, relevance, authority, accuracy, and purpose (CRAAP) scores, indicating an inconsistent level of webpage reliability across results. The patient education materials assessment tool (PEMAT) understandability (86.6%) showed much higher scores than the PEMAT actionability (54.6%). No readability score averaged at or below the American Medical Association (AMA)-recommended sixth-grade reading level.",
                "title_content_6": "Conclusion",
                "title_content_7": "These GT correlations may be due to patients and families with new diagnoses researching those same conditions online. The seasonality results reflect that no prior research has detected seasonality for AVM diagnosis or presentation. The quality study showed a wide variance in website ethics, treatment information quality, website/author qualifications, and actionable next steps regarding AVMs. Overall, this study showed that patients are routinely attempting to access information regarding these intracranial conditions, but the information available, specifically regarding AVMs, is not routinely reliable and the reading level required to understand them is too high."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.7759/cureus.45984",
                "pmid": null,
                "pmc": "PMC10601510",
                "pub_year": "2023",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC10601510/"
            }
        },
        {
            "title": "Is Online Patient-Centered Information About Implant Bone Graft Valid?",
            "abstract": {
                "title_content_0": "Background",
                "title_content_1": "A dental implant is one of the most commonly used treatments to replace missing teeth. A reasonable number of implant cases necessitate using a bone graft before or at the time of implant placement. This study aims to evaluate the quality and readability of online patient-centered information about implant bone grafts.",
                "title_content_2": "Methodology",
                "title_content_3": "This cross-sectional study used Google, Yahoo, and Bing search engines. The keywords were entered to screen 900 websites. The DISCERN, Journal of the American Medical Association (JAMA), and Health on the Net (HON) code tools evaluated the included websites for quality. The Flesch reading-ease score (FRES), Flesch-Kincaid grade level, and simple measure of gobbledygook tests measured readability. Statistical analysis was done using SPSS version 25 (IBM Corp., Armonk, NY, USA).",
                "title_content_4": "Results",
                "title_content_5": "A total of 161 websites were included; 65 (40.4%) of the included websites belonged to a university or medical center. Only five (3.1%) websites were exclusively related to dental implant treatments. DISCERN showed moderate quality for 82 (50.9%) websites. There was a statistical difference between commercial and non-profit organization websites. In the JAMA evaluation, currency was the most commonly achieved in 67 (41.6%) websites. For the HON code, four (2.5%) websites were certified. Based on FRES, the most common readability category was \u201cfair difficult,\u201d accounting for 64 (39.8%), followed by \u201cstandard\u201d in 56 (34.8%) websites.",
                "title_content_6": "Conclusions",
                "title_content_7": "The study findings suggest that English-language patient-centered information about implant bone grafts is challenging to comprehend and of low quality. Hence, there is a need to establish websites that provide trustworthy, high-quality information on implant bone grafts."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.7759/cureus.46263",
                "pmid": null,
                "pmc": "PMC10615150",
                "pub_year": "2023",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC10615150/"
            }
        },
        {
            "title": "Online Patient Education Resources for Anterior Cruciate Ligament Reconstruction: An Assessment of the Accuracy and Reliability of Information on the Internet Over the Past Decade",
            "abstract": {
                "title_content_0": "Purpose: The purpose of this study is to evaluate the quality of patient education materials accessible through popular online search engines regarding anterior\u00a0cruciate\u00a0ligament (ACL) injuries and anterior\u00a0cruciate\u00a0ligament reconstruction (ACLR).",
                "title_content_1": "Methods: Two search terms (\u201cACL surgery\u201d and \u201cACL reconstruction\u201d) were entered into three search engines (Google, Yahoo, and Bing). The quality of information was scored using a novel scoring system developed and overseen by sports medicine orthopedic clinical research fellows and fellowship-trained orthopedic surgeons. Website quality, credibility, and readability were further assessed by the DISCERN score, Journal of the American Medical Association (JAMA) benchmark criteria, and Flesch-Kincaid Reading Grade Level (FKRGL), respectively. The Health On the Net Code of Conduct (HONcode) certification was also utilized to assess the transparency of health information for each website.",
                "title_content_2": "Results: We evaluated 39 websites. The average score for all websites was 11.2\u00b15.6 out of 28 total points. Six out of the 39 websites (41%) were HONcode\u00a0certified. The websites that contained HONcode certification had a higher average JAMA benchmark score (3.5\u00b10.7) and DISCERN score (44.6\u00b114.7) when compared to the websites without the certification, 2.2\u00b11.2\u00a0and 37.6 \u00b1 15.9\u00a0for JAMA and DISCERN, respectively. The mean JAMA benchmark score was 2.7\u00b11.2 (67.5%) for all websites out of a possible four points. The average FKRGL for all 39 websites was 10.0\u00b12.0 (range: 5.4-13).",
                "title_content_3": "Conclusion: The quality of patient education materials accessible on the internet regarding ACL injuries and ACLR can be misleading and directly impact the patient's decision-making process essential to the patient-physician relationship over the past decade.",
                "title_content_4": "Clinical Relevance: The internet can be a helpful online resource, however, surgeon clarification and consultation with qualified healthcare professionals are strongly recommended prior to clinical decision-making regarding potential treatment options."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.7759/cureus.46599",
                "pmid": null,
                "pmc": "PMC10627413",
                "pub_year": "2023",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC10627413/"
            }
        },
        {
            "title": "How Efficient Is ChatGPT in Accessing Accurate and Quality Health-Related Information?",
            "abstract": {
                "title_content_0": "Background and objective",
                "title_content_1": "The field of artificial intelligence (AI) is advancing at a rapid pace, impacting all aspects of human life. Chat Generative Pre-trained Transformer (ChatGPT), which represents one of AI's most recent and remarkable achievements, has garnered significant attention and popularity in the academic community. ChatGPT, a language model-based chatbot developed by OpenAI, responds quickly and provides answers to the questions put to it. This chatbot has the ability to gather content from a variety of sources\u00a0on the internet. However, its success in providing correct information has not yet been comprehensively analyzed. In light of this, this study aimed to engage in a comparative content analysis of health-related information provided by ChatGPT and a few selected websites.",
                "title_content_2": "Methods",
                "title_content_3": "We performed a qualitative analysis of data obtained from various information sources by using the DISCERN score and the\u00a0Journal of the\u00a0American Medical Association (JAMA) benchmark criteria. In addition, readability levels of the content were measured by using the Flesch-Kincaid grade level, Gunning Fog Index, and\u00a0Simple Measure of Gobbledygook (SMOG) index.",
                "title_content_4": "Results",
                "title_content_5": "Based on our findings, there was no statistically significant difference between the websites and ChatGPT in DISCERN scores. However, the JAMA score was statistically significantly higher for websites. With regard to the Flesch-Kincaid grade level, Gunning Fog Index, and SMOG index values,\u00a0the data obtained from the websites had higher readability.",
                "title_content_6": "Conclusion",
                "title_content_7": "Although AI is starting to play a significant role in our everyday lives, it has yet to surpass traditional methods of accessing information in terms of readability and reliability."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.7759/cureus.46662",
                "pmid": null,
                "pmc": "PMC10628365",
                "pub_year": "2023",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC10628365/"
            }
        },
        {
            "title": "Assessment of Quality and Readability of Online Patient-Centered Arabic Web-Based Knowledge About Apicoectomy",
            "abstract": {
                "title_content_0": "Background",
                "title_content_1": "Endodontic microsurgery (apicectomy) can be considered in cases of persistent infection that is resistant to conventional root canal treatment. The aim of this study was to evaluate the quality and readability of the available online information regarding the apicectomy procedure in Arabic.",
                "title_content_2": "Methods",
                "title_content_3": "Online search on the three most commonly used websites (Google, Yahoo, and Bing) using one keyword. The first 100 websites from each search were analyzed for quality and readability using DISCERN instrument scores, the Journal of the American Medical Association (JAMA) benchmarks, the Health On the Net (HON) seal, Flesch Reading Ease Scores (FRES), Flesch-Kincaid Grade Level (FKGL), and the Simplified Measure of Gobbledygook (SMOG) Index.",
                "title_content_4": "Results",
                "title_content_5": "Searching using the Arabic translation for \"root end resection surgery\" revealed 349,900 websites. Following the inclusion criteria, 31 websites were selected and evaluated in this study. The selected websites belonged to either non-profit organizations or commercial websites. The quality of most of the selected websites received a moderate score (83.9%) using the DISCERN tool. None of the selected websites obtained the HON seal. Quality evaluation using the JAMA benchmarks revealed that currency was the most achieved item (45.2%), followed by authorship (22.6%). Evaluation of the readability of the selected websites using the FRES, FKGL, and SMOG showed that the included websites were considered readable.",
                "title_content_6": "Conclusion",
                "title_content_7": "Although the included websites were readable, the quality of the websites was moderate. There is an urgent need to create more trustworthy and readable websites explaining the different endodontic treatments."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.7759/cureus.48333",
                "pmid": null,
                "pmc": "PMC10629976",
                "pub_year": "2023",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC10629976/"
            }
        },
        {
            "title": "Investigating the Readability and Linguistic, Psychological, and Emotional Characteristics of Digital Dementia Information Written in the English Language: Multitrait-Multimethod Text Analysis",
            "abstract": {
                "Background": "Past research in the Western context found that people with dementia search for digital dementia information in peer-reviewed medical research articles, dementia advocacy and medical organizations, and blogs written by other people with dementia. This past work also demonstrated that people with dementia do not perceive English digital dementia information as emotionally or cognitively accessible.",
                "Objective": "In this study, we sought to investigate the readability; linguistic, psychological, and emotional characteristics; and target audiences of digital dementia information. We conducted a textual analysis of 3 different types of text-based digital dementia information written in English: 300 medical articles, 35 websites, and 50 blogs.",
                "Methods": "We assessed the text\u2019s readability using the Flesch Reading Ease and Flesch-Kincaid Grade Level measurements, as well as tone, analytical thinking, clout, authenticity, and word frequencies using a natural language processing tool, Linguistic Inquiry and Word Count Generator. We also conducted a thematic analysis to categorize the target audiences for each information source and used these categorizations for further statistical analysis.",
                "Results": "The median Flesch-Kincaid Grade Level readability score and Flesch Reading Ease score for all types of information (N=1139) were 12.1 and 38.6, respectively, revealing that the readability scores of all 3 information types were higher than the minimum requirement. We found that medical articles had significantly (P=.05) higher word count and analytical thinking scores as well as significantly lower clout, authenticity, and emotional tone scores than websites and blogs. Further, blogs had significantly (P=.48) higher word count and authenticity scores but lower analytical scores than websites. Using thematic analysis, we found that most of the blogs (156/227, 68.7%) and web pages (399/612, 65.2%) were targeted at people with dementia. Website information targeted at a general audience had significantly lower readability scores. In addition, website information targeted at people with dementia had higher word count and lower emotional tone ratings. The information on websites targeted at caregivers had significantly higher clout and lower authenticity scores.",
                "Conclusions": "Our findings indicate that there is an abundance of digital dementia information written in English that is targeted at people with dementia, but this information is not readable by a general audience. This is problematic considering that people with <12 years of education are at a higher risk of developing dementia. Further, our findings demonstrate that digital dementia information written in English has a negative tone, which may be a contributing factor to the mental health crisis many people with dementia face after receiving a diagnosis. Therefore, we call for content creators to lower readability scores to make the information more accessible to a general audience and to focus their efforts on providing information in a way that does not perpetuate overly negative narratives of dementia."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.2196/48143",
                "pmid": "37878351",
                "pmc": "PMC10632922",
                "pub_year": "2023",
                "url": "https://pubmed.ncbi.nlm.nih.gov/37878351/"
            }
        },
        {
            "title": "Online Patient Education Materials on Iron Deficiency Anemia Are Too Difficult to Read and Low Quality: A Readability and Quality Analysis",
            "abstract": {
                "title_content_0": "Introduction",
                "title_content_1": "Patients increasingly rely on online health information to understand and manage their diseases. Concerns about the quality and readability of these materials have been reported in the literature. Poor quality and difficult-to-read information lead to delayed diagnoses and adverse outcomes. We assessed the quality and readability of online health information about iron deficiency anemia (IDA) on Google.",
                "title_content_2": "Method",
                "title_content_3": "We searched for online web pages using the term \"iron deficiency anemia\"on Google. One hundred and twelve out of 200 web pages were included. We assessed web page typology, readability, the Journal of the American Medical Association (JAMA) score, the DISCERN score, and the Health on the Net Foundation certification (HONcode). Statistical analysis was performed with R version 4.2.2.",
                "title_content_4": "Result",
                "title_content_5": "Non-profit and scientific journal web pages were the most common. Scientific journal web pages were of the highest quality. News web pages were the most readable. The first Google Page web pages did not have greater JAMA scores or lower Flesch-Kinkaid Reading Grade Level (FKGL) and Simple Measure of Gobbledygook (SMOG) scores. Forty-six percent of all web pages were high-quality. Web pages on the first Google page were more likely to have HONCode certification.",
                "title_content_6": "Conclusion",
                "title_content_7": "We highlight gaps in the readability and quality of online information about IDA. Online web pages exceeded the recommended reading level for patients. Most web pages were low quality; only a quarter were HONcode-certified; and the first Google page web pages were not higher in quality than the later web pages on search."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.7759/cureus.46902",
                "pmid": null,
                "pmc": "PMC10638891",
                "pub_year": "2023",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC10638891/"
            }
        },
        {
            "title": "A study of generative large language model for medical research and healthcare",
            "abstract": {
                "title_content_0": "There are enormous enthusiasm and concerns in applying large language models (LLMs) to healthcare. Yet current assumptions are based on general-purpose LLMs such as ChatGPT, which are not developed for medical use. This study develops a generative clinical LLM, GatorTronGPT, using 277 billion words of text including (1) 82 billion words of clinical text from 126 clinical departments and approximately 2 million patients at the University of Florida Health and (2) 195 billion words of diverse general English text. We train GatorTronGPT using a GPT-3 architecture with up to 20 billion parameters and evaluate its utility for biomedical natural language processing (NLP) and healthcare text generation. GatorTronGPT improves biomedical natural language processing. We apply GatorTronGPT to generate 20 billion words of synthetic text. Synthetic NLP models trained using synthetic text generated by GatorTronGPT outperform models trained using real-world clinical text. Physicians\u2019 Turing test using 1 (worst) to 9 (best) scale shows that there are no significant differences in linguistic readability (p\u2009=\u20090.22; 6.57 of GatorTronGPT compared with 6.93 of human) and clinical relevance (p\u2009=\u20090.91; 7.0 of GatorTronGPT compared with 6.97 of human) and that physicians cannot differentiate them (p\u2009<\u20090.001). This study provides insights into the opportunities and challenges of LLMs for medical research and healthcare."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1038/s41746-023-00958-w",
                "pmid": "37973919",
                "pmc": "PMC10654385",
                "pub_year": "2023",
                "url": "https://pubmed.ncbi.nlm.nih.gov/37973919/"
            }
        },
        {
            "title": "Assessing the Readability of Online Health Information for Colonoscopy \u2014 Analysis of Articles in 22 European Languages",
            "abstract": {
                "title_content_0": "Patients often search on the Internet information about different medical conditions and procedures. This study aimed to evaluate online health information on colonoscopy, focusing on quantity and comprehensibility of internet resources dedicated to the colonoscopy. This information could be used by European Union (EU) colorectal cancer (CRC) screening providers to address patient\u2019s unfilled educational needs, fear of colonoscopy, and other barriers that deter from CRC screening. The term \u201ccolonoscopy\u201d translated into 22 official EU languages was searched using the Google search engine. For each translation, generated list of websites was assessed with Google Translate. The first 50 websites in each language were assessed for suitability. Records in other languages were excluded. Included websites were free, focused on patient education, and did not have password. Readability assessments were performed with Lix score. A total of 588 websites in Bulgarian, Croatian, Czech, Danish, Dutch, English, Estonian, Finnish, French, German, Greek, Hungarian, Italian, Latvian, Lithuanian, Polish, Portuguese, Romanian, Slovak, Slovenian, Spanish, and Swedish were evaluated. The overall mean Lix score was 56 \u00b1 8 and was classified as very hard to comprehend. There were significant differences in mean Lix scores across the included languages (P<.001). There was not significant correlation (R2 = 0.1, P = 0.142) between Lix score and number of search hits. Although there was a wealth of online patient information on colonoscopy, the comprehensibility of the available information is low. Physician guidance to reliable resources could increase patient\u2019s willingness to undergo a screening colonoscopy."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1007/s13187-023-02344-2",
                "pmid": "37493981",
                "pmc": "PMC10656333",
                "pub_year": "2023",
                "url": "https://pubmed.ncbi.nlm.nih.gov/37493981/"
            }
        },
        {
            "title": "Evaluating the readability of recruitment materials in veterinary clinical research",
            "abstract": {
                "title": "Abstract",
                "Background": "Owner comprehension is vital to recruitment and study success, but limited information exists regarding the readability of public\u2010facing veterinary clinical trial descriptions.",
                "Objectives": "The current study sought to evaluate the readability of public\u2010facing online veterinary clinical trial descriptions from academic institutions and private referral practices.",
                "Animals": "None.",
                "Methods": "This prospective study assessed readability in a convenience sample of veterinary clinical trial study descriptions using 3 common methods: the Flesch\u2010Kincaid Grade Level (F\u2010K), Flesch Reading Ease Score (FRES), and online Automatic Readability Checker (ARC). Results were compared across specialties and between academic and private institutions.",
                "Results": "Grade level and readability consensus scores (RCSs) were obtained for 61 online clinical trial descriptions at universities (n\u2009=\u200949) and private practices (n\u2009=\u200912). Average grade\u2010level RCS for study descriptions was 14.13 (range, 9\u201021). Using Microsoft Word, the FRES score was higher in descriptions from universities compared to private practices (P\u2009=\u2009.03), and F\u2010K scores were lower in university compared to private practice descriptions (P\u2009=\u2009.03). FRES (P\u2009=\u2009.07), F\u2010K (P\u2009=\u2009.12), and readability consensus (P\u2009=\u2009.17) scores obtained from ARC were not different between institution types. Forty\u2010eight studies (79%) had RCSs over 12, equivalent to reading material at college or graduate school levels.",
                "Conclusions and Clinical Importance": "Similar to other areas in veterinary communication, the evaluated veterinary clinical trial descriptions used for advertising and recruitment far exceeded the recommended 6th\u2010grade reading level for medical information. Readability assessments are straightforward to conduct, and ensuring health literacy should be a customary best practice in veterinary medicine and clinical research."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1111/jvim.16857",
                "pmid": "37759419",
                "pmc": "PMC10658532",
                "pub_year": "2023",
                "url": "https://pubmed.ncbi.nlm.nih.gov/37759419/"
            }
        },
        {
            "title": "Anatomical characterization of pulmonary artery and implications to pulmonary artery pressure monitor implantation",
            "abstract": {
                "title_content_0": "In patients with heart failure, guideline directed medical therapy improves outcomes and requires close patient monitoring. Pulmonary artery pressure monitors permit remote assessment of cardiopulmonary haemodynamics and facilitate early intervention that has been shown to decrease heart failure hospitalization. Pressure sensors implanted in the pulmonary vasculature are stabilized through passive or active interaction with the anatomy and communicate with an external reader to relay invasively measured pressure by radiofrequency. A body mass index \u2009>\u200935\u00a0kg/m2\u00a0and chest circumference\u2009>\u2009165\u00a0cm prevent use due to poor communication. Pulmonary vasculature anatomy is variable between patients and the pulmonary artery size, angulation of vessels and depth of sensor location from the chest wall in heart failure patients who may be candidates for pressure sensors remains largely unexamined. The present study analyses the size, angulation, and depth of the pulmonary artery at the position of implantation of two pulmonary artery pressure sensors: the CardioMEMS sensor typically implanted in the left pulmonary artery and the Cordella sensor implanted in the right pulmonary artery. Thirty-four computed tomography pulmonary angiograms from patients with\u00a0heart failure were analysed using the MIMICS software. Distance from the bifurcation of the pulmonary artery to the implant site was shorter for the right pulmonary artery (4.55\u2009\u00b1\u20090.64\u00a0cm vs. 7.4\u2009\u00b1\u20091.3\u00a0cm) and vessel diameter at the implant site was larger (17.15\u2009\u00b1\u20092.87\u00a0mm vs. 11.83\u2009\u00b1\u20092.30\u00a0mm). Link distance (length of the communication path between sensor and reader) was shorter for the left pulmonary artery (9.40\u2009\u00b1\u20091.43\u00a0mm vs. 12.54\u2009\u00b1\u20091.37\u00a0mm). Therefore, the detailed analysis of pulmonary arterial anatomy using computed tomography pulmonary angiograms may alter the choice of implant location to reduce the risk of sensor migration and improve readability by minimizing sensor-to-reader link distance."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1038/s41598-023-47612-9",
                "pmid": "37993563",
                "pmc": "PMC10665414",
                "pub_year": "2023",
                "url": "https://pubmed.ncbi.nlm.nih.gov/37993563/"
            }
        },
        {
            "title": "The Readability and Quality of Web-Based Patient Information on Nasopharyngeal Carcinoma: Quantitative Content Analysis",
            "abstract": {
                "Background": "Nasopharyngeal carcinoma (NPC) is a rare disease that is strongly associated with exposure to the Epstein-Barr virus and is characterized by the formation of malignant cells in nasopharynx tissues. Early diagnosis of NPC is often difficult owing to the location of initial tumor sites and the nonspecificity of initial symptoms, resulting in a higher frequency of advanced-stage diagnoses and a poorer prognosis. Access to high-quality, readable information could improve the early detection of the disease and provide support to patients during disease management.",
                "Objective": "This study aims to assess the quality and readability of publicly available web-based information in the English language about NPC, using the most popular search engines.",
                "Methods": "Key terms relevant to NPC were searched across 3 of the most popular internet search engines: Google, Yahoo, and Bing. The top 25 results from each search engine were included in the analysis. Websites that contained text written in languages other than English, required paywall access, targeted medical professionals, or included nontext content were excluded. Readability for each website was assessed using the Flesch Reading Ease score and the Flesch-Kincaid grade level. Website quality was assessed using the Journal of the American Medical Association (JAMA) and DISCERN tools as well as the presence of a Health on the Net Foundation seal.",
                "Results": "Overall, 57 suitable websites were included in this study; 26% (15/57) of the websites were academic. The mean JAMA and DISCERN scores of all websites were 2.80 (IQR 3) and 57.60 (IQR 19), respectively, with a median of 3 (IQR 2-4) and 61 (IQR 49-68), respectively. Health care industry websites (n=3) had the highest mean JAMA score of 4 (SD 0). Academic websites (15/57, 26%) had the highest mean DISCERN score of 77.5. The Health on the Net Foundation seal was present on only 1 website, which also achieved a JAMA score of 3 and a DISCERN score of 50. Significant differences were observed between the JAMA score of hospital websites and the scores of industry websites (P=.04), news service websites (P<.048), charity and nongovernmental organization websites (P=.03). Despite being a vital source for patients, general practitioner websites were found to have significantly lower JAMA scores compared with charity websites (P=.05). The overall mean readability scores reflected an average reading age of 14.3 (SD 1.1) years.",
                "Conclusions": "The results of this study suggest an inconsistent and suboptimal quality of information related to NPC on the internet. On average, websites presented readability challenges, as written information about NPC was above the recommended reading level of sixth grade. As such, web-based information requires improvement in both quality and accessibility, and healthcare providers should be selective about information recommended to patients, ensuring they are reliable and readable."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.2196/47762",
                "pmid": "38010802",
                "pmc": "PMC10714271",
                "pub_year": "2023",
                "url": "https://pubmed.ncbi.nlm.nih.gov/38010802/"
            }
        },
        {
            "title": "Toward explainable AI (XAI) for mental health detection based on language behavior",
            "abstract": {
                "title_content_0": "Advances in artificial intelligence (AI) in general and Natural Language Processing (NLP) in particular are paving the new way forward for the automated detection and prediction of mental health disorders among the population. Recent research in this area has prioritized predictive accuracy over model interpretability by relying on deep learning methods. However, prioritizing predictive accuracy over model interpretability can result in a lack of transparency in the decision-making process, which is critical in sensitive applications such as healthcare. There is thus a growing need for explainable AI (XAI) approaches to psychiatric diagnosis and prediction. The main aim of this work is to address a gap by conducting a systematic investigation of XAI approaches in the realm of automatic detection of mental disorders from language behavior leveraging textual data from social media. In pursuit of this aim, we perform extensive experiments to evaluate the balance between accuracy and interpretability across predictive mental health models. More specifically, we build BiLSTM models trained on a comprehensive set of human-interpretable features, encompassing syntactic complexity, lexical sophistication, readability, cohesion, stylistics, as well as topics and sentiment/emotions derived from lexicon-based dictionaries to capture multiple dimensions of language production. We conduct extensive feature ablation experiments to determine the most informative feature groups associated with specific mental health conditions. We juxtapose the performance of these models against a \u201cblack-box\u201d domain-specific pretrained transformer adapted for mental health applications. To enhance the interpretability of the transformers models, we utilize a multi-task fusion learning framework infusing information from two relevant domains (emotion and personality traits). Moreover, we employ two distinct explanation techniques: the local interpretable model-agnostic explanations (LIME) method and a model-specific self-explaining method (AGRAD). These methods allow us to discern the specific categories of words that the information-infused models rely on when generating predictions. Our proposed approaches are evaluated on two public English benchmark datasets, subsuming five mental health conditions (attention-deficit/hyperactivity disorder, anxiety, bipolar disorder, depression and psychological stress)."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.3389/fpsyt.2023.1219479",
                "pmid": "38144474",
                "pmc": "PMC10748510",
                "pub_year": "2023",
                "url": "https://pubmed.ncbi.nlm.nih.gov/38144474/"
            }
        },
        {
            "title": "Quality of Web-Based Sickle Cell Disease Resources for Health Care Transition: Website Content Analysis",
            "abstract": {
                "Background": "Adolescents and young adults with sickle cell disease (SCD) transitioning from pediatric to adult health care face a high-risk period associated with increased use of acute health care services and mortality. Although 59% of American citizens report using the internet for health care information, the quality of web-based, patient-facing resources regarding transition in SCD care has not been evaluated.",
                "Objective": "This study aimed to evaluate the quality and readability of web-based health information on SCD, especially as it pertains to the transition to adulthood for inidividuals with SCD. The study also compared the readability and content scores of websites identified in 2018 to those from 2021 to assess any change in quality over time.",
                "Methods": "Keywords representing phrases adolescents may use while searching for information on the internet regarding transition in SCD care, including \u201chydroxyurea\u201d and \u201cSCD transition,\u201d were identified. A web-based search using the keywords was conducted in July 2021 using Google, Yahoo, and Bing. The top 20 links from each search were collected. Duplicate websites, academic journals, and websites not related to SCD health care transition were excluded. Websites were categorized based on the source: health department, hospital or private clinician, professional society, and other websites. Websites were assessed using Health On the Net Foundation code of conduct (HONcode), Flesch Reading Ease (FRE), Flesch-Kincaid Grade Level (FGL), Ensuring Quality Information for Patients (EQIP), and a novel SCD content checklist (SCDCC). EQIP and SCDCC scores range from 0- to 100. Each website was reviewed by 2 research assistants and assessed for interrater reliability. Descriptive statistics were calculated.",
                "Results": "Of the 900 websites collected, 67 (7.4%) met the inclusion criteria: 13 health department, 7 hospital or private clinician, 33 professional society, and 14 other websites. A total of 15 (22%) out of 67 websites had HONcode certification. Websites with HONcode certification had higher FRE and EQIP scores and lower FGL scores than those without HONcode certification, reflecting greater readability. Websites without HONcode certification had higher SCDCC scores, reflecting greater clinical content. Only 7 (10%) websites met the National Institutes of Health recommendation of a seventh-grade or lower reading level. Based on EQIP scores, 6 (9%) websites were of high quality. The mean SCDCC score was 20.60 (SD 22.14) out of 100. The interrater reliability for EQIP and SCDCC ratings was good (intraclass correlation: 0.718 and 0.897, respectively). No source of website scored significantly higher mean EQIP, FRE, FGL, or SCDCC scores than the others (all P<.05).",
                "Conclusions": "Although seeking health care information on the web is very common, the overall quality of information about transition in SCD care on the internet is poor. Changes to current web-based health care information regarding SCD care transitions would benefit transitioning youth by providing expectations, knowledge, skills, and tools to increase self-efficacy."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.2196/48924",
                "pmid": "38100579",
                "pmc": "PMC10750976",
                "pub_year": "2023",
                "url": "https://pubmed.ncbi.nlm.nih.gov/38100579/"
            }
        },
        {
            "title": "Assessing the readability and quality of online information on benign paroxysmal positional vertigo",
            "abstract": {
                "Introduction": "Benign paroxysmal positional vertigo (BPPV) is the most common cause of peripheral vertigo. It can have a significant impact on quality of life, with individuals often seeking information online for reassurance and education. The aim of this study is to assess the readability and quality of online information on BPPV.",
                "Methods": "The terms \u2018benign paroxysmal positional vertigo\u2019 and \u2018BPPV\u2019 were entered into Google. The first 50 websites generated for each search term were screened. Readability was assessed using the Flesch\u2013Kincaid Reading Ease Score (FRES), Flesch\u2013Kincaid Grade Level (FKGL), Simple Measure of Gobbledygook (SMOG) Index and Gunning Fog Index (GFOG). Quality was assessed using the DISCERN instrument. Spearman\u2019s correlation between quality and readability was calculated.",
                "Results": "A total of 39 websites met the inclusion criteria. The mean and 95% confidence intervals for the FRES, FKGL, SMOG, GFOG and DISCERN scores were 50.2 (46.1\u201354.3), 10.6 (9.87\u201311.4), 10.1 (9.5\u201310.7), 13.6 (12.7\u201314.4) and 36.7 (34.6\u201338.7), respectively. Weak correlation was noted between DISCERN and FRES (rs = \u22120.23, p = 0.17).",
                "Conclusion": "Online information on BPPV is generally of poor quality and low readability. It is essential that healthcare professionals inform their patients of this limitation and advocate for improved online patient education resources that are both high quality and easy to comprehend."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1308/rcsann.2022.0150",
                "pmid": "36748797",
                "pmc": "PMC10757881",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/36748797/"
            }
        },
        {
            "title": "Literacy-related factors and knowledge of patient rights charter: evidence from nurses in selected hospitals in Ghana",
            "abstract": {
                "Background": "Systems of across the world have developed and implemented patient rights policies to protect and improve the provider-patient relationship. The Patient Charter of Ghana was developed in 2002 to improve service quality and protect patients\u2019 rights. However, it is not yet known whether those at the frontline of healthcare delivery can read and understand the contents of the charter. While studies have explored the socioeconomic and institutional level factors related to awareness and knowledge of the Patient Rights Charter, there is a lack of literature on its readability and comprehensibility among nurses. This study assesses nurses\u2019 knowledge of the Patient Rights Charter and associated literacy-related factors.",
                "Method": "An exploratory cross-sectional design and quantitative methods were used to collect data on knowledge, comprehension, and readability of the Patient Rights Charter. 205 nurses from four district hospitals in the Central Region were recruited using proportional and total enumeration sampling. Data were collected using structured questionnaires and were processed using SPSS (version 26) and an online text readability consensus calculator (version 2.0). Descriptive and inferential statistical analyses were performed, and data were presented using simple frequencies, readability statistics, and regression output.",
                "Results": "The results show the charter is written at a higher reading grade level; Flesch-Kincaid Grade Level (13.36), Simple Measure of Gobbledygook (11.57), and Coleman-Liau Readability Index (14.2). The average reading grade level score was 14. The Gunning Fox Index (15.40) and the Flesch Reading Ease Score (34%) show the patient charter is difficult to read and will require at least 14 years of education to be able to read. 87.3% of nurses were able to read and comprehend the charter. Very few (8.3%) read at frustration level. Nurses\u2019 actual comprehension of the charter was the only significant predictor of knowledge of the charter.",
                "Conclusion": "Comprehension of the patient charter is an important predictor of its knowledge. The results emphasize the need to enhance the readability and comprehensibility of the charter for providers. Hospitals can stimulate nurses\u2019 knowledge of the charter by simplifying the charter\u2019s language and deliberately educating nurses on its content.",
                "Supplementary Information": "The online version contains supplementary material available at 10.1186/s12912-024-01739-w."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1186/s12912-024-01739-w",
                "pmid": null,
                "pmc": "PMC10801987",
                "pub_year": "2024",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC10801987/"
            }
        },
        {
            "title": "Improving health literacy of antifungal use\u2014Comparison of the readability of antifungal medicines information from Australia, EU, UK, and US of 16 antifungal agents across 5 classes (allylamines, azoles, echinocandins, polyenes, and others)",
            "abstract": {
                "title": "Abstract",
                "title_content_0": "Adherence to antifungals is poor in high endemic regions where antifungal resistance is high. Poor readability of prescription/over-the-counter (OTC) antifungals may contribute to poor adherence, due to the patient not fully understanding the purpose, importance, and dosage of their antifungal medicine. As there are no reports on the readability of antifungals, this study examined the readability of patient-facing antifungal information. Antifungals (n\u00a0=\u00a016; five classes [allylamines, azoles, echinocandins, polyenes, and others\u2014flucytosine and griseofulvin]) were selected. Readability of four sources of information, (i) summary of product characteristics, (ii) patient information leaflets (PILs), (iii) OTC patient information, and (iv) patient web-based information, was calculated using Readable software, to obtain readability scores [(i) Flesch Reading Ease [FRE], (ii) Flesch\u2013Kinkaid Grade Level [FKGL], (iii) Gunning Fog Index, and (iv) Simple Measure of Gobbledygook (SMOG) Index) and text metrics [word count, sentence count, words/sentence, and syllables/word]. PILs, web-based resources, and OTC patient information had good readability (FRE mean\u00a0\u00b1\u00a0sd\u00a0=\u00a052.8\u00a0\u00b1\u00a06.7, 58.6\u00a0\u00b1\u00a06.9, and 57.3\u00a0\u00b1\u00a07.4, respectively), just falling short of the\u00a0\u2265 60 target. For FKGL (target\u00a0\u2264\u00a08.0), PILs, web-based resources, and OTC patient information also had good readability (mean\u00a0\u00b1\u00a0sd\u00a0=\u00a08.5\u00a0\u00b1\u00a01.0, 7.2\u00a0\u00b1\u00a00.86, and 7.8\u00a0\u00b1\u00a00.1, respectively). Improved readability scores observed correlate with reduced words, words/sentence and syllables/word. Improving readability may lead to improved patient health literacy. Healthcare professionals, academics, and publishers preparing written materials regarding antifungals for the lay/patient community are encouraged to employ readability calculators to check the readability of their work, so that the final material is within recommended readability reference parameters, to support the health literacy of their patients/readers."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1093/mmy/myad084",
                "pmid": "37562942",
                "pmc": "PMC10802897",
                "pub_year": "2023",
                "url": "https://pubmed.ncbi.nlm.nih.gov/37562942/"
            }
        },
        {
            "title": "Comprehensiveness, Accuracy, and Readability of Exercise Recommendations Provided by an AI-Based Chatbot: Mixed Methods Study",
            "abstract": {
                "Background": "Regular physical activity is critical for health and disease prevention. Yet, health care providers and patients face barriers to implement evidence-based lifestyle recommendations. The potential to augment care with the increased availability of artificial intelligence (AI) technologies is limitless; however, the suitability of AI-generated exercise recommendations has yet to be explored.",
                "Objective": "The purpose of this study was to assess the comprehensiveness, accuracy, and readability of individualized exercise recommendations generated by a novel AI chatbot.",
                "Methods": "A coding scheme was developed to score AI-generated exercise recommendations across ten categories informed by gold-standard exercise recommendations, including (1) health condition\u2013specific benefits of exercise, (2) exercise preparticipation health screening, (3) frequency, (4) intensity, (5) time, (6) type, (7) volume, (8) progression, (9) special considerations, and (10) references to the primary literature. The AI chatbot was prompted to provide individualized exercise recommendations for 26 clinical populations using an open-source application programming interface. Two independent reviewers coded AI-generated content for each category and calculated comprehensiveness (%) and factual accuracy (%) on a scale of 0%-100%. Readability was assessed using the Flesch-Kincaid formula. Qualitative analysis identified and categorized themes from AI-generated output.",
                "Results": "AI-generated exercise recommendations were 41.2% (107/260) comprehensive and 90.7% (146/161) accurate, with the majority (8/15, 53%) of inaccuracy related to the need for exercise preparticipation medical clearance. Average readability level of AI-generated exercise recommendations was at the college level (mean 13.7, SD 1.7), with an average Flesch reading ease score of 31.1 (SD 7.7). Several recurring themes and observations of AI-generated output included concern for liability and safety, preference for aerobic exercise, and potential bias and direct discrimination against certain age-based populations and individuals with disabilities.",
                "Conclusions": "There were notable gaps in the comprehensiveness, accuracy, and readability of AI-generated exercise recommendations. Exercise and health care professionals should be aware of these limitations when using and endorsing AI-based technologies as a tool to support lifestyle change involving exercise."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.2196/51308",
                "pmid": "38206661",
                "pmc": "PMC10811574",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/38206661/"
            }
        },
        {
            "title": "Development and Preliminary Validation of an Equine Brief Pain Inventory for Owner Assessment of Chronic Pain Due to Osteoarthritis in Horses",
            "abstract": {
                "Simple Summary": "Horses suffering from joint degeneration (osteoarthritis) often suffer pain and lameness that affect their wellbeing and welfare. Owners or caregivers observe and interact with their horses the most, but as pain and disability can vary over time, it would be valuable to have a tool to help them monitor their horses\u2019 level of osteoarthritis pain. To this end, we designed a questionnaire. Items to be included were determined based on the scientific literature about pain expression and behaviour in horses, as well as input from discussions with owners, caregivers, and veterinary experts. We then tested the tool for ease of use and interpretation in 25 owners/caretakers of arthritic horses. Of these, 88% found the tool useful, and 84% found it easy to use. The tool took less than 5 min to complete, and readability tests showed it could be completed reliably by people with English language reading skills comparable to those with a 6th or 7th grade education. The resulting tool is undergoing further reliability testing in a larger population of owners/caretakers of arthritic and healthy horses before it can be introduced in practice.",
                "Abstract": "An owner-completed questionnaire was designed to monitor the level of chronic pain and impact on quality of life in horses with osteoarthritis (OA). A standardized approach to develop and validate subjective-state scales for clinical use was followed. Scale items were generated through literature review, focus group meetings, and expert panel evaluation. The draft tool was tested for reading level and language ambiguity and piloted in 25 owners/caregivers of horses with osteoarthritis, with factor analysis performed on responses. The resulting revised questionnaire is currently undergoing validation in a larger sample population of 60 OA and 20 sound control horses. In the pilot group, 21 people (84%) found the questionnaire easy to complete and 22 people (88%) found it useful. It could be completed within 5 min by all participants. Readability scores (Flesch Reading Ease Score, Flesch\u2013Kincaid grade level, SMOG index) indicated an English language reading level comparable to that of 6th to 7th grade in the U.S. system (age 11\u201312 years). Cronbach\u2019s alpha of all items in the tool was 0.957, indicating excellent inter-item correlation. Interim analysis for 23 OA horses from the sample population showed good test\u2013retest reliability and higher scores compared to 5 control horses. Full validation must be completed for the instrument to be used in clinical practice."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.3390/ani14020181",
                "pmid": "38254349",
                "pmc": "PMC10812429",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/38254349/"
            }
        },
        {
            "title": "The Evaluation of the Suitability, Quality, and Readability of Publicly Available Online Resources for the Self-Management of Fear of Cancer Recurrence",
            "abstract": {
                "title_content_0": "Cancer survivors often rely on the internet for health information, which has varying levels of readability, suitability, and quality. There is a need for high-quality online self-management resources for cancer survivors with fear of cancer recurrence (FCR). This study evaluated the readability, suitability, and quality of publicly available online FCR self-management resources. A Google search using FCR-related keywords identified freely available FCR self-management resources for cancer survivors in English. Resource readability (reading grade level), suitability, and quality were evaluated using relevant assessment tools. Descriptive statistics and cluster analysis identified resources with higher suitability and quality scores. Mean resource (n = 23) readability score was grade 11 (SD = 1.6, Range = 9\u201314). The mean suitability score was 56.0% (SD = 11.4%, Range = 31.0\u201376.3%), indicating average suitability and the mean quality score was 53% (SD = 11.7%, Range = 27\u201380%), indicating fair quality. A cluster of 15 (65%) resources with higher suitability and quality scores was identified. There were no significant associations between suitability or quality scores and the type of organisation that published the resources. Online FCR self-management resources varied in readability, suitability and quality. Resources with higher quality and suitability scores relative to other resources are identified for use by healthcare professionals and cancer survivors. Resources that are more culturally appropriate, with lower reading grade levels and detailed self-management strategies are needed."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.3390/curroncol31010005",
                "pmid": "38248090",
                "pmc": "PMC10814354",
                "pub_year": "2023",
                "url": "https://pubmed.ncbi.nlm.nih.gov/38248090/"
            }
        },
        {
            "title": "Accuracy and comprehensibility of chat-based artificial intelligence for patient information on atrial fibrillation and cardiac implantable electronic devices",
            "abstract": {
                "title": "Abstract",
                "Aims": "Natural language processing chatbots (NLPC) can be used to gather information for medical content. However, these tools contain a potential risk of misinformation. This study aims to evaluate different aspects of responses given by different NLPCs on questions about atrial fibrillation (AF) and clinical implantable electronic devices (CIED).",
                "Methods and results": "Questions were entered into three different NLPC interfaces. Responses were evaluated with regard to appropriateness, comprehensibility, appearance of confabulation, absence of relevant content, and recommendations given for clinically relevant decisions. Moreover, readability was assessed by calculating word count and Flesch Reading Ease score. 52, 60, and 84% of responses on AF and 16, 72, and 88% on CIEDs were evaluated to be appropriate for all responses given by Google Bard, (GB) Bing Chat (BC) and ChatGPT Plus (CGP), respectively. Assessment of comprehensibility showed that 96, 88, and 92% of responses on AF and 92 and 88%, and 100% on CIEDs were comprehensible for all responses created by GB, BC, and CGP, respectively. Readability varied between different NLPCs. Relevant aspects were missing in 52% (GB), 60% (BC), and 24% (CGP) for AF, and in 92% (GB), 88% (BC), and 52% (CGP) for CIEDs.",
                "Conclusion": "Responses generated by an NLPC are mostly easy to understand with varying readability between the different NLPCs. The appropriateness of responses is limited and varies between different NLPCs. Important aspects are often missed to be mentioned. Thus, chatbots should be used with caution to gather medical information about cardiac arrhythmias and devices."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1093/europace/euad369",
                "pmid": "38127304",
                "pmc": "PMC10824484",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/38127304/"
            }
        },
        {
            "title": "Assessing the readability and quality of online information on anosmia",
            "abstract": {
                "Introduction": "Anosmia can have a significant impact on well-being and quality of life. Due to an ageing population and the coronavirus disease 2019, increasing numbers of patients are seeking online information on anosmia. This report systematically assesses the readability and quality of online information on anosmia.",
                "Methods": "The terms \u2018anosmia\u2019 and \u2018loss of smell\u2019 were entered into Google. The first 50 websites generated for each search term were screened. Readability was assessed using the Flesch\u2013Kincaid Reading Ease Score (FRES), Flesch\u2013Kincaid Grade Level (FKGL), Simple Measure of Gobbledygook (SMOG) Index and Gunning Fog Index (GFI). Quality was assessed using the DISCERN instrument. Spearman\u2019s correlation between quality and readability was calculated.",
                "Results": "A total of 79 websites met the inclusion criteria. The mean and 95% confidence interval for the FRES, FKGL, SMOG, GFI and DISCERN scores were 46.31 (42.94\u201349.68), 12.00 (11.27\u201312.73), 10.70 (10.16\u201311.23), 14.62 (13.85\u201315.39) and 2.90 (2.69\u20133.11), respectively. Significant negative correlation was noted between the DISCERN and FRES (rs=\u22120.500; p<0.05).",
                "Discussion": "Online information on anosmia is written above the recommended reading age guidance in the UK, and has moderate deficiencies in quality. As a result, the information may be used inappropriately and could result in worse health outcomes. We recommend that patients are directed to websites produced by health providers or nonprofit organisations that develop material for patient health education.",
                "Conclusions": "Online information on anosmia is of low readability and moderate quality. Healthcare professionals should direct patients towards high-quality resources written for the layperson."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1308/rcsann.2022.0147",
                "pmid": "37051757",
                "pmc": "PMC10830341",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/37051757/"
            }
        },
        {
            "title": "Comparison of large language models in management advice for melanoma: Google's AI BARD, BingAI and ChatGPT",
            "abstract": {
                "title": "Abstract",
                "title_content_0": "Large language models (LLMs) are emerging artificial intelligence (AI) technology refining research and healthcare. Their use in medicine has seen numerous recent applications. One area where LLMs have shown particular promise is in the provision of medical information and guidance to practitioners. This study aims to assess three prominent LLMs\u2014Google's AI BARD, BingAI and ChatGPT\u20104 in providing management advice for melanoma by comparing their responses to current clinical guidelines and existing literature. Five questions on melanoma pathology were prompted to three LLMs. A panel of three experienced Board\u2010certified plastic surgeons evaluated the responses for reliability using reliability matrix (Flesch Reading Ease Score, the Flesch\u2010Kincaid Grade Level and the Coleman\u2010Liau Index), suitability (modified DISCERN score) and comparing them to existing guidelines. t\u2010Test was performed to calculate differences in mean readability and reliability scores between LLMs and p value\u00a0<0.05 was considered statistically significant. The mean readability scores across three LLMs were same. ChatGPT exhibited superiority with a Flesch Reading Ease Score of 35.42 (\u00b121.02), Flesch\u2013Kincaid Grade Level of 11.98 (\u00b14.49) and Coleman\u2013Liau Index of 12.00 (\u00b15.10), however all of these were insignificant (p\u00a0>\u00a00.05). Suitability\u2010wise using DISCERN score, ChatGPT 58 (\u00b16.44) significantly (p\u00a0=\u00a00.04) outperformed BARD 36.2 (\u00b134.06) and was insignificant to BingAI's 49.8 (\u00b122.28). This study demonstrates that ChatGPT marginally outperforms BARD and BingAI in providing reliable, evidence\u2010based clinical advice, but they still face limitations in depth and specificity. Future research should improve LLM performance by integrating specialized databases and expert knowledge to support patient\u2010centred care."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1002/ski2.313",
                "pmid": null,
                "pmc": "PMC10831541",
                "pub_year": "2023",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC10831541/"
            }
        },
        {
            "title": "Validity and reliability of the English version of the Diabetic Foot Self-Care Questionnaire: a cross-cultural adaptation",
            "abstract": {
                "Introduction": "The objective of this study was to carry out the cross-cultural adaptation and validation of the Diabetic Foot Self-Care Questionnaire into the English language, broadening the applicability of this patient-reported outcome measure and improving the monitoring of patients with diabetic foot disease.",
                "Methods": "The validation study into English was conducted in two phases: cross-cultural adaptation and psychometric validation study. Short Form-12 Version 2, EuroQoL-5D and Foot Function Index were used to analyze the criterion validity. Item response, internal consistency, standard error of measurement, minimal detectable change and construct validity were calculated in the validation phase.",
                "Results": "An English version of the questionnaire (DFSQ-UMA-En) was successfully obtained. A total of n\u2009=\u2009193 participants were tested to confirm the validity and reliability of the questionnaire. Internal consistency values ranged from very good to excellent (Cronbach\u2019s \u03b1 =0.889\u20130.981), and reliability was excellent (ICC\u2009=\u20090.854\u20130.959). Standard error measurement value was =2.543. Criterion validity ranged from r\u2009=\u20090.429 to r\u2009=\u20090.844. For construct validity, Kaiser-Meyer-Olkin test was =0.752.",
                "Conclusion": "DFSQ-UMA-En is a valid and reliable tool with good readability and comprehension features. This questionnaire addresses foot self-care behaviors in patients with diabetic foot disease, standing out as essential for early diagnosis and prevention strategies in clinical and research settings."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.3389/fpubh.2023.1326439",
                "pmid": "38332943",
                "pmc": "PMC10851747",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/38332943/"
            }
        },
        {
            "title": "Artificial Intelligence Language Model Performance for Rapid Intraoperative Queries in Plastic Surgery: ChatGPT and the Deep Inferior Epigastric Perforator Flap",
            "abstract": {
                "title_content_0": "Background: The integration of artificial intelligence in healthcare has led to the development of large language models that can address various medical queries, including intraoperatively. This study investigates the potential of ChatGPT in addressing intraoperative questions during the deep inferior epigastric perforator flap procedure. Methods: A series of six intraoperative questions specific to the DIEP flap procedure, derived from real-world clinical scenarios, were proposed to ChatGPT. A panel of four experienced board-certified plastic surgeons evaluated ChatGPT\u2019s performance in providing accurate, relevant, and comprehensible responses. Results: The Likert scale demonstrated to be medically accurate, systematic in presentation, and logical when providing alternative solutions. The mean readability score of the Flesch Reading Ease Score was 28.7 (\u00b10.8), the Flesch\u2013Kincaid Grade Level was 12.4 (\u00b10.5), and the Coleman\u2013Liau Index was 14.5 (\u00b10.5). Suitability-wise, the DISCERN score of ChatGPT was 48 (\u00b12.5) indicating suitable and comprehensible language for experts. Conclusions: Generative AI tools such as ChatGPT can serve as a supplementary tool for surgeons to offer valuable insights and foster intraoperative problem-solving abilities. However, it lacks consideration of individual patient factors and surgical nuances. Nevertheless, further refinement of its training data and rigorous scrutiny under experts to ensure the accuracy and up-to-date nature of the information holds the potential for it to be utilized in the surgical field."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.3390/jcm13030900",
                "pmid": null,
                "pmc": "PMC10856538",
                "pub_year": "2024",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC10856538/"
            }
        },
        {
            "title": "Study to evaluate the readability and visual appearance of online resources for blunt chest trauma: an evaluation of online resources using mixed methods",
            "abstract": {
                "Objectives": "Blunt chest trauma (BCT) is characterised by forceful and non-penetrative impact to the chest region. Increased access to the internet has led to online healthcare resources becoming used by the public to educate themselves about medical conditions. This study aimed to determine whether online resources for BCT are at an appropriate readability level and visual appearance for the public.",
                "Design": "We undertook a (1) a narrative overview assessment of the website; (2) a visual assessment of the identified website material content using an adapted framework of predetermined key criteria based on the Centers for Medicare and Medicaid Services toolkit and (3) a readability assessment using five readability scores and the Flesch reading ease score using Readable software.",
                "Data sources": "Using a range of key search terms, we searched Google, Bing and Yahoo websites on 9 October 2023 for online resources about BCT.",
                "Results": "We identified and assessed 85 websites. The median visual assessment score for the identified websites was 22, with a range of \u221214 to 37. The median readability score generated was 9 (14\u201315 years), with a range of 4.9\u201315.8. There was a significant association between the visual assessment and readability scores with a tendency for websites with lower readability scores having higher scores for the visual assessment (Spearman\u2019s r=\u22120.485; p<0.01). The median score for Flesch reading ease was 63.9 (plain English) with a range of 21.1\u201385.3.",
                "Conclusions": "Although the readability levels and visual appearance were acceptable for the public for many websites, many of the resources had much higher readability scores than the recommended level (8\u201310) and visually were poor.Better use of images would improve the appearance of websites further. Less medical terminology and shorter word and sentence length would also allow the public to comprehend the contained information more easily."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1136/bmjopen-2023-078552",
                "pmid": "38320839",
                "pmc": "PMC10860042",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/38320839/"
            }
        },
        {
            "title": "Can ChatGPT assist authors with abstract writing in medical journals? Evaluating the quality of scientific abstracts generated by ChatGPT and original abstracts",
            "abstract": {
                "Introduction": "ChatGPT, a sophisticated large language model (LLM), has garnered widespread attention for its ability to mimic human-like communication. As recent studies indicate a potential supportive role of ChatGPT in academic writing, we assessed the LLM\u2019s capacity to generate accurate and comprehensive scientific abstracts from published Randomised Controlled Trial (RCT) data, focusing on the adherence to the Consolidated Standards of Reporting Trials for Abstracts (CONSORT-A) statement, in comparison to the original authors\u2019 abstracts.",
                "Methodology": "RCTs, identified in a PubMed/MEDLINE search post-September 2021 across various medical disciplines, were subjected to abstract generation via ChatGPT versions 3.5 and 4, following the guidelines of the respective journals. The overall quality score (OQS) of each abstract was determined by the total number of adequately reported components from the 18-item CONSORT-A checklist. Additional outcome measures included percent adherence to each CONOSORT-A item, readability, hallucination rate, and regression analysis of reporting quality determinants.",
                "Results": "Original abstracts achieved a mean OQS of 11.89 (95% CI: 11.23\u201312.54), outperforming GPT 3.5 (7.89; 95% CI: 7.32\u20138.46) and GPT 4 (5.18; 95% CI: 4.64\u20135.71). Compared to GPT 3.5 and 4 outputs, original abstracts were more adherent with 10 and 14 CONSORT-A items, respectively. In blind assessments, GPT 3.5-generated abstracts were deemed most readable in 62.22% of cases which was significantly greater than the original (31.11%; P = 0.003) and GPT 4-generated (6.67%; P<0.001) abstracts. Moreover, ChatGPT 3.5 exhibited a hallucination rate of 0.03 items per abstract compared to 1.13 by GPT 4. No determinants for improved reporting quality were identified for GPT-generated abstracts.",
                "Conclusions": "While ChatGPT could generate more readable abstracts, their overall quality was inferior to the original abstracts. Yet, its proficiency to concisely relay key information with minimal error holds promise for medical research and warrants further investigations to fully ascertain the LLM\u2019s applicability in this domain."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1371/journal.pone.0297701",
                "pmid": null,
                "pmc": "PMC10866463",
                "pub_year": "2024",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC10866463/"
            }
        },
        {
            "title": "Artificial intelligence compared with human-derived patient educational materials on cirrhosis",
            "abstract": {
                "Background:": "The study compared the readability, grade level, understandability, actionability, and accuracy of standard patient educational material against artificial intelligence chatbot-derived patient educational material regarding cirrhosis.",
                "Methods:": "An identical standardized phrase was used to generate patient educational materials on cirrhosis from 4 large language model-derived chatbots (ChatGPT, DocsGPT, Google Bard, and Bing Chat), and the outputs were compared against a pre-existing human-derived educational material (Epic). Objective scores for readability and grade level were determined using Flesch-Kincaid and Simple Measure of Gobbledygook scoring systems. 14 patients/caregivers and 8 transplant hepatologists were blinded and independently scored the materials on understandability and actionability and indicated whether they believed the material was human or artificial intelligence-generated. Understandability and actionability were determined using the Patient Education Materials Assessment Tool for Printable Materials. Transplant hepatologists also provided medical accuracy scores.",
                "Results:": "Most educational materials scored similarly in readability and grade level but were above the desired sixth-grade reading level. All educational materials were deemed understandable by both groups, while only the human-derived educational material (Epic) was considered actionable by both groups. No significant difference in perceived actionability or understandability among the educational materials was identified. Both groups poorly identified which materials were human-derived versus artificial intelligence-derived.",
                "Conclusions:": "Chatbot-derived patient educational materials have comparable readability, grade level, understandability, and accuracy to human-derived materials. Readability, grade level, and actionability may be appropriate targets for improvement across educational materials on cirrhosis. Chatbot-derived patient educational materials show promise, and further studies should assess their usefulness in clinical practice."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1097/HC9.0000000000000367",
                "pmid": "38358382",
                "pmc": "PMC10871753",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/38358382/"
            }
        },
        {
            "title": "Magnetoencephalography (MEG) Data Processing in Epilepsy Patients with Implanted Responsive Neurostimulation (RNS) Devices",
            "abstract": {
                "title_content_0": "Drug-resistant epilepsy (DRE) is often treated with surgery or neuromodulation. Specifically, responsive neurostimulation (RNS) is a widely used therapy that is programmed to detect abnormal brain activity and intervene with tailored stimulation. Despite the success of RNS, some patients require further interventions. However, having an RNS device in situ is a hindrance to the performance of neuroimaging techniques. Magnetoencephalography (MEG), a non-invasive neurophysiologic and functional imaging technique, aids epilepsy assessment and surgery planning. MEG performed post-RNS is complicated by signal distortions. This study proposes an independent component analysis (ICA)-based approach to enhance MEG signal quality, facilitating improved assessment for epilepsy patients with implanted RNS devices. Three epilepsy patients, two with RNS implants and one without, underwent MEG scans. Preprocessing included temporal signal space separation (tSSS) and an automated ICA-based approach with MNE-Python. Power spectral density (PSD) and signal-to-noise ratio (SNR) were analyzed, and MEG dipole analysis was conducted using single equivalent current dipole (SECD) modeling. The ICA-based noise removal preprocessing method substantially improved the signal-to-noise ratio (SNR) for MEG data from epilepsy patients with implanted RNS devices. Qualitative assessment confirmed enhanced signal readability and improved MEG dipole analysis. ICA-based processing markedly enhanced MEG data quality in RNS patients, emphasizing its clinical relevance."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.3390/brainsci14020173",
                "pmid": "38391747",
                "pmc": "PMC10887328",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/38391747/"
            }
        },
        {
            "title": "Cultural Adaption, Translation, Preliminary Reliability and Validity of Key Psychological and Behavioural Measures for 18 to 25 Year-Olds Living with HIV in Uganda: A Multi-Stage Approach",
            "abstract": {
                "title_content_0": "HIV remains a significant public health issue among young adults living in Uganda. There is a need for reliable and valid measures of key psychological and behavioural constructs that are related to important outcomes for this population. We translated, adapted and tested the psychometric properties of questionnaires measuring HIV stigma, HIV disclosure cognitions and affect, antiretroviral therapy (ART) adherence, social support, personal values, and hope, using a multi-step process. This included: translation, back-translation, expert review, cognitive interviewing, readability and assessments of internal consistency with 93 young adults (18\u201325 years) living with perinatally acquired HIV in Uganda. Preliminary criterion validity was assessed by examining relationships between the adapted measures and wellbeing, HIV disclosure behaviour, HIV disclosure intention and viral load suppression. The measures all showed acceptable reliability and every questionnaire apart from the Agentic and Communal Value Scale was easy to read. Those scales measuring HIV disclosure affect and cognitions, social support, HIV stigma and hope showed relationships with other constructs suggestive of validity. There is preliminary evidence to support the use of these measures in research and clinical contexts for young adults living with perinatally acquired HIV in Uganda.",
                "Supplementary Information": "The online version contains supplementary material available at 10.1007/s10461-023-04193-y."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1007/s10461-023-04193-y",
                "pmid": "37792229",
                "pmc": "PMC10896775",
                "pub_year": "2023",
                "url": "https://pubmed.ncbi.nlm.nih.gov/37792229/"
            }
        },
        {
            "title": "Bridging the literacy gap for surgical consents: an AI-human expert collaborative approach",
            "abstract": {
                "title_content_0": "Despite the importance of informed consent in healthcare, the readability and specificity of consent forms often impede patients\u2019 comprehension. This study investigates the use of GPT-4 to simplify surgical consent forms and introduces an AI-human expert collaborative approach to validate content appropriateness. Consent forms from multiple institutions were assessed for readability and simplified using GPT-4, with pre- and post-simplification readability metrics compared using nonparametric tests. Independent reviews by medical authors and a malpractice defense attorney were conducted. Finally, GPT-4\u2019s potential for generating de novo procedure-specific consent forms was assessed, with forms evaluated using a validated 8-item rubric and expert subspecialty surgeon review. Analysis of 15 academic medical centers\u2019 consent forms revealed significant reductions in average reading time, word rarity, and passive sentence frequency (all P\u2009<\u20090.05) following GPT-4-faciliated simplification. Readability improved from an average college freshman to an 8th-grade level (P\u2009=\u20090.004), matching the average American\u2019s reading level. Medical and legal sufficiency consistency was confirmed. GPT-4 generated procedure-specific consent forms for five varied surgical procedures at an average 6th-grade reading level. These forms received perfect scores on a standardized consent form rubric and withstood scrutiny upon expert subspeciality surgeon review. This study demonstrates the first AI-human expert collaboration to enhance surgical consent forms, significantly improving readability without sacrificing clinical detail. Our framework could be extended to other patient communication materials, emphasizing clear communication and mitigating disparities related to health literacy barriers."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1038/s41746-024-01039-2",
                "pmid": "38459205",
                "pmc": "PMC10923794",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/38459205/"
            }
        },
        {
            "title": "Web-Based Information on Spinal Cord Stimulation: Qualitative Assessment of Publicly Accessible Online Resources",
            "abstract": {
                "Background": "Despite the growing accessibility of web-based information related to spinal cord stimulation (SCS), the content and quality of commonly encountered websites remain unknown.",
                "Objective": "This study aimed to assess the content and quality of web-based information on SCS.",
                "Methods": "This qualitative study was prospectively registered in Open Science Framework. Google Trends was used to identify the top trending, SCS-related search queries from 2012 to 2022. Top queried terms were then entered into separate search engines. Information found on websites within the first 2 pages of results was extracted and assessed for quality using the DISCERN instrument, the Journal of the American Medical Association benchmark criteria, and the Health on the Net Foundation code of conduct certification. Website readability and SCS-related information were also assessed.",
                "Results": "After exclusions, 42 unique sites were identified (scientific resources: n=6, nonprofit: n=12, for-profit: n=20, news or media: n=2, and personal or blog: n=2). Overall, information quality was moderate (DISCERN). Few sites met all the Journal of the American Medical Association benchmark criteria (n=3, 7%) or had Health on the Net Foundation certification (n=7, 16%). On average, information was difficult to read, requiring a 9th- to 10th-grade level of reading comprehension. Sites described SCS subcategories (n=14, 33%), indications (n=38, 90%), contraindications (n=14, 33%), side effects or risks (n=28, 66%), device considerations (n=25, 59%), follow-up (n=22, 52%), expected outcomes (n=31, 73%), provided authorship details (n=20, 47%), and publication dates (n=19, 45%). The proportion of for-profit sites reporting authorship information was comparatively less than other site types (n=3, 15%). Almost all sites focused on surgically implanted SCS (n=37, 88%). On average, nonprofit sites contained the greatest number of peer-reviewed reference citations (n=6, 50%). For-profit sites showed the highest proportion of physician or clinical referrals among site types (n=17, 85%) indicating implicit bias (ie, auto-referral).",
                "Conclusions": "Overall, our findings suggest the public may be exposed to incomplete or dated information from unidentifiable sources that could put consumers and patient groups at risk."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.2196/50031",
                "pmid": "38393781",
                "pmc": "PMC10924266",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/38393781/"
            }
        },
        {
            "title": "Conclusiveness, readability and textual characteristics of plain language summaries from medical and non-medical organizations: a cross-sectional study",
            "abstract": {
                "title_content_0": "This cross-sectional study compared plain language summaries (PLSs) from medical and non-medical organizations regarding conclusiveness, readability and textual characteristics. All Cochrane (medical PLSs, n\u2009=\u20098638) and Campbell Collaboration and International Initiative for Impact Evaluation (non-medical PLSs, n\u2009=\u2009163) PLSs of latest versions of systematic reviews published until 10 November 2022 were analysed. PLSs were classified into three conclusiveness categories (conclusive, inconclusive and unclear) using a machine learning tool for medical PLSs and by two experts for non-medical PLSs. A higher proportion of non-medical PLSs were conclusive (17.79% vs 8.40%, P\u2009<\u20090.0001), they had higher readability (median number of years of education needed to read the text with ease 15.23 (interquartile range (IQR) 14.35 to 15.96) vs 15.51 (IQR 14.31 to 16.77), P\u2009=\u20090.010), used more words (median 603 (IQR 539.50 to 658.50) vs 345 (IQR 202 to 476), P\u2009<\u20090.001). Language analysis showed that medical PLSs scored higher for disgust and fear, and non-medical PLSs scored higher for positive emotions. The reason for the observed differences between medical and non-medical fields may be attributed to the differences in publication methodologies or disciplinary differences. This approach to analysing PLSs is crucial for enhancing the overall quality of PLSs and knowledge translation to the general public."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1038/s41598-024-56727-6",
                "pmid": "38472285",
                "pmc": "PMC10933350",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/38472285/"
            }
        },
        {
            "title": "Exploring the potential of ChatGPT in medical dialogue summarization: a study on consistency with human preferences",
            "abstract": {
                "Background": "Telemedicine has experienced rapid growth in recent years, aiming to enhance medical efficiency and reduce the workload of healthcare professionals. During the COVID-19 pandemic in 2019, it became especially crucial, enabling remote screenings and access to healthcare services while maintaining social distancing. Online consultation platforms have emerged, but the demand has strained the availability of medical professionals, directly leading to research and development in automated medical consultation. Specifically, there is a need for efficient and accurate medical dialogue summarization algorithms to condense lengthy conversations into shorter versions focused on relevant medical facts. The success of large language models like generative pre-trained transformer (GPT)-3 has recently prompted a paradigm shift in natural language processing (NLP) research. In this paper, we will explore its impact on medical dialogue summarization.",
                "Methods": "We present the performance and evaluation results of two approaches on a medical dialogue dataset. The first approach is based on fine-tuned pre-trained language models, such as bert-based summarization (BERTSUM) and bidirectional auto-regressive Transformers (BART). The second approach utilizes a large language models (LLMs) GPT-3.5 with inter-context learning (ICL). Evaluation is conducted using automated metrics such as ROUGE and BERTScore.",
                "Results": "In comparison to the BART and ChatGPT models, the summaries generated by the BERTSUM model not only exhibit significantly lower ROUGE and BERTScore values but also fail to pass the testing for any of the metrics in manual evaluation. On the other hand, the BART model achieved the highest ROUGE and BERTScore values among all evaluated models, surpassing ChatGPT. Its ROUGE-1, ROUGE-2, ROUGE-L, and BERTScore values were 14.94%, 53.48%, 32.84%, and 6.73% higher respectively than ChatGPT\u2019s best results. However, in the manual evaluation by medical experts, the summaries generated by the BART model exhibit satisfactory performance only in the \u201cReadability\u201d metric, with less than 30% passing the manual evaluation in other metrics. When compared to the BERTSUM and BART models, the ChatGPT model was evidently more favored by human medical experts.",
                "Conclusion": "On one hand, the GPT-3.5 model can manipulate the style and outcomes of medical dialogue summaries through various prompts. The generated content is not only better received than results from certain human experts but also more comprehensible, making it a promising avenue for automated medical dialogue summarization. On the other hand, automated evaluation mechanisms like ROUGE and BERTScore fall short in fully assessing the outputs of large language models like GPT-3.5. Therefore, it is necessary to research more appropriate evaluation criteria."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1186/s12911-024-02481-8",
                "pmid": null,
                "pmc": "PMC10938713",
                "pub_year": "2024",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC10938713/"
            }
        },
        {
            "title": "Quality assessment of available Internet information on early orthodontic treatment",
            "abstract": {
                "Background": "This study aimed to evaluate the content, reliability, quality and readability of information on Internet websites about early orthodontic treatment.",
                "Methods": "The \u201cearly orthodontic treatment\u201d search term was individually entered into four web search engines. The content quality and reliability were reviewed with DISCERN, Journal of American Medical Association (JAMA), and Health on the Net code (HONcode) tools using the contents of websites meeting predetermined criteria. The readability of websites was evaluated with Flesch Reading Facilitate Score (FRES) and Flesch\u2013Kincaid Grade Level (FKGL).",
                "Results": "Eighty-six websites were suitable for inclusion and scoring of the 200 websites. 80.2% of websites belonged to orthodontists, 15.1% to multidisciplinary dental clinics and 4.7% to professional organizations. The mean DISCERN score of all websites (parts 1 and 2) was 27.98/75, ranging between 19 and 67. Professional organization websites had the highest scores for DISCERN criteria. Moreover, 45.3% of websites were compatible with JAMA\u2019s disclosure criterion, 7% with the currency criterion, 5.8% with the authorship criterion and 5.8% with the attribution criterion. Only three websites met all JAMA criteria, and these websites belonged to professional organizations. None of the websites had the HONcode logo. Mean FRES and FKGL were 47.6 and 11.6, respectively.",
                "Conclusions": "The quality of web-based information about early orthodontic treatment is poor, and readability is insufficient. More accurate and higher quality Internet sources are required on the web."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1186/s12903-024-04019-w",
                "pmid": null,
                "pmc": "PMC10949753",
                "pub_year": "2024",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC10949753/"
            }
        },
        {
            "title": "Improving the Readability of Patient Education Materials in Physical Therapy",
            "abstract": {
                "title_content_0": "Introduction",
                "title_content_1": "Printed patient educational materials (PEM) are often written above the recommended sixth- to eighth-grade reading levels, resulting in decreased client understanding and subsequent poor health literacy. Researchers have demonstrated that it is possible to improve readability to enhance clients\u2019 understanding and health literacy. The purpose of this study was to evaluate the readability of physical therapy (PT) PEM with and without modifications for improvement.",
                "title_content_2": "Methods",
                "title_content_3": "A convenience sample of 38 PT PEM of at least 10 sentences was obtained from a large suburban hospital system in the Midwestern region of the United States. Original and three modified versions (exclusion, revision, and combined exclusion/revision of words with >3 syllables) of the documents were assessed with the Simple Measure of \u201cGobbledygook\u201d (SMOG). All document means were compared to the recommended reading levels, and the original document means were compared with modified conditions.",
                "title_content_4": "Results",
                "title_content_5": "A majority of the documents were above an eighth-grade reading level. All modified conditions resulted in statistically significant reading level decreases, but only the combined modified condition decreased to the eighth-grade level.",
                "title_content_6": "Conclusion",
                "title_content_7": "Even with modifications, most PEM were above the recommended reading levels. Additional methods for improving readability and increased education about health literacy for healthcare professionals may be necessary to improve client comprehension."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.7759/cureus.54525",
                "pmid": "38516499",
                "pmc": "PMC10956377",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/38516499/"
            }
        },
        {
            "title": "Evaluation of the Accuracy, Credibility, and Readability of Statin-Related Websites: Cross-Sectional Study",
            "abstract": {
                "Background": "Cardiovascular disease (CVD) represents the greatest burden of mortality worldwide, and statins are the most commonly prescribed drug in its management. A wealth of information pertaining to statins and their side effects is on the internet; however, to date, no assessment of the accuracy, credibility, and readability of this information has been undertaken.",
                "Objective": "This study aimed to evaluate the quality (accuracy, credibility, and readability) of websites likely to be visited by the general public undertaking a Google search of the side effects and use of statin medications.",
                "Methods": "Following a Google web search, we reviewed the top 20 consumer-focused websites with statin information. Website accuracy, credibility, and readability were assessed based on website category (commercial, not-for-profit, and media), website rank, and the presence or absence of the Health on the Net Code of Conduct (HONcode) seal. Accuracy and credibility were assessed following the development of checklists (with 20 and 13 items, respectively). Readability was assessed using the Simple Measure of Gobbledegook scores.",
                "Results": "Overall, the accuracy score was low (mean 14.35 out of 20). While side effects were comprehensively covered by 18 websites, there was little information about statin use in primary and secondary prevention. None of the websites met all criteria on the credibility checklist (mean 7.8 out of 13). The median Simple Measure of Gobbledegook score was 9.65 (IQR 8.825-10.85), with none of the websites meeting the recommended reading grade of 6, even the media websites. A website bearing the HONcode seal did not mean that the website was more comprehensive or readable.",
                "Conclusions": "The quality of statin-related websites tended to be poor. Although the information contained was accurate, it was not comprehensive and was presented at a reading level that was too difficult for an average reader to fully comprehend. As such, consumers risk being uninformed about this pharmacotherapy."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.2196/42849",
                "pmid": "38483461",
                "pmc": "PMC10979333",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/38483461/"
            }
        },
        {
            "title": "Generative Pre-trained Transformer 4 makes cardiovascular magnetic resonance reports easy to understand",
            "abstract": {
                "Background": "Patients are increasingly using Generative Pre-trained Transformer 4 (GPT-4) to better understand their own radiology findings.",
                "Purpose": "To evaluate the performance of GPT-4 in transforming cardiovascular magnetic resonance (CMR) reports into text that is comprehensible to medical laypersons.",
                "Methods": "ChatGPT with GPT-4 architecture was used to generate three different explained versions of 20 various CMR reports (n = 60) using the same prompt: \u201cExplain the radiology report in a language understandable to a medical layperson\u201d. Two cardiovascular radiologists evaluated understandability, factual correctness, completeness of relevant findings, and lack of potential harm, while 13 medical laypersons evaluated the understandability of the original and the GPT-4 reports on a Likert scale (1 \u201cstrongly disagree\u201d, 5 \u201cstrongly agree\u201d). Readability was measured using the Automated Readability Index (ARI). Linear mixed-effects models (values given as median [interquartile range]) and intraclass correlation coefficient (ICC) were used for statistical analysis.",
                "Results": "GPT-4 reports were generated on average in 52\u00a0s\u00a0\u00b1\u00a013. GPT-4 reports achieved a lower ARI score (10 [9\u201312] vs 5 [4\u20136]; p\u00a0<\u00a00.001) and were subjectively easier to understand for laypersons than original reports (1 [1] vs 4 [4,5]; p\u00a0<\u00a00.001). Eighteen out of 20 (90%) standard CMR reports and 2/60 (3%) GPT-generated reports had an ARI score corresponding to the 8th grade level or higher. Radiologists\u2019 ratings of the GPT-4 reports reached high levels for correctness (5 [4, 5]), completeness (5 [5]), and lack of potential harm (5 [5]); with \u201cstrong agreement\u201d for factual correctness in 94% (113/120) and completeness of relevant findings in 81% (97/120) of reports. Test-retest agreement for layperson understandability ratings between the three simplified reports generated from the same original report was substantial (ICC: 0.62; p\u00a0<\u00a00.001). Interrater agreement between radiologists was almost perfect for lack of potential harm (ICC: 0.93, p\u00a0<\u00a00.001) and moderate to substantial for completeness (ICC: 0.76, p\u00a0<\u00a00.001) and factual correctness (ICC: 0.55, p\u00a0<\u00a00.001).",
                "Conclusion": "GPT-4 can reliably transform complex CMR reports into more understandable, layperson-friendly language while largely maintaining factual correctness and completeness, and can thus help convey patient-relevant radiology information in an easy-to-understand manner."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1016/j.jocmr.2024.101035",
                "pmid": "38460841",
                "pmc": "PMC10981113",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/38460841/"
            }
        },
        {
            "title": "A content analysis of medication adherence material in patient educational resources about gout",
            "abstract": {
                "title": "Abstract",
                "Objective": "This study aimed to investigate how medication adherence is addressed in online gout resources in six countries. We investigated how often adherence was referred to, the strategies suggested to improve patient adherence, and the types of nonadherence that were targeted. We also examined the readability of the adherence material.",
                "Methods": "A content analysis was conducted on 151 online gout resources from medical and health organisations in six predominantly English-speaking countries. Two reviewers coded the content of the websites into categories (kappa 0.80). The analysis involved coding the resources for reasons for nonadherence, and adherence-promoting strategies. Flesch-Kincaid Reading Ease scores and word count were also computed.",
                "Results": "Out of 151 websites examined, 77 websites discussed medication adherence (51%), with intentional nonadherence being more prevalent than unintentional nonadherence. 67 websites targeted different types of nonadherence, including drug-specific concerns (50%), misconceptions of gout curability and the necessity of medication (16%), forgetfulness (16%), and other practical challenges (5%). Strategies to promote adherence were found in one-third of the websites, with medication education being the most prevalent strategy (17%), followed by healthcare provider engagement (13%) and memory aid strategies (6%). On average, about 11% of the words (89.27, SD\u2009=\u200976.35) in the entire document were focused on adherence. Difficult reading comprehension was found in one-fifth of adherence-related websites.",
                "Conclusion": "Findings reveal limited medication adherence coverage and narrow strategies in online gout resources. Improved adherence portrayal is needed for effective gout management through comprehensive strategies and clear, understandable information."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1093/rap/rkae042",
                "pmid": "38629107",
                "pmc": "PMC11018534",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/38629107/"
            }
        },
        {
            "title": "Biomedical text readability after hypernym substitution with fine-tuned large language models",
            "abstract": {
                "title_content_0": "The advent of patient access to complex medical information online has highlighted the need for simplification of biomedical text to improve patient understanding and engagement in taking ownership of their health. However, comprehension of biomedical text remains a difficult task due to the need for domain-specific expertise. We aimed to study the simplification of biomedical text via large language models (LLMs) commonly used for general natural language processing tasks involve text comprehension, summarization, generation, and prediction of new text from prompts. Specifically, we finetuned three variants of large language models to perform substitutions of complex words and word phrases in biomedical text with a related hypernym. The output of the text substitution process using LLMs was evaluated by comparing the pre- and post-substitution texts using four readability metrics and two measures of sentence complexity. A sample of 1,000 biomedical definitions in the National Library of Medicine\u2019s Unified Medical Language System (UMLS) was processed with three LLM approaches, and each showed an improvement in readability and sentence complexity after hypernym substitution. Readability scores were translated from a pre-processed collegiate reading level to a post-processed US high-school level. Comparison between the three LLMs showed that the GPT-J-6b approach had the best improvement in measures of sentence complexity. This study demonstrates the merit of hypernym substitution to improve readability of complex biomedical text for the public and highlights the use case for fine-tuning open-access large language models for biomedical natural language processing."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1371/journal.pdig.0000489",
                "pmid": null,
                "pmc": "PMC11020904",
                "pub_year": "2024",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11020904/"
            }
        },
        {
            "title": "The Birth of the Contextual Health Education Readability Score in an Examination of Online Influenza Patient Education Materials",
            "abstract": {
                "title_content_0": "Introduction",
                "title_content_1": "Influenza is a major global health concern, with its rapid spread and mutation rate posing significant challenges in public health education and communication. Effective patient education materials (PEMs) are crucial for informed decision-making and improved health outcomes. This study evaluates the efficacy of online influenza PEMs using traditional readability tools and introduces the Contextual Health Education Readability Score (CHERS) to address the limitations of existing methods that do not capture the diverse array of visual and thematic means displayed.",
                "title_content_2": "Materials and methods",
                "title_content_3": "A comprehensive search was conducted to select relevant online influenza PEMs. This involved looking through Google's first two pages of results sorted by relevance, for a total of 20 results. These materials were evaluated using established readability tools (e.g., Flesch Reading Ease, Flesch-Kincaid Grade Level) and the Patient Education Materials Assessment Tool (PEMAT) for understandability and actionability. The study also involved the creation of CHERS, integrating factors such as semantic complexity, cultural relevance, and visual aid effectiveness. The development of CHERS included weighting each component based on its impact on readability and comprehension.",
                "title_content_4": "Results",
                "title_content_5": "The traditional readability tools demonstrated significant variability in the readability of the selected materials. The PEMAT analysis revealed general trends toward clarity in purpose and use of everyday language but indicated a need for improvement in summaries and visual aids.\u00a0The CHERS formula was calculated as follows: CHERS = (0.4 \u00d7 Average Sentence Length) + (0.3 \u00d7 Average Syllables per Word) + (0.15 \u00d7 Semantic Complexity Score) + (0.1 \u00d7 Cultural Relevance Score) + (0.05 \u00d7 Visual Aid Effectiveness Score), integrating multiple dimensions beyond traditional readability metrics.",
                "title_content_6": "Discussion",
                "title_content_7": "The study highlighted the limitations of traditional readability tools in assessing the complexity and cultural relevance of health information. The introduction of CHERS addressed these gaps by incorporating additional dimensions crucial for understanding in a healthcare context. The recommendations provided for creating effective influenza PEMs focused on language simplicity, cultural sensitivity, and actionability. This may enable further research into evaluating current PEMs and clarifying means of creating more effective content in the future.",
                "title_content_8": "Conclusions",
                "title_content_9": "The study underscores the need for comprehensive readability assessments in PEMs. The creation of CHERS marks a significant advancement in this field, providing a more holistic approach to evaluating health literacy materials. Its application could lead to the development of more inclusive and effective educational content, thereby improving public health outcomes and reducing the global burden of influenza. Future research should focus on further validating CHERS and exploring its applicability to other health conditions."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.7759/cureus.56715",
                "pmid": "38650807",
                "pmc": "PMC11033604",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/38650807/"
            }
        },
        {
            "title": "Empowering inclusivity: improving readability of living kidney donation information with ChatGPT",
            "abstract": {
                "Background": "Addressing disparities in living kidney donation requires making information accessible across literacy levels, especially important given that the average American adult reads at an 8th-grade level. This study evaluated the effectiveness of ChatGPT, an advanced AI language model, in simplifying living kidney donation information to an 8th-grade reading level or below.",
                "Methods": "We used ChatGPT versions 3.5 and 4.0 to modify 27 questions and answers from Donate Life America, a key resource on living kidney donation. We measured the readability of both original and modified texts using the Flesch-Kincaid formula. A paired t-test was conducted to assess changes in readability levels, and a statistical comparison between the two ChatGPT versions was performed.",
                "Results": "Originally, the FAQs had an average reading level of 9.6\u2009\u00b1\u20091.9. Post-modification, ChatGPT 3.5 achieved an average readability level of 7.72\u2009\u00b1\u20091.85, while ChatGPT 4.0 reached 4.30\u2009\u00b1\u20091.71, both with a p-value <0.001 indicating significant reduction. ChatGPT 3.5 made 59.26% of answers readable below 8th-grade level, whereas ChatGPT 4.0 did so for 96.30% of the texts. The grade level range for modified answers was 3.4\u201311.3 for ChatGPT 3.5 and 1\u20138.1 for ChatGPT 4.0.",
                "Conclusion": "Both ChatGPT 3.5 and 4.0 effectively lowered the readability grade levels of complex medical information, with ChatGPT 4.0 being more effective. This suggests ChatGPT's potential role in promoting diversity and equity in living kidney donation, indicating scope for further refinement in making medical information more accessible."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.3389/fdgth.2024.1366967",
                "pmid": "38659656",
                "pmc": "PMC11039889",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/38659656/"
            }
        },
        {
            "title": "How artificial intelligence can provide information about subdural hematoma: Assessment of readability, reliability, and quality of ChatGPT, BARD, and perplexity responses",
            "abstract": {
                "title_content_0": "Subdural hematoma is defined as blood collection in the subdural space between the dura mater and arachnoid. Subdural hematoma is a condition that neurosurgeons frequently encounter and has acute, subacute and chronic forms. The incidence in adults is reported to be 1.72\u201320.60/100.000 people annually. Our study aimed to evaluate the quality, reliability and readability of the answers to questions asked to ChatGPT, Bard, and perplexity about \u201cSubdural Hematoma.\u201d In this observational and cross-sectional study, we asked ChatGPT, Bard, and perplexity to provide the 100 most frequently asked questions about \u201cSubdural Hematoma\u201d separately. Responses from both chatbots were analyzed separately for readability, quality, reliability and adequacy. When the median readability scores of ChatGPT, Bard, and perplexity answers were compared with the sixth-grade reading level, a statistically significant difference was observed in all formulas (P\u2005<\u2005.001). All 3 chatbot responses were found to be difficult to read. Bard responses were more readable than ChatGPT\u2019s (P\u2005<\u2005.001) and perplexity\u2019s (P\u2005<\u2005.001) responses for all scores evaluated. Although there were differences between the results of the evaluated calculators, perplexity\u2019s answers were determined to be more readable than ChatGPT\u2019s answers (P\u2005<\u2005.05). Bard answers were determined to have the best GQS scores (P\u2005<\u2005.001). Perplexity responses had the best Journal of American Medical Association and modified DISCERN scores (P\u2005<\u2005.001). ChatGPT, Bard, and perplexity\u2019s current capabilities are inadequate in terms of quality and readability of \u201cSubdural Hematoma\u201d related text content. The readability standard for patient education materials as determined by the American Medical Association, National Institutes of Health, and the United States Department of Health and Human Services is at or below grade 6. The readability levels of the responses of artificial intelligence applications such as ChatGPT, Bard, and perplexity are significantly higher than the recommended 6th grade level."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1097/MD.0000000000038009",
                "pmid": null,
                "pmc": "PMC11062651",
                "pub_year": "2024",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11062651/"
            }
        },
        {
            "title": "A Multidisciplinary Assessment of ChatGPT\u2019s Knowledge of Amyloidosis: Observational Study",
            "abstract": {
                "Background": "Amyloidosis, a rare multisystem condition, often requires complex, multidisciplinary care. Its low prevalence underscores the importance of efforts to ensure the availability of high-quality patient education materials for better outcomes. ChatGPT (OpenAI) is a large language model powered by artificial intelligence that offers a potential avenue for disseminating accurate, reliable, and accessible educational resources for both patients and providers. Its user-friendly interface, engaging conversational responses, and the capability for users to ask follow-up questions make it a promising future tool in delivering accurate and tailored information to patients.",
                "Objective": "We performed a multidisciplinary assessment of the accuracy, reproducibility, and readability of ChatGPT in answering questions related to amyloidosis.",
                "Methods": "In total, 98 amyloidosis questions related to cardiology, gastroenterology, and neurology were curated from medical societies, institutions, and amyloidosis Facebook support groups and inputted into ChatGPT-3.5 and ChatGPT-4. Cardiology- and gastroenterology-related responses were independently graded by a board-certified cardiologist and gastroenterologist, respectively, who specialize in amyloidosis. These 2 reviewers (RG and DCK) also graded general questions for which disagreements were resolved with discussion. Neurology-related responses were graded by a board-certified neurologist (AAH) who specializes in amyloidosis. Reviewers used the following grading scale: (1) comprehensive, (2) correct but inadequate, (3) some correct and some incorrect, and (4) completely incorrect. Questions were stratified by categories for further analysis. Reproducibility was assessed by inputting each question twice into each model. The readability of ChatGPT-4 responses was also evaluated using the Textstat library in Python (Python Software Foundation) and the Textstat readability package in R software (R Foundation for Statistical Computing).",
                "Results": "ChatGPT-4 (n=98) provided 93 (95%) responses with accurate information, and 82 (84%) were comprehensive. ChatGPT-3.5 (n=83) provided 74 (89%) responses with accurate information, and 66 (79%) were comprehensive. When examined by question category, ChatGTP-4 and ChatGPT-3.5 provided 53 (95%) and 48 (86%) comprehensive responses, respectively, to \u201cgeneral questions\u201d (n=56). When examined by subject, ChatGPT-4 and ChatGPT-3.5 performed best in response to cardiology questions (n=12) with both models producing 10 (83%) comprehensive responses. For gastroenterology (n=15), ChatGPT-4 received comprehensive grades for 9 (60%) responses, and ChatGPT-3.5 provided 8 (53%) responses. Overall, 96 of 98 (98%) responses for ChatGPT-4 and 73 of 83 (88%) for ChatGPT-3.5 were reproducible. The readability of ChatGPT-4\u2019s responses ranged from 10th to beyond graduate US grade levels with an average of 15.5 (SD 1.9).",
                "Conclusions": "Large language models are a promising tool for accurate and reliable health information for patients living with amyloidosis. However, ChatGPT\u2019s responses exceeded the American Medical Association\u2019s recommended fifth- to sixth-grade reading level. Future studies focusing on improving response accuracy and readability are warranted. Prior to widespread implementation, the technology\u2019s limitations and ethical implications must be further explored to ensure patient safety and equitable implementation."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.2196/53421",
                "pmid": "38640472",
                "pmc": "PMC11069089",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/38640472/"
            }
        },
        {
            "title": "Large language models and bariatric surgery patient education: a comparative readability analysis of GPT-3.5, GPT-4, Bard, and online institutional resources",
            "abstract": {
                "Background": "The readability of online bariatric surgery patient education materials (PEMs) often surpasses the recommended 6th grade level. Large language models (LLMs), like ChatGPT and Bard, have the potential to revolutionize PEM delivery. We aimed to evaluate the readability of PEMs produced by U.S. medical institutions compared to LLMs, as well as the ability of LLMs to simplify their responses.",
                "Methods": "Responses to frequently asked questions (FAQs) related to bariatric surgery were gathered from top-ranked health institutions. FAQ responses were also generated from GPT-3.5, GPT-4, and Bard. LLMs were then prompted to improve the readability of their initial responses. The readability of institutional responses, initial LLM responses, and simplified LLM responses were graded using validated readability formulas. Accuracy and comprehensiveness of initial and simplified LLM responses were also compared.",
                "Results": "Responses to 66 FAQs were included. All institutional and initial LLM responses had poor readability, with average reading levels ranging from 9th grade to college graduate. Simplified responses from LLMs had significantly improved readability, with reading levels ranging from 6th grade to college freshman. When comparing simplified LLM responses, GPT-4 responses demonstrated the highest readability, with reading levels ranging from 6th to 9th grade. Accuracy was similar between initial and simplified responses from all LLMs. Comprehensiveness was similar between initial and simplified responses from GPT-3.5 and GPT-4. However, 34.8% of Bard's simplified responses were graded as less comprehensive compared to initial.",
                "Conclusion": "Our study highlights the efficacy of LLMs in enhancing the readability of bariatric surgery PEMs. GPT-4 outperformed other models, generating simplified PEMs from 6th to 9th grade reading levels. Unlike GPT-3.5 and GPT-4, Bard\u2019s simplified responses were graded as less comprehensive. We advocate for future studies examining the potential role of LLMs as dynamic and personalized sources of PEMs for diverse patient populations of all literacy levels.",
                "Supplementary Information": "The online version contains supplementary material available at 10.1007/s00464-024-10720-2."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1007/s00464-024-10720-2",
                "pmid": "38472531",
                "pmc": "PMC11078810",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/38472531/"
            }
        },
        {
            "title": "Dr. Google to Dr. ChatGPT: assessing the content and quality of artificial intelligence-generated medical information on appendicitis",
            "abstract": {
                "Introduction": "Generative artificial intelligence (AI) chatbots have recently been posited as potential sources of online medical information for patients making medical decisions. Existing online patient-oriented medical information has repeatedly been shown to be of variable quality and difficult readability. Therefore, we sought to evaluate the content and quality of AI-generated medical information on acute appendicitis.",
                "Methods": "A modified DISCERN assessment tool, comprising 16 distinct criteria each scored on a 5-point Likert scale (score range 16\u201380), was used to assess AI-generated content. Readability was determined using the Flesch Reading Ease (FRE) and Flesch-Kincaid Grade Level (FKGL) scores. Four popular chatbots, ChatGPT-3.5 and ChatGPT-4, Bard, and Claude-2, were prompted to generate medical information about appendicitis. Three investigators independently scored the generated texts blinded to the identity of the AI platforms.",
                "Results": "ChatGPT-3.5, ChatGPT-4, Bard, and Claude-2 had overall mean (SD) quality scores of 60.7 (1.2), 62.0 (1.0), 62.3 (1.2), and 51.3 (2.3), respectively, on a scale of 16\u201380. Inter-rater reliability was 0.81, 0.75, 0.81, and 0.72, respectively, indicating substantial agreement. Claude-2 demonstrated a significantly lower mean quality score compared to ChatGPT-4 (p\u2009=\u20090.001), ChatGPT-3.5 (p\u2009=\u20090.005), and Bard (p\u2009=\u20090.001). Bard was the only AI platform that listed verifiable sources, while Claude-2 provided fabricated sources. All chatbots except for Claude-2 advised readers to consult a physician if experiencing symptoms. Regarding readability, FKGL and FRE scores of ChatGPT-3.5, ChatGPT-4, Bard, and Claude-2 were 14.6 and 23.8, 11.9 and 33.9, 8.6 and 52.8, 11.0 and 36.6, respectively, indicating difficulty readability at a college reading skill level.",
                "Conclusion": "AI-generated medical information on appendicitis scored favorably upon quality assessment, but most either fabricated sources or did not provide any altogether. Additionally, overall readability far exceeded recommended levels for the public. Generative AI platforms demonstrate measured potential for patient education and engagement about appendicitis."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1007/s00464-024-10739-5",
                "pmid": "38443499",
                "pmc": "PMC11078845",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/38443499/"
            }
        },
        {
            "title": "Assessing the Responses of Large Language Models (ChatGPT-4, Gemini, and Microsoft Copilot) to Frequently Asked Questions in Breast Imaging: A Study on Readability and Accuracy",
            "abstract": {
                "title_content_0": "Background",
                "title_content_1": "Large language models (LLMs), such as ChatGPT-4, Gemini, and Microsoft Copilot, have been instrumental in various domains, including healthcare, where they enhance health literacy and aid in patient decision-making. Given the complexities involved in breast imaging procedures, accurate and comprehensible information is vital for patient engagement and compliance. This study aims to evaluate the readability and accuracy of the information provided by three prominent LLMs, ChatGPT-4, Gemini, and Microsoft Copilot, in response to frequently asked questions in breast imaging, assessing their potential to improve patient understanding and facilitate healthcare communication.",
                "title_content_2": "Methodology",
                "title_content_3": "We collected the most common questions on breast imaging from clinical practice and posed them to LLMs. We then evaluated the responses in terms of readability and accuracy. Responses from LLMs were analyzed for readability using the Flesch Reading Ease and Flesch-Kincaid Grade Level tests and for accuracy through a radiologist-developed Likert-type scale.",
                "title_content_4": "Results",
                "title_content_5": "The study found significant variations among LLMs. Gemini and Microsoft Copilot scored higher on readability scales (p < 0.001), indicating their responses were easier to understand. In contrast, ChatGPT-4 demonstrated greater accuracy in its responses (p < 0.001).",
                "title_content_6": "Conclusions",
                "title_content_7": "While LLMs such as ChatGPT-4 show promise in providing accurate responses, readability issues may limit their utility in patient education. Conversely, Gemini and Microsoft Copilot, despite being less accurate, are more accessible to a broader patient audience. Ongoing adjustments and evaluations of these models are essential to ensure they meet the diverse needs of patients, emphasizing the need for continuous improvement and oversight in the deployment of artificial intelligence technologies in healthcare."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.7759/cureus.59960",
                "pmid": "38726360",
                "pmc": "PMC11080394",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/38726360/"
            }
        },
        {
            "title": "Readability Analysis of Patient Education Material on Rotator Cuff Injuries From the Top 25 Ranking Orthopaedic Institutions",
            "abstract": {
                "Introduction:": "Rotator cuff injuries (RCIs) are incredibly common in the US adult population. Forty-three percent of adults have basic or below-basic literacy levels; nonetheless, patient educational materials (PEMs) are frequently composed at levels exceeding these reading capabilities. This study investigates the readability of PEMs on RCIs published by leading US orthopaedic institutions.",
                "Methods:": "The top 25 orthopaedic institutions on the 2022 U.S. News & World Report Best Hospitals Specialty Ranking were selected. Readability scores of PEMs related to RCI were calculated using the www.readabilityformulas.com website.",
                "Results:": "Among the 25 analyzed PEM texts, all exceeded the sixth-grade reading level. Only four of 168 scores (2.4%) were below the eighth-grade level.",
                "Discussion:": "This study indicates that PEMs on rotator cuff injuries from top orthopedic institutions are too complex for many Americans, with readability levels ranging from 8.5 to 16th grade, well above the CDC-recommended eighth-grade level. The research highlights a widespread issue with high reading levels across healthcare information and underscores the need for healthcare providers to adopt patient-centered communication strategies to improve comprehension and accessibility.",
                "Conclusion:": "PEMs on rotator cuff injuries from leading orthopedic institutions often have a reading level beyond that of many Americans, exceeding guidelines from the NIH and CDC that recommend PEMs be written at an eighth-grade reading level. To increase accessibility, enhance healthcare literacy, and improve patient outcomes, institutions should simplify these materials to meet recommended readability standards."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.5435/JAAOSGlobal-D-24-00085",
                "pmid": null,
                "pmc": "PMC11081572",
                "pub_year": "2024",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11081572/"
            }
        },
        {
            "title": "Leveraging Large Language Models for Improved Patient Access and Self-Management: Assessor-Blinded Comparison Between Expert- and AI-Generated Content",
            "abstract": {
                "Background": "While large language models (LLMs) such as ChatGPT and Google Bard have shown significant promise in various fields, their broader impact on enhancing patient health care access and quality, particularly in specialized domains such as oral health, requires comprehensive evaluation.",
                "Objective": "This study aims to assess the effectiveness of Google Bard, ChatGPT-3.5, and ChatGPT-4 in offering recommendations for common oral health issues, benchmarked against responses from human dental experts.",
                "Methods": "This comparative analysis used 40 questions derived from patient surveys on prevalent oral diseases, which were executed in a simulated clinical environment. Responses, obtained from both human experts and LLMs, were subject to a blinded evaluation process by experienced dentists and lay users, focusing on readability, appropriateness, harmlessness, comprehensiveness, intent capture, and helpfulness. Additionally, the stability of artificial intelligence responses was also assessed by submitting each question 3 times under consistent conditions.",
                "Results": "Google Bard excelled in readability but lagged in appropriateness when compared to human experts (mean 8.51, SD 0.37 vs mean 9.60, SD 0.33; P=.03). ChatGPT-3.5 and ChatGPT-4, however, performed comparably with human experts in terms of appropriateness (mean 8.96, SD 0.35 and mean 9.34, SD 0.47, respectively), with ChatGPT-4 demonstrating the highest stability and reliability. Furthermore, all 3 LLMs received superior harmlessness scores comparable to human experts, with lay users finding minimal differences in helpfulness and intent capture between the artificial intelligence models and human responses.",
                "Conclusions": "LLMs, particularly ChatGPT-4, show potential in oral health care, providing patient-centric information for enhancing patient education and clinical care. The observed performance variations underscore the need for ongoing refinement and ethical considerations in health care settings. Future research focuses on developing strategies for the safe integration of LLMs in health care settings."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.2196/55847",
                "pmid": "38663010",
                "pmc": "PMC11082737",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/38663010/"
            }
        },
        {
            "title": "An evaluation of written materials for supporting hypertensive patient education and counselling when performing a new medicine service in Poland",
            "abstract": {
                "title_content_0": "Background. The New Medicine Service (NMS) was developed in England more than ten years ago, as a three-stage consultation led by community pharmacists to support patients taking new medication for a chronic disease. In Poland, the scheme was officially introduced in January 2023. However, its implementation into common practice has been presented with various obstacles, including the need to develop relationships with general practitioners, resolve the payment structure, and provide training with adequate supporting materials. Hence, written materials have been designed for use as an optional tool for counselling patients receiving an NMS in community pharmacies. Methods. The present study evaluates the ability of these materials to inform patients about the need to adhere to anti-hypertensive medication. A group of 401 randomly-selected adult visitors to pharmacies and/or healthcare centres were surveyed; one third had hypertension in their history. Results. The structure, grammar and readability of the text achieved the required threshold of 40% according to the Plain Language Index. The designed materials effectively informed the patients about anti-hypertensive medication, reflected in an increased score in a knowledge test, and were rated positively regarding information level, comprehensibility and presentation. Conclusion. The proposed material may serve as an additional, \u201cpatient-friendly\u201d educational tool for use as part of an NMS.",
                "Supplementary Information": "The online version contains supplementary material available at 10.1186/s12909-024-05523-x."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1186/s12909-024-05523-x",
                "pmid": "38730316",
                "pmc": "PMC11088063",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/38730316/"
            }
        },
        {
            "title": "Assessment of the Arabic patient-centered online information about orthodontic pain: A quality and readability assessment",
            "abstract": {
                "Background": "This study assesses the quality and readability of Arabic online information about orthodontic pain. With the increasing reliance on the internet for health information, especially among Arabic speakers, it\u2019s critical to ensure the accuracy and comprehensiveness of available content. Our methodology involved a systematic search using the Arabic term for (Orthodontic Pain) in Google, Bing, and Yahoo. This search yielded 193,856 results, from which 74 websites were selected based on predefined criteria, excluding duplicates, scientific papers, and non-Arabic content.",
                "Materials and methods": "For quality assessment, we used the DISCERN instrument, the Journal of the American Medical Association (JAMA) benchmarks, and the Health on the Net (HON) code. Readability was evaluated using the Simplified Measure of Gobbledygook (SMOG), Flesch Reading Ease Score (FRES), and Flesch-Kincaid Grade Level (FKGL) scores.",
                "Results": "Results indicated that none of the websites received the HONcode seal. The DISCERN assessment showed median total scores of 14.96 (\u00b1 5.65), with low overall quality ratings. In JAMA benchmarks, currency was the most achieved aspect, observed in 45 websites (60.81%), but none met all four criteria simultaneously. Readability scores suggested that the content was generally understandable, with a median FKGL score of 6.98 and a median SMOG score of 3.98, indicating middle school-level readability.",
                "Conclusion": "This study reveals a significant gap in the quality of Arabic online resources on orthodontic pain, highlighting the need for improved standards and reliability. Most websites failed to meet established quality criteria, underscoring the necessity for more accurate and trustworthy health information for Arabic-speaking patients."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1371/journal.pone.0303308",
                "pmid": "38781283",
                "pmc": "PMC11115317",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/38781283/"
            }
        },
        {
            "title": "Assessment of the Quality and Readability of Information Provided by ChatGPT in Relation to the Use of Platelet-Rich Plasma Therapy for Osteoarthritis",
            "abstract": {
                "title_content_0": "Objective: This study aimed to evaluate the quality and readability of information generated by ChatGPT versions 3.5 and 4 concerning platelet-rich plasma (PRP) therapy in the management of knee osteoarthritis (OA), exploring whether large language models (LLMs) could play a significant role in patient education. Design:\nA total of 23 common patient queries regarding the role of PRP therapy in knee OA management were presented to ChatGPT versions 3.5 and 4. The quality of the responses was assessed using the DISCERN criteria, and readability was evaluated using six established assessment tools. Results: Both ChatGPT versions 3.5 and 4 produced moderate quality information. The quality of information provided by ChatGPT version 4 was significantly better than version 3.5, with mean DISCERN scores of 48.74 and 44.59, respectively. Both models scored highly with respect to response relevance and had a consistent emphasis on the importance of shared decision making. However, both versions produced content significantly above the recommended 8th grade reading level for patient education materials (PEMs), with mean reading grade levels (RGLs) of 17.18 for ChatGPT version 3.5 and 16.36 for ChatGPT version 4, indicating a potential barrier to their utility in patient education. Conclusions: While ChatGPT versions 3.5 and 4 both demonstrated the capability to generate information of moderate quality regarding the role of PRP therapy for knee OA, the readability of the content remains a significant barrier to widespread usage, exceeding the recommended reading levels for PEMs. Although ChatGPT version 4 showed improvements in quality and source citation, future iterations must focus on producing more accessible content to serve as a viable resource in patient education. Collaboration between healthcare providers, patient organizations, and AI developers is crucial to ensure the generation of high quality, peer reviewed, and easily understandable information that supports informed healthcare decisions."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.3390/jpm14050495",
                "pmid": null,
                "pmc": "PMC11122161",
                "pub_year": "2024",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11122161/"
            }
        },
        {
            "title": "ChatGPT makes medicine easy to swallow: an exploratory case study on simplified radiology reports",
            "abstract": {
                "Objectives": "To assess the quality of simplified radiology reports generated with the large language model (LLM) ChatGPT and to discuss challenges and chances of ChatGPT-like LLMs for medical text simplification.",
                "Methods": "In this exploratory case study, a radiologist created three fictitious radiology reports which we simplified by prompting ChatGPT with \u201cExplain this medical report to a child using simple language.\u201d In a questionnaire, we tasked 15 radiologists to rate the quality of the simplified radiology reports with respect to their factual correctness, completeness, and potential harm for patients. We used Likert scale analysis and inductive free-text categorization to assess the quality of the simplified reports.",
                "Results": "Most radiologists agreed that the simplified reports were factually correct, complete, and not potentially harmful to the patient. Nevertheless, instances of incorrect statements, missed relevant medical information, and potentially harmful passages were reported.",
                "Conclusion": "While we see a need for further adaption to the medical field, the initial insights of this study indicate a tremendous potential in using LLMs like ChatGPT to improve patient-centered care in radiology and other medical domains.",
                "Clinical relevance statement": "Patients have started to use ChatGPT to simplify and explain their medical reports, which is expected to affect patient-doctor interaction. This phenomenon raises several opportunities and challenges for clinical routine.",
                "Key Points": "\u2022 Patients have started to use ChatGPT to simplify their medical reports, but their quality was unknown.\u2022 In a questionnaire, most participating radiologists overall asserted good quality to radiology reports simplified with ChatGPT. However, they also highlighted a notable presence of errors, potentially leading patients to draw harmful conclusions.\u2022 Large language models such as ChatGPT have vast potential to enhance patient-centered care in radiology and other medical domains. To realize this potential while minimizing harm, they need supervision by medical experts and adaption to the medical field.",
                "Graphical Abstract": "",
                "Supplementary Information": "The online version contains supplementary material available at 10.1007/s00330-023-10213-1."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1007/s00330-023-10213-1",
                "pmid": "37794249",
                "pmc": "PMC11126432",
                "pub_year": "2023",
                "url": "https://pubmed.ncbi.nlm.nih.gov/37794249/"
            }
        },
        {
            "title": "Evaluating ChatGPT-3.5 and ChatGPT-4.0 Responses on Hyperlipidemia for Patient Education",
            "abstract": {
                "title_content_0": "Introduction",
                "title_content_1": "Hyperlipidemia is prevalent worldwide and affects a significant number of US\u00a0adults. It significantly contributes to ischemic heart disease and millions of deaths annually. With the increasing use of the internet for health information, tools like ChatGPT (OpenAI, San Francisco, CA, USA) have gained traction. ChatGPT version 4.0, launched in March 2023, offers enhanced features over its predecessor but requires a monthly fee. This study compares the accuracy, comprehensibility, and response length of the free and paid versions of ChatGPT for patient education on hyperlipidemia.",
                "title_content_2": "Materials and methods",
                "title_content_3": "ChatGPT versions 3.5 and 4.0 were prompted in three different ways and 25 questions from the Cleveland Clinic's frequently asked questions (FAQs) on hyperlipidemia. Prompts included\u00a0no prompting (Form 1), patient-friendly prompting (Form 2), and physician-level prompting (Form 3). Responses were categorized as incorrect, partially correct, or correct. Additionally, the grade level and word count from each response were recorded for analysis.",
                "title_content_4": "Results",
                "title_content_5": "Overall, scoring frequencies for ChatGPT version 3.5 were: five (6.67%) incorrect, 18 partially correct (24%), and 52 (69.33%) correct. Scoring frequencies for ChatGPT version 4.0 were: one (1.33%) incorrect, 18 (24.00%) partially correct, and 56 (74.67%) correct. Correct answers did not significantly differ between ChatGPT version 3.5 and ChatGPT version 4.0 (p = 0.586). ChatGPT version 3.5 had a significantly higher grade reading level than version 4.0 (p = 0.0002). ChatGPT version 3.5 had a significantly higher word count than version 4.0 (p = 0.0073).",
                "title_content_6": "Discussion",
                "title_content_7": "There was no significant difference in accuracy between the free and paid versions of hyperlipidemia FAQs. Both versions provided accurate but sometimes partially complete responses. Version 4.0 offered more concise and readable information, aligning with the readability of most online medical resources despite exceeding the National Institutes of Health's (NIH's) recommended eighth-grade reading level. The paid version demonstrated superior adaptability in tailoring responses based on the input.",
                "title_content_8": "Conclusion",
                "title_content_9": "Both versions of ChatGPT provide reliable medical information, with the paid version offering more adaptable and readable responses. Healthcare providers can recommend ChatGPT as a source of patient education, regardless of the version used. Future research should explore diverse question formulations and ChatGPT's handling of incorrect information."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.7759/cureus.61067",
                "pmid": "38803402",
                "pmc": "PMC11128363",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/38803402/"
            }
        },
        {
            "title": "Assessment of medical information on irritable bowel syndrome information in Wikipedia and Baidu Encyclopedia: comparative study",
            "abstract": {
                "Background": "Irritable bowel syndrome (IBS) is a functional gastrointestinal disorder (FGID) with heterogeneous clinical presentations. There are no clear testing parameters for its diagnosis, and the complex pathophysiology of IBS and the limited time that doctors have to spend with patients makes it difficult to adequately educate patients in the outpatient setting. An increased awareness of IBS means that patients are more likely to self-diagnose and self-manage IBS based on their own symptoms. These factors may make patients more likely to turn to Internet resources. Wikipedia is the most popular online encyclopedia among English-speaking users, with numerous validations. However, in Mandarin-speaking regions, the Baidu Encyclopedia is most commonly used. There have been no studies on the reliability, readability, and objectivity of IBS information on the two sites. This is an urgent issue as these platforms are accessed by approximately 1.45 billion people.",
                "Objective": "We compared the IBS content on Wikipedia (in English) and Baidu Baike (in Chinese), two online encyclopedias, in terms of reliability, readability, and objectivity.",
                "Methods": "The Baidu Encyclopedia (in Chinese) and Wikipedia (in English) were evaluated based on the Rome IV IBS definitions and diagnoses. All possible synonyms and derivatives for IBS and IBS-related FGIDs were screened and identified. Two gastroenterology experts evaluated the scores of articles for both sites using the DISCERN instrument, the Journal of the American Medical Association scoring system (JAMA), and the Global Quality Score (GQS).",
                "Results": "Wikipedia scored higher overall with DISCERN (p\u00a0<\u00a0.0001), JAMA (p\u00a0<\u00a0.0001) and GQS (p\u00a0<\u00a0.05) than the Baidu Encyclopedia. Specifically, Wikipedia scored higher in DISCERN Section 1 (p\u00a0<\u00a0.0001), DISCERN Section 2 (p\u00a0<\u00a0.01), DISCERN Section 3 (p\u00a0<\u00a0.001), and the General DISCERN score (p\u00a0<\u00a0.0001) than the Baidu Encyclopedia. Both sites had low DISCERN Section 2 scores (p\u00a0=\u00a0.18). Wikipedia also had a larger percentage of high quality scores in total DISCERN, DISCERN Section 1, and DISCERN Section 3 (p\u00a0<\u00a0.0001, P\u00a0<\u00a0.0001, P\u00a0<\u00a0.0004, respectively, based on the above 3 (60%) rule).",
                "Conclusions": "Wikipedia provides more reliable, higher quality, and more objective IBS-related health information than the Baidu Encyclopedia. However, there should be improvements in the information quality for both sites. Medical professionals and institutions should collaborate with these online platforms to offer better health information for IBS."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.7717/peerj.17264",
                "pmid": "38803580",
                "pmc": "PMC11129691",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/38803580/"
            }
        },
        {
            "title": "ChatGPT-4 Can Help Hand Surgeons Communicate Better With Patients",
            "abstract": {
                "title_content_0": "The American Society for Surgery of the Hand and British Society for Surgery of the Hand produce patient-focused information above the sixth-grade readability recommended by the American Medical Association. To promote health equity, patient-focused content should be aimed at an appropriate level of health literacy. Artificial intelligence\u2013driven large language models may be able to assist hand surgery societies in improving the readability of the information provided to patients. The readability was calculated for all the articles written in English on the American Society for Surgery of the Hand and British Society for Surgery of the Hand websites, in terms of seven of the commonest readability formulas. Chat Generative Pre-Trained Transformer version 4 (ChatGPT-4) was then asked to rewrite each article at a sixth-grade readability level. The readability for each response was calculated and compared with the unedited articles. Chat Generative Pre-Trained Transformer version 4 was able to improve the readability across all chosen readability formulas and was successful in achieving a mean sixth-grade readability level in terms of the Flesch Kincaid Grade Level and Simple Measure of Gobbledygook calculations. It increased the mean Flesch Reading Ease score, with higher scores representing more readable material. This study demonstrated that ChatGPT-4 can be used to improve the readability of patient-focused material in hand surgery. However, ChatGPT-4 is interested primarily in sounding natural, and not in seeking truth, and hence, each response must be evaluated by the surgeon to ensure that information accuracy is not being sacrificed for the sake of readability by this powerful tool."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1016/j.jhsg.2024.03.008",
                "pmid": null,
                "pmc": "PMC11133925",
                "pub_year": "2024",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11133925/"
            }
        },
        {
            "title": "What makes an article a must read in medical education?",
            "abstract": {
                "Background": "The dissemination of published scholarship is intended to bring new evidence and ideas to a wide audience. However, the increasing number of articles makes it challenging to determine where to focus one\u2019s attention. This study describes factors that may influence decisions to read and recommend a medical education article.",
                "Methods": "Authors analyzed data collected from March 2021 through September 2022 during a monthly process to identify \u201cMust Read\u201d articles in medical education. An international team of health sciences educators, learners, and researchers voted on titles and abstracts to advance articles to full text review. Full texts were rated using five criteria: relevance, methodology, readability, originality, and whether it addressed a critical issue in medical education. At an end-of-month meeting, 3\u20134 articles were chosen by consensus as \u201cMust Read\u201d articles. Analyses were used to explore the associations of article characteristics and ratings with Must Read selection.",
                "Results": "Over a period of 19 months, 7487 articles from 856 journals were screened, 207 (2.8%) full texts were evaluated, and 62 (0.8%) were chosen as Must Reads. During screening, 3976 articles (53.1%) received no votes. BMC Medical Education had the largest number of articles at screening (n\u2009=\u20091181, 15.8%). Academic Medicine had the largest number as Must Reads (n\u2009=\u200922, 35.5%). In logistic regressions adjusting for the effect of individual reviewers, all rating criteria were independently associated with selection as a Must Read (p\u2009<\u20090.05), with methodology (OR 1.44 (95%CI\u2009=\u20091.23\u20131.69) and relevance (OR 1.43 (95%CI\u2009=\u20091.20\u20131.70)) having the highest odds ratios.",
                "Conclusions": "Over half of the published medical education articles did not appeal to a diverse group of potential readers; this represents a missed opportunity to make an impact and potentially wasted effort. Our findings suggest opportunities to enhance value in the production and dissemination of medical education scholarship.",
                "Supplementary Information": "The online version contains supplementary material available at 10.1186/s12909-024-05564-2."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1186/s12909-024-05564-2",
                "pmid": "38807077",
                "pmc": "PMC11134941",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/38807077/"
            }
        },
        {
            "title": "Supporting health and social care professionals in serious illness conversations: Development, validation, and preliminary evaluation of an educational booklet",
            "abstract": {
                "title_content_0": "Serious illness conversations aim to align the care process with the goals and preferences of adult patients suffering from any advanced disease. They represent a challenge for healthcare professionals and require specific skills. Conversation guides consistent with task-centered instructional strategies may be particularly helpful to improve the quality of communication. This study aims to develop, validate, and preliminarily evaluate an educational booklet to support Italian social and healthcare professionals in serious illness conversations. A three-step approach, including development, validation, and evaluation, was followed. A co-creation process with meaningful stakeholders led to the development of the booklet, validated by 15 experts on clarity, completeness, coherence, and relevance. It underwent testing on readability (Gulpease index, 0 = lowest-100 = maximum) and design (Baker Able Leaflet Design criteria, 0 = worst to 32 = best). Twenty-two professionals with different scope of practice and care settings evaluated acceptability (acceptable if score \u226530), usefulness, feasibility to use (1 = not at all to 10 = extremely), and perceived acquired knowledge (1 = not at all to 5 = extremely). After four rounds of adjustments, the booklet scored 97% for relevance, 60 for readability, and 25/32 for design. In all, 18 (81.8%), 19 (86.4%) and 17 (77.3%) professionals deemed the booklet acceptable, moderate to highly useful, and feasible to use, respectively; 18/22 perceived gain in knowledge and all would recommend it to colleagues. The booklet has good readability, excellent design, high content validity, and a high degree of perceived usefulness and acquired knowledge. The booklet is tailored to users\u2019 priorities, mirrors their most frequent daily practice challenges, and offers 1-minute, 2-minute and 5-minute solutions for each scenario. The co-creation process ensured the development of an educational resource that could be useful regardless of the scope of practice and the care setting to support professionals in serious illness conversations."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1371/journal.pone.0304180",
                "pmid": "38820471",
                "pmc": "PMC11142603",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/38820471/"
            }
        },
        {
            "title": "MAD-FC: A fold change visualization with readability, proportionality, and symmetry",
            "abstract": {
                "title_content_0": "We propose a fold change transform that demonstrates a combination of visualization properties exhibited by log and linear plots of fold change. A fold change visualization should ideally exhibit: (1) readability, where fold change values are recoverable from datapoint position; (2) proportionality, where fold change values of the same direction are proportionally distant from the point of no change; (3) symmetry, where positive and negative fold changes of the same magnitude are equidistant to the point of no change; and (4) high dynamic range, where datapoint values are distinguishable across orders of magnitude within a fixed plot area and pixel resolution. A linear visualization has readability and partial proportionality but lacks high dynamic range and symmetry (because negative direction fold changes are bound between [0, 1] while positive are between (1, \u221e)). Log plots of fold change have partial readability, high dynamic range, and symmetry, but lack proportionality because of the log transform. We outline a new transform, named mirrored axis distortion of fold change (MAD-FC), that extends a linear visualization of fold change data to exhibit readability, proportionality, and symmetry (but still has the limited dynamic range of linear plots). We illustrate the use of MAD-FC with biomedical data using various fold change plots. We argue that MAD plots may be a more useful visualization than log or linear plots for applications that do not require a high dynamic range (less than 8 units in log2 space)."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1371/journal.pone.0304632",
                "pmid": "38820396",
                "pmc": "PMC11142613",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/38820396/"
            }
        },
        {
            "title": "Artificial Intelligence in Postoperative Care: Assessing Large Language Models for Patient Recommendations in Plastic Surgery",
            "abstract": {
                "title_content_0": "Since their release, the medical community has been actively exploring large language models\u2019 (LLMs) capabilities, which show promise in providing accurate medical knowledge. One potential application is as a patient resource. This study analyzes and compares the ability of the currently available LLMs, ChatGPT-3.5, GPT-4, and Gemini, to provide postoperative care recommendations to plastic surgery patients. We presented each model with 32 questions addressing common patient concerns after surgical cosmetic procedures and evaluated the medical accuracy, readability, understandability, and actionability of the models\u2019 responses. The three LLMs provided equally accurate information, with GPT-3.5 averaging the highest on the Likert scale (LS) (4.18 \u00b1\u00a00.93) (p = 0.849), while Gemini provided significantly more readable (p = 0.001) and understandable responses (p = 0.014; p = 0.001). There was no difference in the actionability of the models\u2019 responses (p = 0.830). Although LLMs have shown their potential as adjunctive tools in postoperative patient care, further refinement and research are imperative to enable their evolution into comprehensive standalone resources."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.3390/healthcare12111083",
                "pmid": null,
                "pmc": "PMC11171524",
                "pub_year": "2024",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11171524/"
            }
        },
        {
            "title": "A Cross-Sectional Analysis of the Readability of Online Information Regarding Hip Osteoarthritis",
            "abstract": {
                "title_content_0": "Introduction",
                "title_content_1": "Osteoarthritis (OA) is an age-related degenerative joint disease. There is a 25% risk of symptomatic hip OA in patients who live up to 85 years of age. It can impair a person\u2019s daily activities and increase their reliance on healthcare services. It is primarily managed with education, weight loss and exercise, supplemented with pharmacological interventions. Poor health literacy is associated with negative treatment outcomes and patient dissatisfaction. A literature search found there are no previously published studies examining the readability of online information about hip OA.",
                "title_content_2": "Objectives",
                "title_content_3": "To assess the readability of healthcare websites regarding hip OA.",
                "title_content_4": "Methods",
                "title_content_5": "The terms \u201chip pain\u201d, \u201chip osteoarthritis\u201d, \u201chip arthritis\u201d, and \u201chip OA\u201d were searched on Google and Bing. Of 240 websites initially considered, 74 unique websites underwent evaluation using the WebFX online readability software (WebFX\u00ae, Harrisburg, USA). Readability was determined using the Flesch Reading Ease Score (FRES), Flesch-Kincaid Reading Grade Level (FKGL), Gunning Fog Index (GFI), Simple Measure of Gobbledygook (SMOG), Coleman-Liau Index (CLI), and Automated Readability Index (ARI). In line with recommended guidelines and previous studies, FRES >65 or a grade level score of sixth grade and under was considered acceptable.",
                "title_content_6": "Results",
                "title_content_7": "The average FRES was 56.74\u00b18.18 (range 29.5-79.4). Only nine (12.16%) websites had a FRES score >65. The average FKGL score was 7.62\u00b11.69 (range 4.2-12.9). Only seven (9.46%) websites were written at or below a sixth-grade level according to the FKGL score. The average GFI score was 9.20\u00b12.09 (range 5.6-16.5). Only one (1.35%) website was written at or below a sixth-grade level according to the GFI score.\u00a0The average SMOG score was 7.29\u00b11.41 (range 5.4-12.0). Only eight (10.81%) websites were written at or below a sixth-grade level according to the SMOG score. The average CLI score was 13.86\u00b11.75 (range 9.6-19.7). All 36 websites were written above a sixth-grade level according to the CLI score. The average ARI score was 6.91\u00b12.06 (range 3.1-14.0). Twenty-eight\u00a0(37.84%) websites were written at or below a sixth-grade level according to the ARI score.",
                "title_content_8": "One-sample t-tests showed that FRES (p<0.001, CI -10.2 to -6.37), FKGL (p<0.001, CI 1.23 to 2.01), GFI (p<0.001, CI 2.72 to 3.69), SMOG (p<0.001, CI 0.97 to 1.62), CLI (p<0.001, CI 7.46 to 8.27), and ARI (p<0.001, CI 0.43 to 1.39) scores were significantly different from the accepted standard.",
                "title_content_9": "One-way analysis of variance (ANOVA) testing of FRES scores (p=0.009) and CLI scores (p=0.009) showed a significant difference between categories. Post hoc testing showed a significant difference between academic and non-profit categories for FRES scores (p=0.010, CI -15.17 to -1.47) and CLI scores (p=0.008, CI 0.35 to 3.29).",
                "title_content_10": "Conclusions",
                "title_content_11": "Most websites regarding hip OA are written above recommended reading levels, hence exceeding the comprehension levels of the average patient. Readability of these resources must be improved to improve patient access to online healthcare information which can lead to improved patient understanding of their own condition and treatment outcomes."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.7759/cureus.60536",
                "pmid": "38887325",
                "pmc": "PMC11181007",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/38887325/"
            }
        },
        {
            "title": "ChatGPT: is it good for our glaucoma patients?",
            "abstract": {
                "Purpose": "Our study investigates ChatGPT and its ability to communicate with glaucoma patients.",
                "Methods": "We inputted eight glaucoma-related questions/topics found on the American Academy of Ophthalmology (AAO)\u2019s website into ChatGPT. We used the Flesch\u2013Kincaid test, Gunning Fog Index, SMOG Index, and Dale\u2013Chall readability formula to evaluate the comprehensibility of its responses for patients. ChatGPT\u2019s answers were compared with those found on the AAO\u2019s website.",
                "Results": "ChatGPT\u2019s responses required reading comprehension of a higher grade level (average = grade 12.5 \u00b1 1.6) than that of the text on the AAO\u2019s website (average = 9.4 grade \u00b1 3.5), (0.0384). For the eight responses, the key ophthalmic terms appeared 34 out of 86 times in the ChatGPT responses vs. 86 out of 86 times in the text on the AAO\u2019s website. The term \u201ceye doctor\u201d appeared once in the ChatGPT text, but the formal term \u201cophthalmologist\u201d did not appear. The term \u201cophthalmologist\u201d appears 26 times on the AAO\u2019s website. The word counts of the answers produced by ChatGPT and those on the AAO\u2019s website were similar (p = 0.571), with phrases of a homogenous length.",
                "Conclusion": "ChatGPT trains on the texts, phrases, and algorithms inputted by software engineers. As ophthalmologists, through our websites and journals, we should consider encoding the phrase \u201csee an ophthalmologist\u201d. Our medical assistants should sit with patients during their appointments to ensure that the text is accurate and that they fully comprehend its meaning. ChatGPT is effective for providing general information such as definitions or potential treatment options for glaucoma. However, ChatGPT has a tendency toward repetitive answers and, due to their elevated readability scores, these could be too difficult for a patient to read."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.3389/fopht.2023.1260415",
                "pmid": "38983063",
                "pmc": "PMC11182305",
                "pub_year": "2023",
                "url": "https://pubmed.ncbi.nlm.nih.gov/38983063/"
            }
        },
        {
            "title": "Evaluating ChatGPT-4\u2019s performance as a digital health advisor for otosclerosis surgery",
            "abstract": {
                "Purpose": "This study aims to evaluate the effectiveness of ChatGPT-4, an artificial intelligence (AI) chatbot, in providing accurate and comprehensible information to patients regarding otosclerosis surgery.",
                "Methods": "On October 20, 2023, 15 hypothetical questions were posed to ChatGPT-4 to simulate physician-patient interactions about otosclerosis surgery. Responses were evaluated by three independent ENT specialists using the DISCERN scoring system. The readability was evaluated using multiple indices: Flesch Reading Ease (FRE), Flesch-Kincaid Grade Level (FKGL), Gunning Fog Index (Gunning FOG), Simple Measure of Gobbledygook (SMOG), Coleman-Liau Index (CLI), and Automated Readability Index (ARI).",
                "Results": "The responses from ChatGPT-4 received DISCERN scores ranging from poor to excellent, with an overall score of 50.7\u2009\u00b1\u20098.2. The readability analysis indicated that the texts were above the 6th-grade level, suggesting they may not be easily comprehensible to the average reader. There was a significant positive correlation between the referees\u2019 scores. Despite providing correct information in over 90% of the cases, the study highlights concerns regarding the potential for incomplete or misleading answers and the high readability level of the responses.",
                "Conclusion": "While ChatGPT-4 shows potential in delivering health information accurately, its utility is limited by the level of readability of its responses. The study underscores the need for continuous improvement in AI systems to ensure the delivery of information that is both accurate and accessible to patients with varying levels of health literacy. Healthcare professionals should supervise the use of such technologies to enhance patient education and care."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.3389/fsurg.2024.1373843",
                "pmid": "38903865",
                "pmc": "PMC11188327",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/38903865/"
            }
        },
        {
            "title": "Assessing parental comprehension of online resources on childhood pain",
            "abstract": {
                "title_content_0": "We aimed to examine the patient education materials (PEMs) on the internet about \u201cChild Pain\u201d in terms of readability, reliability, quality and content. For our observational study, a search was made on February 28, 2024, using the keywords \u201cChild Pain,\u201d \u201cPediatric Pain,\u201d and \u201cChildren Pain\u201d in the Google search engine. The readability of PEMs was assessed using computer-based readability formulas (Flesch Reading Ease Score [FRES], Flesch-Kincaid Grade Level [FKGL], Automated readability index (ARI), Gunning Fog [GFOG], Coleman-Liau score [CL], Linsear Write [LW], Simple Measure of Gobbledygook [SMOG]). The reliability and quality of websites were determined using the Journal of American Medical Association (JAMA) score, Global Quality Score (GQS), and DISCERN score. 96 PEM websites included in our study. We determined that the FRES was 64 (32\u201384), the FKGL was 8.24 (4.01\u201315.19), ARI was 8.95 (4.67\u201317.38), GFOG was 11 (7.1\u201319.2), CL was 10.1 (6.95\u201315.64), LW was 8.08 (3.94\u201319.0) and SMOG was 8.1 (4.98\u201313.93). The scores of readability formulas showed that, the readability level of PEMs was statistically higher than sixth-grade level with all formulas (P\u2005=\u2005.011 for FRES, P\u2005<\u2005.001 for GFOG, P\u2005<\u2005.001 for ARI, P\u2005<\u2005.001 for FKGL, P\u2005<\u2005.001 for CL and P\u2005<\u2005.001 for SMOG), except LW formula (P\u2005=\u2005.112). The websites had moderate-to-low reliability and quality. Health-related websites had the highest quality with JAMA score. We found a weak negative correlation between Blexb score and JAMA score (P\u2005=\u2005.013). Compared to the sixth-grade level recommended by the American Medical Association and the National Institutes of Health, the readability grade level of child pain-related internet-based PEMs is quite high. On the other hand, the reliability and quality of PEMs were determined as moderate-to-low. The low readability and quality of PEMs could cause an anxious parent and unnecessary hospital admissions. PEMs on issues threatening public health should be prepared with attention to the recommendations on readability."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1097/MD.0000000000038569",
                "pmid": "38905405",
                "pmc": "PMC11191864",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/38905405/"
            }
        },
        {
            "title": "Exploring the Role of ChatGPT-4, BingAI, and Gemini as Virtual Consultants to Educate Families about Retinopathy of Prematurity",
            "abstract": {
                "title_content_0": "Background: Large language models (LLMs) are becoming increasingly important as they are being used more frequently for providing medical information. Our aim is to evaluate the effectiveness of electronic artificial intelligence (AI) large language models (LLMs), such as ChatGPT-4, BingAI, and Gemini in responding to patient inquiries about retinopathy of prematurity (ROP). Methods: The answers of LLMs for fifty real-life patient inquiries were assessed using a 5-point Likert scale by three ophthalmologists. The models\u2019 responses were also evaluated for reliability with the DISCERN instrument and the EQIP framework, and for readability using the Flesch Reading Ease (FRE), Flesch-Kincaid Grade Level (FKGL), and Coleman-Liau Index. Results: ChatGPT-4 outperformed BingAI and Gemini, scoring the highest with 5 points in 90% (45 out of 50) and achieving ratings of \u201cagreed\u201d or \u201cstrongly agreed\u201d in 98% (49 out of 50) of responses. It led in accuracy and reliability with DISCERN and EQIP scores of 63 and 72.2, respectively. BingAI followed with scores of 53 and 61.1, while Gemini was noted for the best readability (FRE score of 39.1) but lower reliability scores. Statistically significant performance differences were observed particularly in the screening, diagnosis, and treatment categories. Conclusion: ChatGPT-4 excelled in providing detailed and reliable responses to ROP-related queries, although its texts were more complex. All models delivered generally accurate information as per DISCERN and EQIP assessments."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.3390/children11060750",
                "pmid": null,
                "pmc": "PMC11202218",
                "pub_year": "2024",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11202218/"
            }
        },
        {
            "title": "Large Language Models for Intraoperative Decision Support in Plastic Surgery: A Comparison between ChatGPT-4 and Gemini",
            "abstract": {
                "title_content_0": "Background and Objectives: Large language models (LLMs) are emerging as valuable tools in plastic surgery, potentially reducing surgeons\u2019 cognitive loads and improving patients\u2019 outcomes. This study aimed to assess and compare the current state of the two most common and readily available LLMs, Open AI\u2019s ChatGPT-4 and Google\u2019s Gemini Pro (1.0 Pro), in providing intraoperative decision support in plastic and reconstructive surgery procedures. Materials and Methods: We presented each LLM with 32 independent intraoperative scenarios spanning 5 procedures. We utilized a 5-point and a 3-point Likert scale for medical accuracy and relevance, respectively. We determined the readability of the responses using the Flesch\u2013Kincaid Grade Level (FKGL) and Flesch Reading Ease (FRE) score. Additionally, we measured the models\u2019 response time. We compared the performance using the Mann\u2013Whitney U test and Student\u2019s t-test. Results: ChatGPT-4 significantly outperformed Gemini in providing accurate (3.59 \u00b1 0.84 vs. 3.13 \u00b1 0.83, p-value = 0.022) and relevant (2.28 \u00b1 0.77 vs. 1.88 \u00b1 0.83, p-value = 0.032) responses. Alternatively, Gemini provided more concise and readable responses, with an average FKGL (12.80 \u00b1 1.56) significantly lower than ChatGPT-4\u2032s (15.00 \u00b1 1.89) (p < 0.0001). However, there was no difference in the FRE scores (p = 0.174). Moreover, Gemini\u2019s average response time was significantly faster (8.15 \u00b1 1.42 s) than ChatGPT\u2019-4\u2032s (13.70 \u00b1 2.87 s) (p < 0.0001). Conclusions: Although ChatGPT-4 provided more accurate and relevant responses, both models demonstrated potential as intraoperative tools. Nevertheless, their performance inconsistency across the different procedures underscores the need for further training and optimization to ensure their reliability as intraoperative decision-support tools."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.3390/medicina60060957",
                "pmid": null,
                "pmc": "PMC11205293",
                "pub_year": "2024",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11205293/"
            }
        },
        {
            "title": "Assessing ChatGPT Ability to Answer Frequently Asked Questions About Essential Tremor",
            "abstract": {
                "Background:": "Large-language models (LLMs) driven by artificial intelligence allow people to engage in direct conversations about their health. The accuracy and readability of the answers provided by ChatGPT, the most famous LLM, about Essential Tremor (ET), one of the commonest movement disorders, have not yet been evaluated.",
                "Methods:": "Answers given by ChatGPT to 10 questions about ET were evaluated by 5 professionals and 15 laypeople with a score ranging from 1 (poor) to 5 (excellent) in terms of clarity, relevance, accuracy (only for professionals), comprehensiveness, and overall value of the response. We further calculated the readability of the answers.",
                "Results:": "ChatGPT answers received relatively positive evaluations, with median scores ranging between 4 and 5, by both groups and independently from the type of question. However, there was only moderate agreement between raters, especially in the group of professionals. Moreover, readability levels were poor for all examined answers.",
                "Discussion:": "ChatGPT provided relatively accurate and relevant answers, with some variability as judged by the group of professionals suggesting that the degree of literacy about ET has influenced the ratings and, indirectly, that the quality of information provided in clinical practice is also variable. Moreover, the readability of the answer provided by ChatGPT was found to be poor. LLMs will likely play a significant role in the future; therefore, health-related content generated by these tools should be monitored."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.5334/tohm.917",
                "pmid": null,
                "pmc": "PMC11225576",
                "pub_year": "2024",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11225576/"
            }
        },
        {
            "title": "Developing Medication Reviews to Improve the Aruban Healthcare System: A Mixed-Methods Pilot Study",
            "abstract": {
                "title_content_0": "This study investigated whether and how medication reviews (MRs) conducted by pharmacists and general practitioners (GPs) with patient involvement can be performed on the island of Aruba (Dutch Caribbean). In this mixed-methods pilot study (both qualitative and quantitative), constructive and observational methodologies were combined. Healthcare providers\u2019 and patients\u2019 views on MRs and aspects of Aruban healthcare and culture relevant to MRs were examined. These insights were used to develop a protocol for conducting and implementing MRs in Aruba. Surveys were distributed and semi-structured interviews were held among Aruban community pharmacists and GPs, and a pilot program was created in which MRs were carried out with four Aruban patients and their GPs. According to the included healthcare providers, the main purpose of MRs is to optimize the patient experience and achieve concordance. Even though pharmacists and GPs consider their partnership equal, they have different views as to who should bear which responsibility in the MR process in matters regarding patient selection and follow-up. Common Aruban themes that were mentioned by the healthcare providers and deemed relevant for conducting MRs included behaviour/culture, healthcare, lifestyle, and therapy compliance. Anamnesis should be concise during the MR, and questions about medication storage, concerns, beliefs, and practical problems, as well as checks for limited health literacy, were considered important. In the pilot, at least three to, maximally, eight pharmacotherapy-related problems (PRPs) were detected per MR consultation, such as an incorrect dosage of acetylsalicylic acid, an inappropriate combination tablet for blood pressure regulation, and the absence of important laboratory values. All patients considered their consultation to be positive and of added value. In addition, it was observed that an MR can potentially generate cost savings. The information obtained from the healthcare providers and patients, together with the basic principles for MRs, as applied in the Netherlands, led to a definitive and promising MR format with practical recommendations for community pharmacists in Aruba: in comparison with the Dutch MR approach, GPs and pharmacists in Aruba could collaborate more on patient selection for MRs and their follow-up, because of their specific knowledge regarding the medications patients are taking chronically (pharmacists), and possible low levels of health literacy (GPs). Taking into account the Aruban culture, pharmacists could ask extra questions during MRs, referring to lifestyle (high prevalence of obesity), readability of medication labels (limited literacy), and herbal product use (Latin American culture). GPs and medical specialists sometimes experience miscommunication regarding the prescription of medication, which means that pharmacists must carefully take into account possible duplicate medications or interactions."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.3390/pharmacy12040108",
                "pmid": null,
                "pmc": "PMC11270182",
                "pub_year": "2024",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11270182/"
            }
        },
        {
            "title": "Quality Assessment of Internet Information Regarding Periodontitis in Persons Living with HIV",
            "abstract": {
                "title_content_0": "The Internet is the most used source of HIV information second to information received from healthcare professionals. The aim of this study was to assess the quality of Internet information about periodontitis in people living with HIV (PLWH). An Internet search was performed on 18 April 2024 using the search terms \u201cPeriodontitis\u201d, \u201cPeriodontal disease\u201d, and \u201cGum disease\u201d in combination with \u201cHIV\u201d in the most popular search engines (Google\u2122, Bing\u2122, and YAHOO!\u00ae). The first 20 results from each search term engine were pooled for analysis. Quality was assessed by JAMA benchmarks. Readability was assessed using the Flesch reading ease score (FRES). Origin of the site, type of author, and information details were also recorded. The quality of Internet information about periodontitis in PLWH varied. The mean JAMA score was 2.81 (SD = 1.0). The websites were generally fairly difficult to read (mean FRES = 57.1, SD = 15.0). Most websites provided some advice about self-treatment of oral problems, accompanied by a strong recommendation to seek professional dental care. In conclusion, advanced reading skills on periodontitis in PLWH were required and quality features were mostly not provided. Therefore, healthcare professionals should be actively involved in developing high-quality information resources and direct patients to evidence-based materials on the Internet."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.3390/ijerph21070857",
                "pmid": null,
                "pmc": "PMC11276730",
                "pub_year": "2024",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11276730/"
            }
        },
        {
            "title": "Improving biomedical entity linking for complex entity mentions with LLM-based text simplification",
            "abstract": {
                "title": "Abstract",
                "title_content_0": "Large amounts of important medical information are captured in free-text documents in biomedical research and within healthcare systems, which can be made accessible through natural language processing (NLP). A key component in most biomedical NLP pipelines is entity linking, i.e. grounding textual mentions of named entities to a reference of medical concepts, usually derived from a terminology system, such as the Systematized Nomenclature of Medicine Clinical Terms. However, complex entity mentions, spanning multiple tokens, are notoriously hard to normalize due to the difficulty of finding appropriate candidate concepts. In this work, we propose an approach to preprocess such mentions for candidate generation, building upon recent advances in text simplification with generative large language models. We evaluate the feasibility of our method in the context of the entity linking track of the BioCreative VIII SympTEMIST shared task. We find that instructing the latest Generative Pre-trained Transformer model with a few-shot prompt for text simplification results in mention spans that are easier to normalize. Thus, we can improve recall during candidate generation by 2.9\u00a0percentage points compared to our baseline system, which achieved the best score in the original shared task evaluation. Furthermore, we show that this improvement in recall can be fully translated into top-1 accuracy through careful initialization of a subsequent reranking model. Our best system achieves an accuracy of 63.6% on the SympTEMIST test set. The proposed approach has been integrated into the open-source xMEN toolkit, which is available online via https://github.com/hpi-dhc/xmen."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1093/database/baae067",
                "pmid": null,
                "pmc": "PMC11281847",
                "pub_year": "2024",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11281847/"
            }
        },
        {
            "title": "Readability analysis of ChatGPT's responses on lung cancer",
            "abstract": {
                "title_content_0": "For common diseases such as lung cancer, patients often use the internet to obtain medical information. As a result of advances in artificial intelligence and large language models such as ChatGPT, patients and health professionals use these tools to obtain medical information. The aim of this study was to evaluate the readability of ChatGPT-generated responses with different readability scales in the context of lung cancer. The most common questions in the lung cancer section of Medscape\u00ae were reviewed, and questions on the definition, etiology, risk factors, diagnosis, treatment, and prognosis of lung cancer (both NSCLC and SCLC) were selected. A set of 80 questions were asked 10 times to ChatGPT via the OpenAI API. ChatGPT's responses were tested using various readability formulas. The mean Flesch Reading Ease, Flesch-Kincaid Grade Level, Gunning FOG Scale, SMOG Index, Automated Readability Index, Coleman-Liau Index, Linsear Write Formula, Dale-Chall Readability Score, and Spache Readability Formula scores are at a moderate level (mean and standard deviation: 40.52\u2009\u00b1\u20099.81, 12.56\u2009\u00b1\u20091.66, 13.63\u2009\u00b1\u20091.54, 14.61\u2009\u00b1\u20091.45, 15.04\u2009\u00b1\u20091.97, 14.24\u2009\u00b1\u20091.90, 11.96\u2009\u00b1\u20092.55, 10.03\u2009\u00b1\u20090.63 and 5.93\u2009\u00b1\u20090.50, respectively). The readability levels of the answers generated by ChatGPT are \"collage\" and above and are difficult to read. Perhaps in the near future, the ChatGPT can be programmed to produce responses that are appropriate for people of different educational and age groups."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1038/s41598-024-67293-2",
                "pmid": "39060365",
                "pmc": "PMC11282056",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/39060365/"
            }
        },
        {
            "title": "Using artificial intelligence to generate medical literature for urology patients: a comparison of three different large language models",
            "abstract": {
                "Purpose": "Large language models (LLMs) are a form of artificial intelligence (AI) that uses deep learning techniques to understand, summarize and generate content. The potential benefits of LLMs in healthcare is predicted to be immense. The objective of this study was to examine the quality of patient information leaflets (PILs) produced by 3 LLMs on urological topics.",
                "Methods": "Prompts were created to generate PILs from 3 LLMs: ChatGPT-4, PaLM 2 (Google Bard) and Llama 2 (Meta) across four urology topics (circumcision, nephrectomy, overactive bladder syndrome, and transurethral resection of the prostate). PILs were evaluated using a quality assessment checklist. PIL readability was assessed by the Average Reading Level Consensus Calculator.",
                "Results": "PILs generated by PaLM 2 had the highest overall average quality score (3.58), followed by Llama 2 (3.34) and ChatGPT-4 (3.08). PaLM 2 generated PILs were of the highest quality in all topics except TURP and was the only LLM to include images. Medical inaccuracies were present in all generated content including instances of significant error. Readability analysis identified PaLM 2 generated PILs as the simplest (age 14\u201315 average reading level). Llama 2 PILs were the most difficult (age 16\u201317 average).",
                "Conclusion": "While LLMs can generate PILs that may help reduce healthcare professional workload, generated content requires clinician input for accuracy and inclusion of health literacy aids, such as images. LLM-generated PILs were above the average reading level for adults, necessitating improvement in LLM algorithms and/or prompt design. How satisfied patients are to LLM-generated PILs remains to be evaluated."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1007/s00345-024-05146-3",
                "pmid": "39073590",
                "pmc": "PMC11286728",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/39073590/"
            }
        },
        {
            "title": "Readability of Online Patient Education Materials for the 10 Most Common Hand Conditions",
            "abstract": {
                "title_content_0": "Background",
                "title_content_1": "Millions of individuals every day turn to the internet for assistance in understanding their hand conditions and potential treatments. While online educational resources appear abundant, there are concerns about whether resources meet the readability recommendations agreed upon by the American Medical Association (AMA) and the National Institutes of Health (NIH). Identifying educational resources that are readable for the majority of patients could improve a patient's understanding of their medical condition, subsequently improving their health outcomes.",
                "title_content_2": "Methods",
                "title_content_3": "The readability of the top five websites for the 10 most common hand conditions was examined using the Flesch-Kincaid (FK) analysis, comprising the FK reading ease and FK grade level. The FK reading ease score is an indicator of how difficult a text is to comprehend, while the FK grade level score is the grade level an individual reading a particular text would need to fully understand the text.",
                "title_content_4": "Results",
                "title_content_5": "The average FK reading ease was 56.00, which correlates with \u201cfairly difficult (high school)\u201d. The average FK corresponded to an eighth-grade reading level, far above the sixth-grade reading level recommendation set by the AMA and NIH.",
                "title_content_6": "Conclusion",
                "title_content_7": "Patient education, satisfaction, and the patient-physician relationship can all be improved by providing patients with more readable educational materials. Our study shows there is an opportunity for drastic improvement in the readability of online educational materials. Guiding patients with effective search techniques, advocating for the creation of more readable materials, and having a better understanding of the health literacy barriers patients face will allow hand surgeons to provide more comprehensive care to patients."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.7759/cureus.63820",
                "pmid": "39099975",
                "pmc": "PMC11297599",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/39099975/"
            }
        },
        {
            "title": "An Evaluation of the Content Quality, Readability, and Reliability of Publicly Available Web-Based Information on Pneumothorax Surgery in Ireland",
            "abstract": {
                "title_content_0": "Introduction",
                "title_content_1": "The internet is increasingly the first port of call for patients introduced to new treatments. Unfortunately, many websites are of poor quality, thereby limiting patients\u2019 ability to make informed health decisions.\u00a0Within thoracic surgery, the treatment options for pneumothoraces may be less intuitive for patients to understand compared to procedures such as lobectomies and wedge resections. Therefore, patients must receive high-quality information to make informed treatment decisions.\u00a0No study to date has evaluated online information regarding pneumothorax surgery. Knowledge regarding the same may allow physicians to recommend appropriate websites to patients and supplement remaining knowledge gaps.",
                "title_content_2": "Objective",
                "title_content_3": "This study aims to evaluate the content, readability, and reliability of online information regarding pneumothorax surgery.",
                "title_content_4": "Methods",
                "title_content_5": "A total of 11 search terms including \"pneumothorax surgery,\"\u00a0\"pleurectomy,\"\u00a0and \"pleurodesis\"\u00a0were each entered into Google, Bing, and Yahoo. The top 20 websites found through each search were screened, yielding 660 websites.",
                "title_content_6": "Only free websites designed for patient consumption that provided information on pneumothorax surgery were included. This criterion excluded 581 websites, leaving 79 websites to be evaluated.",
                "title_content_7": "To evaluate website reliability, the Journal of American Medical Association (JAMA) and DISCERN benchmark criteria were applied.\u00a0To evaluate the readability, 10 standardized tools were utilized including the Flesch-Kincaid Reading Ease Score. To evaluate website content, a novel,\u00a0self-designed 10-part questionnaire was utilized to assess whether information deemed essential by the authors was included.\u00a0It evaluated whether websites comprehensively described the surgery process for patients, including pre- and post-operative care.\u00a0Website authorship and year of publication were also noted.",
                "title_content_8": "Results",
                "title_content_9": "The mean JAMA score was 1.69 \u00b1 1.29\u00a0out of 4, with only nine websites achieving all four reliability criteria.\u00a0The median readability score was 13.42 (IQR: 11.48-16.23), which corresponded to a 13th-14th\u00a0school grade standard. Only four websites were written at a sixth-grade reading level.\u00a0In the novel content questionnaire, 31.6% of websites (n = 25) did not mention any side effects of pneumothorax surgery. Similarly, 39.2% (n = 31) did not mention alternative treatment options.\u00a0There was no correlation between the date of website update and JAMA (r = 0.158, p = 0.123), DISCERN (r = 0.098, p = 0.341), or readability (r = 0.053, p = 0.606) scores.",
                "title_content_10": "Conclusion",
                "title_content_11": "Most websites were written above the sixth-grade reading level, as recommended by the US Department of Health and Human Services. Furthermore, the exclusion of essential information regarding pneumothorax surgery from websites highlights the current gaps in online information.\u00a0These findings emphasize the need to create\u00a0and disseminate\u00a0comprehensive, reliable websites on pneumothorax surgery that enable patients to make informed health decisions."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.7759/cureus.63800",
                "pmid": "39099997",
                "pmc": "PMC11297662",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/39099997/"
            }
        },
        {
            "title": "Assessing the Readability of Patient Education Materials on Cardiac Catheterization From Artificial Intelligence Chatbots: An Observational Cross-Sectional Study",
            "abstract": {
                "title_content_0": "Background: Artificial intelligence (AI) is a burgeoning new field that has increased in popularity over the past couple of years, coinciding with the public release of large language model (LLM)-driven chatbots. These chatbots, such as ChatGPT, can be engaged directly in conversation, allowing users to ask them questions or issue other commands. Since LLMs are trained on large amounts of text data, they can also answer questions reliably and factually, an ability that has allowed them to serve as a source for medical inquiries. This study seeks to assess the readability of patient education materials on cardiac catheterization across four of the most common chatbots: ChatGPT, Microsoft Copilot, Google Gemini, and Meta AI.",
                "title_content_1": "Methodology: A set of 10 questions regarding cardiac catheterization was developed using website-based patient education materials on the topic. We then asked these questions in consecutive order to\u00a0four of the most common chatbots: ChatGPT, Microsoft Copilot, Google Gemini, and Meta AI. The Flesch Reading Ease Score (FRES) was used to assess the readability score. Readability grade levels were assessed using six tools: Flesch-Kincaid Grade Level (FKGL), Gunning Fog Index (GFI), Coleman-Liau Index (CLI), Simple Measure of Gobbledygook (SMOG) Index, Automated Readability Index (ARI), and FORCAST Grade Level.",
                "title_content_2": "Results: The mean FRES across all four chatbots was 40.2, while overall mean grade levels for the four chatbots were 11.2, 13.7, 13.7, 13.3, 11.2, and 11.6 across the FKGL, GFI, CLI, SMOG, ARI, and FORCAST indices, respectively. Mean reading grade levels across the six tools were 14.8 for ChatGPT, 12.3 for Microsoft Copilot, 13.1 for Google Gemini, and 9.6 for Meta AI. Further, FRES values for the four chatbots were 31, 35.8, 36.4, and 57.7, respectively.",
                "title_content_3": "Conclusions: This study shows that AI chatbots are capable of providing answers to medical questions regarding cardiac catheterization. However, the responses across the four chatbots had overall mean reading grade levels at the 11th-13th-grade level, depending on the tool used. This means that the materials were at the high school and even college reading level, which far exceeds the recommended sixth-grade level for patient education materials. Further, there is significant variability in the readability levels provided by different chatbots as, across all six grade-level assessments, Meta AI had the lowest scores and\u00a0ChatGPT generally had the highest."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.7759/cureus.63865",
                "pmid": "39099896",
                "pmc": "PMC11297732",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/39099896/"
            }
        },
        {
            "title": "Blepharoplasty Online: Critical Analysis of Content and Patient Comprehensibility",
            "abstract": {
                "Introduction": "Patients frequently turn to online information for decision-making factors about aesthetic procedures. The quality of online medical content is an essential supplement to clinical education. These resources assist patients in understanding the risks, benefits, and appropriateness of their desired procedure. This study examines the breadth and readability of online blepharoplasty information, elucidating its educational utility.",
                "Methods": "A depersonalized Google search was conducted using the Startpage Search Engine, investigating key phrases, \u201cblepharoplasty decision making factors\u201d, \u201ceye lift decision making factors\u201d, and \u201ceyelid lift decision making factors\u201d. The first three pages of results for each search term, totaling 90 links were screened. Data were extracted for various decision-making factors, subspecialty, gender, and readability.",
                "Results": "Twenty-six websites met inclusion for analysis. Thirteen websites were plastic surgery based, five otolaryngology (ENT), five ophthalmology/oculoplastic, one oral-maxillofacial (OMFS), and two mixed-based practices. Most blepharoplasty webpages identified were that of private practice and male surgeons. Half were subspecialties other than plastic surgery. Thirteen common decision-making factors were identified. The most common factors addressed across all texts were recovery followed by cosmetic and functional goals. The least discussed were genetic factors. Average Readability exceeded the 12th grade. There were no significant differences in readability means among subspecialties.",
                "Conclusion": "This study examines the online blepharoplasty sphere among US-based practices providing clinical education to patients. No appreciable differences among gender, subspecialty, and readability on decision-making factors were found, highlighting a consistency among surgeons. Most websites fell short of readability standards, however, emphasizing a need for clearer information to patients.",
                "No Level Assigned": "This journal requires that authors assign a level of evidence to each submission to which Evidence-Based Medicine rankings are applicable. This excludes Review Articles, Book Reviews, and manuscripts that concern Basic Science, Animal Studies, Cadaver Studies, and Experimental Studies. For a full description of these Evidence-Based Medicine ratings, please refer to the Table of Contents or the online Instructions to Authors www.springer.com/00266."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1007/s00266-024-04083-1",
                "pmid": "38789805",
                "pmc": "PMC11300528",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/38789805/"
            }
        },
        {
            "title": "A pilot feasibility study comparing large language models in extracting key information from ICU patient text records from an Irish population",
            "abstract": {
                "Background": "Artificial intelligence, through improved data management and automated summarisation, has the potential to enhance intensive care unit (ICU) care. Large language models (LLMs) can interrogate and summarise large volumes of medical notes to create succinct discharge summaries. In this study, we aim to investigate the potential of LLMs to accurately and concisely synthesise ICU discharge summaries.",
                "Methods": "Anonymised clinical notes from ICU admissions were used to train and validate a prompting structure in three separate LLMs (ChatGPT, GPT-4 API and Llama 2) to generate concise clinical summaries. Summaries were adjudicated by staff intensivists on ability to identify and appropriately order a pre-defined list of important clinical events as well as readability, organisation, succinctness, and overall rank.",
                "Results": "In the development phase, text from five ICU episodes was used to develop a series of prompts to best capture clinical summaries. In the testing phase, a summary produced by each LLM from an additional six ICU episodes was utilised for evaluation. Overall ability to identify a pre-defined list of important clinical events in the summary was 41.5\u2009\u00b1\u200915.2% for GPT-4 API, 19.2\u2009\u00b1\u200920.9% for ChatGPT and 16.5\u2009\u00b1\u200914.1% for Llama2 (p\u2009=\u20090.002). GPT-4 API followed by ChatGPT had the highest score to appropriately order a pre-defined list of important clinical events in the summary as well as readability, organisation, succinctness, and overall rank, whilst Llama2 scored lowest for all. GPT-4 API produced minor hallucinations, which were not present in the other models.",
                "Conclusion": "Differences exist in large language model performance in readability, organisation, succinctness, and sequencing of clinical events compared to others. All encountered issues with narrative coherence and omitted key clinical data and only moderately captured all clinically meaningful data in the correct order. However, these technologies suggest future potential for creating succinct discharge summaries.",
                "Supplementary Information": "The online version contains supplementary material available at 10.1186/s40635-024-00656-1."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1186/s40635-024-00656-1",
                "pmid": "39147878",
                "pmc": "PMC11327225",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/39147878/"
            }
        },
        {
            "title": "Content Analysis of Arabic Websites As Patient Resources for Osteoporosis",
            "abstract": {
                "title_content_0": "Background: Osteoporosis is a prevalent metabolic bone disease in the Middle East. Middle Easterners rely on the Internet as a source of information about osteoporosis and its treatment. Adequate awareness can help to prevent osteoporosis and its complications. Websites covering osteoporosis in Arabic must be of good quality and readability to be beneficial for people in the Middle East.",
                "title_content_1": "Methods: Two Arabic terms for osteoporosis were searched on Google.com (Google Inc., Mountainview, CA), and the first 100 results for each term were examined for eligibility. Two independent raters evaluated the websites using DISCERN\u00a0and the Journal of the American Medical Association (JAMA) criteria for quality and reliability. The Flesch Kincaid grade level (FKGL), Simple Measure of Gobbledygook (SMOG), and Flesch Reading Ease (FRE) scale were used to evaluate the readability of each website\u2019s content.",
                "title_content_2": "Results: Twenty-five websites were included and evaluated in our study. The average DISCERN\u00a0score was 28.36\u00b112.18 out of 80 possible scores. The average JAMA score was 1.05\u00b11.15 out of four total scores. The readability scores of all websites were, on average, 50.71\u00b121.96 on the FRE scale, 9.25\u00b14.89 on the FKGL, and 9.74\u00b12.94 on the SMOG. There was a significant difference (p = 0.026 and 0.044) in the DISCERN\u00a0and JAMA scores, respectively, between the websites on the first Google page and the websites seen on later pages.",
                "title_content_3": "Conclusion: The study found Arabic websites covering osteoporosis to be of low quality and difficult readability. Because these websites are a major source for patient education, improving their quality and readability is a must. The use of simpler language is needed, as is covering more aspects of the diseases, such as prevention."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.7759/cureus.64880",
                "pmid": "39156464",
                "pmc": "PMC11330570",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/39156464/"
            }
        },
        {
            "title": "A comparison of the diagnostic ability of large language models in challenging clinical cases",
            "abstract": {
                "Introduction": "The rise of accessible, consumer facing large language models (LLM) provides an opportunity for immediate diagnostic support for clinicians.",
                "Objectives": "To compare the different performance characteristics of common LLMS utility in solving complex clinical cases and assess the utility of a novel tool to grade LLM output.",
                "Methods": "Using a newly developed rubric to assess the models\u2019 diagnostic utility, we measured to models\u2019 ability to answer cases according to accuracy, readability, clinical interpretability, and an assessment of safety. Here we present a comparative analysis of three LLM models\u2014Bing, Chat GPT, and Gemini\u2014across a diverse set of clinical cases as presented in the New England Journal of Medicines case series.",
                "Results": "Our results suggest that models performed differently when presented with identical clinical information, with Gemini performing best. Our grading tool had low interobserver variability and proved a reliable tool to grade LLM clinical output.",
                "Conclusion": "This research underscores the variation in model performance in clinical scenarios and highlights the importance of considering diagnostic model performance in diverse clinical scenarios prior to deployment. Furthermore, we provide a new tool to assess LLM output."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.3389/frai.2024.1379297",
                "pmid": "39161790",
                "pmc": "PMC11330891",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/39161790/"
            }
        },
        {
            "title": "Open Brain AI and language assessment",
            "abstract": {
                "title_content_0": "Neurolinguistic assessments play a vital role in neurological examinations, revealing a wide range of language and communication impairments associated with developmental disorders and acquired neurological conditions. Yet, a thorough neurolinguistic assessment is time-consuming and laborious and takes valuable resources from other tasks. To empower clinicians, healthcare providers, and researchers, we have developed Open Brain AI (OBAI). The aim of this computational platform is twofold. First, it aims to provide advanced AI tools to facilitate spoken and written language analysis, automate the analysis process, and reduce the workload associated with time-consuming tasks. The platform currently incorporates multilingual tools for English, Danish, Dutch, Finnish, French, German, Greek, Italian, Norwegian, Polish, Portuguese, Romanian, Russian, Spanish, and Swedish. The tools involve models for (i) audio transcription, (ii) automatic translation, (iii) grammar error correction, (iv) transcription to the International Phonetic Alphabet, (v) readability scoring, (vi) phonology, morphology, syntax, semantic measures (e.g., counts and proportions), and lexical measures. Second, it aims to support clinicians in conducting their research and automating everyday tasks with \u201cOBAI Companion,\u201d an AI language assistant that facilitates language processing, such as structuring, summarizing, and editing texts. OBAI also provides tools for automating spelling and phonology scoring. This paper reviews OBAI\u2019s underlying architectures and applications and shows how OBAI can help professionals focus on higher-value activities, such as therapeutic interventions."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.3389/fnhum.2024.1421435",
                "pmid": "39165904",
                "pmc": "PMC11333242",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/39165904/"
            }
        },
        {
            "title": "Improving accuracy of GPT-3/4 results on biomedical data using a retrieval-augmented language model",
            "abstract": {
                "title_content_0": "Large language models (LLMs) have made a significant impact on the fields of general artificial intelligence. General purpose LLMs exhibit strong logic and reasoning skills and general world knowledge but can sometimes generate misleading results when prompted on specific subject areas. LLMs trained with domain-specific knowledge can reduce the generation of misleading information (i.e. hallucinations) and enhance the precision of LLMs in specialized contexts. Training new LLMs on specific corpora however can be resource intensive. Here we explored the use of a retrieval-augmented generation (RAG) model which we tested on literature specific to a biomedical research area. OpenAI\u2019s GPT-3.5, GPT-4, Microsoft\u2019s Prometheus, and a custom RAG model were used to answer 19 questions pertaining to diffuse large B-cell lymphoma (DLBCL) disease biology and treatment. Eight independent reviewers assessed LLM responses based on accuracy, relevance, and readability, rating responses on a 3-point scale for each category. These scores were then used to compare LLM performance. The performance of the LLMs varied across scoring categories. On accuracy and relevance, the RAG model outperformed other models with higher scores on average and the most top scores across questions. GPT-4 was more comparable to the RAG model on relevance versus accuracy. By the same measures, GPT-4 and GPT-3.5 had the highest scores for readability of answers when compared to the other LLMs. GPT-4 and 3.5 also had more answers with hallucinations than the other LLMs, due to non-existent references and inaccurate responses to clinical questions. Our findings suggest that an oncology research-focused RAG model may outperform general-purpose LLMs in accuracy and relevance when answering subject-related questions. This framework can be tailored to Q&A in other subject areas. Further research will help understand the impact of LLM architectures, RAG methodologies, and prompting techniques in answering questions across different subject areas."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1371/journal.pdig.0000568",
                "pmid": "39167594",
                "pmc": "PMC11338460",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/39167594/"
            }
        },
        {
            "title": "Comparative Analysis of Accuracy, Readability, Sentiment, and Actionability: Artificial Intelligence Chatbots (ChatGPT and Google Gemini) versus Traditional Patient Information Leaflets for Local Anesthesia in Eye Surgery",
            "abstract": {
                "Background and Aim:": "Eye surgeries often evoke strong negative emotions in patients, including fear and anxiety. Patient education material plays a crucial role in informing and empowering individuals. Traditional sources of medical information may not effectively address individual patient concerns or cater to varying levels of understanding. This study aims to conduct a comparative analysis of the accuracy, completeness, readability, tone, and understandability of patient education material generated by AI chatbots versus traditional Patient Information Leaflets (PILs), focusing on local anesthesia in eye surgery.",
                "Methods:": "Expert reviewers evaluated responses generated by AI chatbots (ChatGPT and Google Gemini) and a traditional PIL (Royal College of Anaesthetists\u2019 PIL) based on accuracy, completeness, readability, sentiment, and understandability. Statistical analyses, including ANOVA and Tukey HSD tests, were conducted to compare the performance of the sources.",
                "Results:": "Readability analysis showed variations in complexity among the sources, with AI chatbots offering simplified language and PILs maintaining better overall readability and accessibility. Sentiment analysis revealed differences in emotional tone, with Google Gemini exhibiting the most positive sentiment. AI chatbots demonstrated superior understandability and actionability, while PILs excelled in completeness. Overall, ChatGPT showed slightly higher accuracy (scores expressed as mean \u00b1 standard deviation) (4.71 \u00b1 0.5 vs 4.61 \u00b1 0.62) and completeness (4.55 \u00b1 0.58 vs 4.47 \u00b1 0.58) compared to Google Gemini, but PILs performed best (4.84 \u00b1 0.37 vs 4.88 \u00b1 0.33) in terms of both accuracy and completeness (p-value for completeness <0.05).",
                "Conclusion:": "AI chatbots show promise as innovative tools for patient education, complementing traditional PILs. By leveraging the strengths of both AI-driven technologies and human expertise, healthcare providers can enhance patient education and empower individuals to make informed decisions about their health and medical care."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.22599/bioj.377",
                "pmid": null,
                "pmc": "PMC11342839",
                "pub_year": "2024",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11342839/"
            }
        },
        {
            "title": "Evaluation of Generative Language Models in Personalizing Medical Information: Instrument Validation Study",
            "abstract": {
                "Background": "Although uncertainties exist regarding implementation, artificial intelligence\u2013driven generative language models (GLMs) have enormous potential in medicine. Deployment of GLMs could improve patient comprehension of clinical texts and improve low health literacy.",
                "Objective": "The goal of this study is to evaluate the potential of ChatGPT-3.5 and GPT-4 to tailor the complexity of medical information to patient-specific input education level, which is crucial if it is to serve as a tool in addressing low health literacy.",
                "Methods": "Input templates related to 2 prevalent chronic diseases\u2014type II diabetes and hypertension\u2014were designed. Each clinical vignette was adjusted for hypothetical patient education levels to evaluate output personalization. To assess the success of a GLM (GPT-3.5 and GPT-4) in tailoring output writing, the readability of pre- and posttransformation outputs were quantified using the Flesch reading ease score (FKRE) and the Flesch-Kincaid grade level (FKGL).",
                "Results": "Responses (n=80) were generated using GPT-3.5 and GPT-4 across 2 clinical vignettes. For GPT-3.5, FKRE means were 57.75 (SD 4.75), 51.28 (SD 5.14), 32.28 (SD 4.52), and 28.31 (SD 5.22) for 6th grade, 8th grade, high school, and bachelor\u2019s, respectively; FKGL mean scores were 9.08 (SD 0.90), 10.27 (SD 1.06), 13.4 (SD 0.80), and 13.74 (SD 1.18). GPT-3.5 only aligned with the prespecified education levels at the bachelor\u2019s degree. Conversely, GPT-4\u2019s FKRE mean scores were 74.54 (SD 2.6), 71.25 (SD 4.96), 47.61 (SD 6.13), and 13.71 (SD 5.77), with FKGL mean scores of 6.3 (SD 0.73), 6.7 (SD 1.11), 11.09 (SD 1.26), and 17.03 (SD 1.11) for the same respective education levels. GPT-4 met the target readability for all groups except the 6th-grade FKRE average. Both GLMs produced outputs with statistically significant differences (P<.001; 8th grade P<.001; high school P<.001; bachelors P=.003; FKGL: 6th grade P=.001; 8th grade P<.001; high school P<.001; bachelors P<.001) between mean FKRE and FKGL across input education levels.",
                "Conclusions": "GLMs can change the structure and readability of medical text outputs according to input-specified education. However, GLMs categorize input education designation into 3 broad tiers of output readability: easy (6th and 8th grade), medium (high school), and difficult (bachelor\u2019s degree). This is the first result to suggest that there are broader boundaries in the success of GLMs in output text simplification. Future research must establish how GLMs can reliably personalize medical texts to prespecified education levels to enable a broader impact on health care literacy."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.2196/54371",
                "pmid": "39137416",
                "pmc": "PMC11350306",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/39137416/"
            }
        },
        {
            "title": "Evaluation of the Quality and Readability of Web-Based Information Regarding Foreign Bodies of the Ear, Nose, and Throat: Qualitative Content Analysis",
            "abstract": {
                "Background": "Foreign body (FB) inhalation, ingestion, and insertion account for 11% of emergency admissions for ear, nose, and throat conditions. Children are disproportionately affected, and urgent intervention may be needed to maintain airway patency and prevent blood vessel occlusion. High-quality, readable online information could help reduce poor outcomes from FBs.",
                "Objective": "We aim to evaluate the quality and readability of available online health information relating to FBs.",
                "Methods": "In total, 6 search phrases were queried using the Google search engine. For each search term, the first 30 results were captured. Websites in the English language and displaying health information were included. The provider and country of origin were recorded. The modified 36-item Ensuring Quality Information for Patients tool was used to assess information quality. Readability was assessed using a combination of tools: Flesch Reading Ease score, Flesch-Kincaid Grade Level, Gunning-Fog Index, and Simple Measure of Gobbledygook.",
                "Results": "After the removal of duplicates, 73 websites were assessed, with the majority originating from the United States (n=46, 63%). Overall, the quality of the content was of moderate quality, with a median Ensuring Quality Information for Patients score of 21 (IQR 18-25, maximum 29) out of a maximum possible score of 36. Precautionary measures were not mentioned on 41% (n=30) of websites and 30% (n=22) did not identify disk batteries as a risky FB. Red flags necessitating urgent care were identified on 95% (n=69) of websites, with 89% (n=65) advising patients to seek medical attention and 38% (n=28) advising on safe FB removal. Readability scores (Flesch Reading Ease score=12.4, Flesch-Kincaid Grade Level=6.2, Gunning-Fog Index=6.5, and Simple Measure of Gobbledygook=5.9 years) showed most websites (56%) were below the recommended sixth-grade level.",
                "Conclusions": "The current quality and readability of information regarding FBs is inadequate. More than half of the websites were above the recommended sixth-grade reading level, and important information regarding high-risk FBs such as disk batteries and magnets was frequently excluded. Strategies should be developed to improve access to high-quality information that informs patients and parents about risks and when to seek medical help. Strategies to promote high-quality websites in search results also have the potential to improve outcomes."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.2196/55535",
                "pmid": "39145998",
                "pmc": "PMC11362703",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/39145998/"
            }
        },
        {
            "title": "Refining the Universal, School-Based OurFutures Mental Health Program to Be Trauma Informed, Gender and Sexuality Diversity Affirmative, and Adherent to Proportionate Universalism: Mixed Methods Participatory Design Process",
            "abstract": {
                "Background": "Mental disorders are the leading cause of disease burden among youth. Effective prevention of mental disorders during adolescence is a critical public health strategy to reduce both individual and societal harms. Schools are an important setting for prevention; however, existing universal school-based mental health interventions have shown null, and occasionally iatrogenic, effects in preventing symptoms of common disorders, such as depression and anxiety.",
                "Objective": "This study aims to report the adaptation process of an established, universal, school-based prevention program for depression and anxiety, OurFutures Mental Health. Using a 4-stage process; triangulating quantitative, qualitative, and evidence syntheses; and centering the voices of young people, the revised program is trauma-informed; lesbian, gay, bisexual, transgender, nonbinary, queer, questioning, and otherwise gender and sexuality diverse (LGBTQA+) affirmative; relevant to contemporary youth; and designed to tailor intervention dosage to those who need it most (proportionate universalism).",
                "Methods": "Program adaptation occurred from April 2022 to July 2023 and involved 4 stages. Stage 1 comprised mixed methods analysis of student evaluation data (n=762; mean age 13.5, SD 0.62 y), collected immediately after delivering the OurFutures Mental Health program in a previous trial. Stage 2 consisted of 3 focus groups with high school students (n=39); regular meetings with a purpose-built, 8-member LGBTQA+ youth advisory committee; and 2 individual semistructured, in-depth interviews with LGBTQA+ young people via Zoom (Zoom Video Communications) or WhatsApp (Meta) text message. Stage 3 involved a clinical psychologist providing an in-depth review of all program materials with the view of enhancing readability, improving utility, and normalizing emotions while retaining key cognitive behavioral therapy elements. Finally, stage 4 involved fortnightly consultations among researchers and clinicians on the intervention adaptation, drawing on the latest evidence from existing literature in school-based prevention interventions, trauma-informed practice, and adolescent mental health.",
                "Results": "Drawing on feedback from youth, clinical psychologists, and expert youth mental health researchers, sourced from stages 1 to 4, a series of adaptations were made to the storylines, characters, and delivery of therapeutic content contained in the weekly manualized program content, classroom activities, and weekly student and teacher lesson summaries.",
                "Conclusions": "The updated OurFutures Mental Health program is a trauma-informed, LBGTQA+ affirmative program aligned with the principles of proportionate universalism. The program adaptation responds to recent mixed findings on universal school-based mental health prevention programs, which include null, small beneficial, and small iatrogenic effects. The efficacy of the refined OurFutures Mental Health program is currently being tested through a cluster randomized controlled trial with up to 1400 students in 14 schools across Australia. It is hoped that the refined program will advance the current stalemate in universal school-based prevention of common mental disorders and ultimately improve the mental health and well-being of young people in schools."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.2196/54637",
                "pmid": "39167794",
                "pmc": "PMC11375394",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/39167794/"
            }
        },
        {
            "title": "Assessment of the Quality and Readability of Web-Based Arabic Health Information on Halitosis: Infodemiological Study",
            "abstract": {
                "Background": "Halitosis, characterized by an undesirable mouth odor, represents a common concern.",
                "Objective": "This study aims to assess the quality and readability of web-based Arabic health information on halitosis as the internet is becoming a prominent global source of medical information.",
                "Methods": "A total of 300 Arabic websites were retrieved from Google using 3 commonly used phrases for halitosis in Arabic. The quality of the websites was assessed using benchmark criteria established by the Journal of the American Medical Association, the DISCERN tool, and the presence of the Health on the Net Foundation Code of Conduct (HONcode). The assessment of readability (Flesch Reading Ease [FRE], Simple Measure of Gobbledygook, and Flesch-Kincaid Grade Level [FKGL]) was conducted using web-based readability indexes.",
                "Results": "A total of 127 websites were examined. Regarding quality assessment, 87.4% (n=111) of websites failed to fulfill any Journal of the American Medical Association requirements, highlighting a lack of authorship (authors\u2019 contributions), attribution (references), disclosure (sponsorship), and currency (publication date). The DISCERN tool had a mean score of 34.55 (SD 7.46), with the majority (n=72, 56.6%) rated as moderate quality, 43.3% (n=55) as having a low score, and none receiving a high DISCERN score, indicating a general inadequacy in providing quality health information to make decisions and treatment choices. No website had HONcode certification, emphasizing the concern over the credibility and trustworthiness of these resources. Regarding readability assessment, Arabic halitosis websites had high readability scores, with 90.5% (n=115) receiving an FRE score \u226580, 98.4% (n=125) receiving a Simple Measure of Gobbledygook score <7, and 67.7% (n=86) receiving an FKGL score <7. There were significant correlations between the DISCERN scores and the quantity of words (P<.001) and sentences (P<.001) on the websites. Additionally, there was a significant relationship (P<.001) between the number of sentences and FKGL and FRE scores.",
                "Conclusions": "While readability was found to be very good, indicating that the information is accessible to the public, the quality of Arabic halitosis websites was poor, reflecting a significant gap in providing reliable and comprehensive health information. This highlights the need for improving the availability of high-quality materials to ensure Arabic-speaking populations have access to reliable information about halitosis and its treatment options, tying quality and availability together as critical for effective health communication."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.2196/54072",
                "pmid": "39196637",
                "pmc": "PMC11391154",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/39196637/"
            }
        },
        {
            "title": "Thematic coverage and readability of online patient information on cochlear implant care",
            "abstract": {
                "Introduction": "The treatment of patients with a cochlear implant (CI) is usually an elective, complex and interdisciplinary process. As an important source of information, patients often access the internet prior to treatment. The quality of internet-based information regarding thematic coverage has not yet been analysed in detail. Therefore, the aim of this study was to analyse the information on CI care available on the internet regarding its thematic coverage and readability.",
                "Material methods": "Eight search phrases related to CI care were defined as part of the study. A checklist for completeness of thematic coverage was then created for each search phrase. The current German CI clinical practice guideline and the white paper on CI care in Germany were used as a basis. As a further parameter, readability was assessed using Flesch Reading Ease Scores. The search phrases were used for an internet search with Google. The first ten results were then analysed with regard to thematic coverage, readability and the provider of the website.",
                "Results": "A total of 80 websites were identified, which were set up by 54 different providers (16 providers were found in multiple entries) from eight different provider groups. The average completeness of thematic coverage was 41.6\u2009\u00b1\u200928.2%. Readability according to the Flesch Reading Ease Score was categorised as \"hard to read\" on average (34.7\u2009\u00b1\u200914.2 points, range: 0\u201372). There was a negative statistically significant correlation between the thematic coverage of content and readability (Spearman's rank correlation: r\u2009= \u2212\u00a00.413, p\u2009=\u20090.00014).",
                "Summary": "The completeness of thematic coverage of information on CI care available on the internet was highly heterogeneous and had a significant negative correlation with the\u00a0readability. This result should be taken into account by both the providers of internet information and by patients when using internet-based information on CI care and help to further improve the quality of web-based information.",
                "Supplementary Information": "The online version contains supplementary material available at 10.1007/s00405-024-08694-x."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1007/s00405-024-08694-x",
                "pmid": "38705897",
                "pmc": "PMC11392990",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/38705897/"
            }
        },
        {
            "title": "Online Patient Education in Obstructive Sleep Apnea: ChatGPT versus Google Search",
            "abstract": {
                "title_content_0": "The widespread implementation of artificial intelligence technologies provides an appealing alternative to traditional search engines for online patient healthcare education. This study assessed ChatGPT-3.5\u2019s capabilities as a source of obstructive sleep apnea (OSA) information, using Google Search as a comparison. Ten frequently searched questions related to OSA were entered into Google Search and ChatGPT-3.5. The responses were assessed by two independent researchers using the Global Quality Score (GQS), Patient Education Materials Assessment Tool (PEMAT), DISCERN instrument, CLEAR tool, and readability scores (Flesch Reading Ease and Flesch\u2013Kincaid Grade Level). ChatGPT-3.5 significantly outperformed Google Search in terms of GQS (5.00 vs. 2.50, p < 0.0001), DISCERN reliability (35.00 vs. 29.50, p = 0.001), and quality (11.50 vs. 7.00, p = 0.02). The CLEAR tool scores indicated that ChatGPT-3.5 provided excellent content (25.00 vs. 15.50, p < 0.001). PEMAT scores showed higher understandability (60\u201391% vs. 44\u201380%) and actionability for ChatGPT-3.5 (0\u201340% vs. 0%). Readability analysis revealed that Google Search responses were easier to read (FRE: 56.05 vs. 22.00; FKGL: 9.00 vs. 14.00, p < 0.0001). ChatGPT-3.5 delivers higher quality and more comprehensive OSA information compared to Google Search, although its responses are less readable. This suggests that while ChatGPT-3.5 can be a valuable tool for patient education, efforts to improve readability are necessary to ensure accessibility and utility for all patients. Healthcare providers should be aware of the strengths and weaknesses of various healthcare information resources and emphasize the importance of critically evaluating online health information, advising patients on its reliability and relevance."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.3390/healthcare12171781",
                "pmid": null,
                "pmc": "PMC11394980",
                "pub_year": "2024",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11394980/"
            }
        },
        {
            "title": "Comparison of ChatGPT versions in informing patients with rotator cuff injuries",
            "abstract": {
                "Background": "The aim of this study is to evaluate whether Chat Generative Pretrained Transformer (ChatGPT) can be recommended as a resource for informing patients planning rotator cuff repairs, and to assess the differences between ChatGPT 3.5 and 4.0 versions in terms of information content and readability.",
                "Methods": "In August 2023, 13 commonly asked questions by patients with rotator cuff disease were posed to ChatGPT 3.5 and ChatGPT 4 programs using different internet protocol computers by 3 experienced surgeons in rotator cuff surgery. After converting the answers of both versions into text, the quality and readability of the answers were examined.",
                "Results": "The average Journal of the American Medical Association score for both versions was 0, and the average DISCERN score was 61.6. A statistically significant and strong correlation was found between ChatGPT 3.5 and 4.0 DISCERN scores. There was excellent agreement in DISCERN scores for both versions among the 3 evaluators. ChatGPT 3.5 was found to be less readable than ChatGPT 4.0.",
                "Conclusion": "The information provided by the ChatGPT conversational system was evaluated as of high quality, but there were significant shortcomings in terms of reliability due to the lack of citations. Despite the ChatGPT 4.0 version having higher readability scores, both versions were considered difficult to read."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1016/j.jseint.2024.04.016",
                "pmid": null,
                "pmc": "PMC11401580",
                "pub_year": "2024",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11401580/"
            }
        },
        {
            "title": "Evaluating the impacts of digital ECG denoising on the interpretive capabilities of healthcare professionals",
            "abstract": {
                "title": "Abstract",
                "Aims": "Electrocardiogram (ECG) interpretation is an essential skill across multiple medical disciplines; yet, studies have consistently identified deficiencies in the interpretive performance of healthcare professionals linked to a variety of educational and technological factors. Despite the established correlation between noise interference and erroneous diagnoses, research evaluating the impacts of digital denoising software on clinical ECG interpretation proficiency is lacking.",
                "Methods and results": "Forty-eight participants from a variety of medical professions and experience levels were prospectively recruited for this study. Participants\u2019 capabilities in classifying common cardiac rhythms were evaluated using a sequential blinded and semi-blinded interpretation protocol on a challenging set of single-lead ECG signals (42 \u00d7 10\u2005s) pre- and post-denoising with robust, cloud-based ECG processing software. Participants\u2019 ECG rhythm interpretation performance was greatest when raw and denoised signals were viewed in a combined format that enabled comparative evaluation. The combined view resulted in a 4.9% increase in mean rhythm classification accuracy (raw: 75.7% \u00b1 14.5% vs. combined: 80.6% \u00b1 12.5%, P = 0.0087), a 6.2% improvement in mean five-point graded confidence score (raw: 4.05 \u00b1 0.58 vs. combined: 4.30 \u00b1 0.48, P < 0.001), and 9.7% reduction in the mean proportion of undiagnosable data (raw: 14.2% \u00b1 8.2% vs. combined: 4.5% \u00b1 2.4%, P < 0.001), relative to raw signals alone. Participants also had a predominantly positive perception of denoising as it related to revealing previously unseen pathologies, improving ECG readability, and reducing time to diagnosis.",
                "Conclusion": "Our findings have demonstrated that digital denoising software improves the efficacy of rhythm interpretation on single-lead ECGs, particularly when raw and denoised signals are provided in a combined viewing format, warranting further investigation into the impact of such technology on clinical decision-making and patient outcomes."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1093/ehjdh/ztae063",
                "pmid": null,
                "pmc": "PMC11417490",
                "pub_year": "2024",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11417490/"
            }
        },
        {
            "title": "Accuracy and Readability of Artificial Intelligence Chatbot Responses to Vasectomy-Related Questions: Public Beware",
            "abstract": {
                "title_content_0": "Purpose",
                "title_content_1": "Artificial intelligence (AI) has rapidly gained popularity with the growth of ChatGPT (OpenAI, San Francisco, USA) and other large-language model chatbots, and these programs have tremendous potential to impact medicine. One important area of consequence in medicine and public health is that patients may use these programs in search of answers to medical questions. Despite the increased utilization of AI chatbots by the public, there is little research to assess the reliability of ChatGPT and alternative programs when queried for medical information. This study seeks to elucidate the accuracy and readability of AI chatbots in answering patient questions regarding urology. As vasectomy is one of the most common urologic procedures, this study investigates AI-generated responses to frequently asked vasectomy-related questions. For this study, five popular and free-to-access AI platforms were utilized to undertake this investigation.",
                "title_content_2": "Methods",
                "title_content_3": "Fifteen vasectomy-related questions were individually queried to five AI chatbots from November-December 2023: ChatGPT (OpenAI, San Francisco, USA), Bard (Google Inc., Mountainview, USA) Bing (Microsoft, Redmond, USA) Perplexity (Perplexity AI Inc., San Francisco, USA), and Claude (Anthropic, San Francisco, USA). Responses from each platform were graded by two attending urologists, two urology research faculty, and one urological resident physician using a Likert (1-6) scale: (1-completely inaccurate, 6-completely accurate) based on comparison to existing American Urological Association guidelines. Flesch-Kincaid Grade levels (FKGL) and Flesch Reading Ease scores (FRES) (1-100) were calculated for each response. To assess differences in Likert, FRES, and FKGL, Kruskal-Wallis tests were performed using GraphPad Prism V10.1.0 (GraphPad,\u00a0San Diego,\u00a0USA) with Alpha set at 0.05.",
                "title_content_4": "Results",
                "title_content_5": "Analysis shows that ChatGPT provided the most accurate responses across the five AI chatbots with an average score of 5.04 on the Likert scale. Subsequently, Microsoft Bing (4.91), Anthropic Claude (4.65), Google Bard (4.43), and Perplexity (4.41) followed. All five chatbots were found to score, on average, higher than 4.41 corresponding to a score of at least \"somewhat accurate.\" Google Bard received the highest Flesch Reading Ease score (49.67) and lowest Grade level (10.1) when compared to the other chatbots. Anthropic Claude scored 46.7 on the FRES and 10.55 on the FKGL. Microsoft Bing scored 45.57 on the FRES and 11.56 on the FKGL. Perplexity scored 36.4 on the FRES and 13.29 on the FKGL. ChatGPT had the lowest FRES of 30.4 and highest FKGL of 14.2.",
                "title_content_6": "Conclusion",
                "title_content_7": "This study investigates the use of AI in medicine, specifically urology, and it helps to determine whether large-language model chatbots can be reliable sources of freely available medical information. All five AI chatbots on average were able to achieve at least \u201csomewhat accurate\u201d on a 6-point Likert scale. In terms of readability, all five AI chatbots on average had Flesch Reading Ease scores of less than 50 and were higher than a 10th-grade level. In this small-scale study, there were several significant differences identified between the readability scores of each AI chatbot. However, there were no significant differences found among their accuracies. Thus, our study suggests that major AI chatbots may perform similarly in their ability to be correct but differ in their ease of being comprehended by the general public."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.7759/cureus.67996",
                "pmid": null,
                "pmc": "PMC11427961",
                "pub_year": "2024",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11427961/"
            }
        },
        {
            "title": "Unveiling readability challenges: An extensive analysis of consent document accessibility in clinical trials",
            "abstract": {
                "Background:": "Clinical research trials rely on informed consent forms (ICFs) to explain all aspects of the study to potential participants. Despite efforts to ensure the readability of ICFs, concerns about their complexity and participant understanding persist. There is a noted gap between Institutional Review Board (IRB) standards and the actual readability levels of ICFs, which often exceed the recommended 8th-grade reading level. This study evaluates the readability of over five thousand ICFs from ClinicalTrials.gov in the USA to assess their literacy levels.",
                "Methods:": "We analyzed 5,239 US-based ICFs from ClinicalTrials.gov using readability metrics such as the Flesch Reading Ease, Flesch-Kincaid Grade Level, Gunning Fog Index, and the percentage of difficult words. We examined trends in readability levels across studies initiated from 2005 to 2024.",
                "Results:": "Most ICFs exceeded the recommended 8th-grade reading level, with an average Flesch-Kincaid Grade Level of 10.99. While 91% of the ICFs were written above the 8th-grade level, there was an observable improvement in readability, with fewer studies exceeding a 10th-grade reading level in recent years.",
                "Conclusions:": "The study reveals a discrepancy between the recommended readability levels and actual ICFs, highlighting a need for simplification. Despite a trend toward improvement in more recent years, ongoing efforts are necessary to ensure ICFs are comprehensible to participants of varied educational backgrounds, reinforcing the ethical integrity of the consent process."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1017/cts.2024.595",
                "pmid": null,
                "pmc": "PMC11428065",
                "pub_year": "2024",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11428065/"
            }
        },
        {
            "title": "Recommendations for developing accessible patient information leaflets for clinical trials to address English language literacy as a barrier to research participation",
            "abstract": {
                "Background": "Low English language literacy is a common barrier to participation in clinical trials. Patient information leaflets (PILs) used in clinical trials are often lengthy, complex and have poor readability; this is a persistent and prevalent problem common to trials across the world. Simplifying the information provided in PILs can lead to improved understanding, comprehension and knowledge.The aim of this project was to develop recommendations for developing accessible PILs for clinical trials through a literature review of published and grey literature and co-working with marginalised communities, patients, and health and social care charities.",
                "Methods": "A literature review of MEDLINE, Embase and online resources was conducted, and recommendations for developing accessible PILs were extracted from eligible published and grey literature. Grey literature which contained insights into more inclusive forms of communication was also identified and summarised. Meetings were held with two racially marginalised community groups, two groups involving autistic adults and/or adults with learning difficulties and a patient advisory group. Examples of accessible PILs were shared and discussions held about the content and format of the PILs and suggestions for changes/improvements. National Voices, a coalition of health and social care charities in England, held a national online workshop with charities and lived experience partners. Recommendations identified from the multiple sources were coded, collated and refined to develop an overarching framework of recommendations.",
                "Results": "The framework consists of 74 recommendations for developing accessible PILs for clinical trials. Recommendations cover the five topics of formatting, information presentation, writing style, content and accessibility.",
                "Conclusions": "This project has developed a comprehensive framework of recommendations to guide researchers in the development of accessible PILs for clinical trials. Findings from previous research and from co-working with marginalised communities, patients and health and social care charities were collated to ensure that a diverse range of voices and experiences informed the framework. These recommendations aim to support researchers to develop better study information to reduce English language literacy as a barrier to participation in clinical trials.",
                "Supplementary Information": "The online version contains supplementary material available at 10.1186/s13063-024-08471-5."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1186/s13063-024-08471-5",
                "pmid": null,
                "pmc": "PMC11430508",
                "pub_year": "2024",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11430508/"
            }
        },
        {
            "title": "Assessing the Readability and Comprehensibility of Online Patient Educational Materials for Common Psychotic Disorders",
            "abstract": {
                "title_content_0": "Background",
                "title_content_1": "In our age of technology, millions of people use the Internet daily for health-related searches and guidance, both patients and caregivers alike. However, health literacy remains notably low among U.S. adults, and this issue is particularly critical for individuals with severe mental illnesses. Poor health literacy is often linked to low socioeconomic status and correlates with adverse patient outcomes and limited healthcare access. With the average reading level of the U.S. adult at the eighth-grade level, guidelines recommend health information be written to match. This study focuses on the readability of top Google search results for common psychotic disorders, emphasizing the need for accessible online health information to support vulnerable populations with severe mental illnesses.",
                "title_content_2": "Methods",
                "title_content_3": "The top five most visited websites for eight psychiatric conditions were included in this study. These conditions included schizophrenia, schizoaffective disorder, schizophreniform disorder, delusional disorder, bipolar 1 disorder, major depressive disorder (MDD) with psychotic features, substance-induced psychotic disorder, and psychotic disorder due to a general medical condition. The Flesch-Kincaid (FK) reading ease and grade level score were calculated for each webpage. Additionally, all institutions and organizations that created each webpage were noted.",
                "title_content_4": "Results",
                "title_content_5": "The average FK grade level was 9.9 (corresponding to a 10th-grade level), while the overall FK reading ease was 37.3 (corresponding to college-level difficulty) across all disorders analyzed. Websites on MDD with psychotic features had the lowest average FK grade level, 8.6, and best reading ease score. Websites discussing delusional disorder had the highest average FK grade level, 11.2, while those with information on schizophreniform disorder had the lowest average reading ease with a score of 31.7, corresponding to \u201cdifficult (college)\u201d level reading.",
                "title_content_6": "Conclusion",
                "title_content_7": "Both patient education and compliance can be improved with more accessible and readable patient educational materials. Our study shows significant opportunities for improvement in the readability and comprehensibility of online educational materials for eight of the most common psychotic disorders. Physicians and other healthcare providers should be aware of this barrier, recommending specific websites, literature, and resources for patients and their caregivers. Further efforts should be aimed at creating new and easy-to-comprehend online material for mental health disorders, ensuring the best quality and care for these patients."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.7759/cureus.68064",
                "pmid": null,
                "pmc": "PMC11438542",
                "pub_year": "2024",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11438542/"
            }
        },
        {
            "title": "An Observational Study to Evaluate Readability and Reliability of AI-Generated Brochures for Emergency Medical Conditions",
            "abstract": {
                "title_content_0": "Introduction",
                "title_content_1": "The study assesses the readability of AI-generated brochures for common emergency medical conditions like heart attack, anaphylaxis, and syncope. Thus, the study aims to compare the AI-generated responses for patient information guides of common emergency medical conditions using ChatGPT and Google Gemini.",
                "title_content_2": "Methodology",
                "title_content_3": "Brochures for each condition were created by both AI tools. Readability was assessed using the Flesch-Kincaid Calculator, evaluating word count, sentence count and ease of understanding. Reliability was measured using the Modified DISCERN Score. The similarity between AI outputs was determined using Quillbot. Statistical analysis was performed with R (v4.3.2).",
                "title_content_4": "Results",
                "title_content_5": "ChatGPT and Gemini produced brochures with no statistically significant differences in word count (p= 0.2119), sentence count (p=0.1276), readability (p=0.3796), or reliability (p=0.7407). However, ChatGPT provided more detailed content with 32.4% more words (582.80 vs. 440.20) and 51.6% more sentences (67.00 vs. 44.20). In addition, Gemini's brochures were slightly easier to read with a higher ease score (50.62 vs. 41.88). Reliability varied by topic with ChatGPT scoring higher for Heart Attack (4 vs. 3) and Choking (3 vs. 2), while Google Gemini scored higher for Anaphylaxis (4 vs. 3) and Drowning (4 vs. 3), highlighting the need for topic-specific evaluation.",
                "title_content_6": "Conclusions",
                "title_content_7": "Although AI-generated brochures from ChatGPT and Gemini are comparable in readability and reliability for patient information on emergency medical conditions, this study highlights that there is no statistically significant difference in the responses generated by the two AI tools."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.7759/cureus.68307",
                "pmid": "39350844",
                "pmc": "PMC11441454",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/39350844/"
            }
        },
        {
            "title": "Empowering patients: how accurate and readable are large language models in renal cancer education",
            "abstract": {
                "Background": "The incorporation of Artificial Intelligence (AI) into healthcare sector has fundamentally transformed patient care paradigms, particularly through the creation of patient education materials (PEMs) tailored to individual needs. This Study aims to assess the precision and readability AI-generated information on kidney cancer using ChatGPT 4.0, Gemini AI, and Perplexity AI., comparing these outputs to PEMs provided by the American Urological Association (AUA) and the European Association of Urology (EAU). The objective is to guide physicians in directing patients to accurate and understandable resources.",
                "Methods": "PEMs published by AUA and EAU were collected and categorized. kidney cancer-related queries, identified via Google Trends (GT), were input into CahtGPT-4.0, Gemini AI, and Perplexity AI. Four independent reviewers assessed the AI outputs for accuracy grounded on five distinct categories, employing a 5-point Likert scale. A readability evaluation was conducted utilizing established formulas, including Gunning Fog Index (GFI), Simple Measure of Gobbledygook (SMOG), and Flesch-Kincaid Grade Formula (FKGL). AI chatbots were then tasked with simplifying their outputs to achieve a sixth-grade reading level.",
                "Results": "The PEM published by the AUA was the most readable with a mean readability score of 9.84 \u00b1 1.2, in contrast to EAU (11.88 \u00b1 1.11), ChatGPT-4.0 (11.03 \u00b1 1.76), Perplexity AI (12.66 \u00b1 1.83), and Gemini AI (10.83 \u00b1 2.31). The Chatbots demonstrated the capability to simplify text lower grade levels upon request, with ChatGPT-4.0 achieving a readability grade level ranging from 5.76 to 9.19, Perplexity AI from 7.33 to 8.45, Gemini AI from 6.43 to 8.43. While official PEMS were considered accurate, the LLMs generated outputs exhibited an overall high level of accuracy with minor detail omission and some information inaccuracies. Information related to kidney cancer treatment was found to be the least accurate among the evaluated categories.",
                "Conclusion": "Although the PEM published by AUA being the most readable, both authoritative PEMs and Large Language Models (LLMs) generated outputs exceeded the recommended readability threshold for general population. AI Chatbots can simplify their outputs when explicitly instructed. However, notwithstanding their accuracy, LLMs-generated outputs are susceptible to detail omission and inaccuracies. The variability in AI performance necessitates cautious use as an adjunctive tool in patient education."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.3389/fonc.2024.1457516",
                "pmid": "39391252",
                "pmc": "PMC11464325",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/39391252/"
            }
        },
        {
            "title": "The user-centered design and development of a childhood and adolescent obesity Electronic Health Record tool, a mixed-methods study",
            "abstract": {
                "Background": "Childhood and adolescent obesity are persistent public health issues in the United States. Childhood obesity Electronic Health Record (EHR) tools strengthen provider-patient relationships and improve outcomes, but there are currently limited EHR tools that are linked to adolescent mHealth apps. This study is part of a larger study entitled, CommitFit, which features both an adolescent-targeted mobile health application (mHealth app) and an ambulatory EHR tool. The CommitFit mHealth app was designed to be paired with the CommitFit EHR tool for integration into clinical spaces for shared decision-making with patients and clinicians.",
                "Objectives": "The objective of this sub-study was to identify the functional and design needs and preferences of healthcare clinicians and professionals for the development of the CommitFit EHR tool, specifically as it relates to childhood and adolescent obesity management.",
                "Methods": "We utilized a user-centered design process with a mixed-method approach. Focus groups were used to assess current in-clinic practices, deficits, and general beliefs and preferences regarding the management of childhood and adolescent obesity. A pre- and post-focus group survey helped assess the perception of the design and functionality of the CommitFit EHR tool and other obesity clinic needs. Iterative design development of the CommitFit EHR tool occurred throughout the process.",
                "Results": "A total of 12 healthcare providers participated throughout the three focus group sessions. Two themes emerged regarding EHR design: (1) Functional Needs, including Enhancing Clinical Practices and Workflow, and (2) Visualization, including Colors and Graphs. Responses from the surveys (n\u2009=\u200952) further reflect the need for Functionality and User-Interface Design by clinicians. Clinicians want the CommitFit EHR tool to enhance in-clinic adolescent lifestyle counseling, be easy to use, and presentable to adolescent patients and their caregivers. Additionally, we found that clinicians preferred colors and graphs that improved readability and usability. During each step of feedback from focus group sessions and the survey, the design of the CommitFit EHR tool was updated and co-developed by clinicians in an iterative user-centered design process.",
                "Conclusion": "More research is needed to explore clinician actual user analytics for the CommitFit EHR tool to evaluate real-time workflow, design, and function needs. The effectiveness of the CommitFit mHealth and EHR tool as a weight management intervention needs to be evaluated in the future."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.3389/fdgth.2024.1396085",
                "pmid": "39411348",
                "pmc": "PMC11476727",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/39411348/"
            }
        },
        {
            "title": "Digital Education on Hospital Nutrition Diets: What Do Patients Want to Know?",
            "abstract": {
                "title_content_0": "Introduction: Therapeutic nutrition plays an imperative role during a patient\u2019s hospital course. There is a tremendous body of literature that emphasizes the systematic delivery of information regarding hospital nutrition diets. A major component of delivering healthcare information is the principle of providing quality healthcare information, but this has not yet been investigated on hospital nutrition diets. This study aimed to evaluate the comprehension and readability of patient education materials regarding therapeutic hospital diets. Methodology: The methodology employed the use of publicly available questions regarding hospital nutrition diets and categorized them per Rothwell\u2019s Classification of Questions. Additionally, the questions were extracted online and have an associated digital article linked to the question. These articles underwent analysis for readability scores. Results: This study\u2019s findings reveal that most hospital diets do not meet the recommended grade-reading levels. Conclusions: This underscores the need for healthcare providers to enhance patient education regarding hospital diets. The prevalence of \u201cFact\u201d questions showcases the importance of clearly explaining diets and dietary restrictions to patients."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.3390/nu16193314",
                "pmid": null,
                "pmc": "PMC11478968",
                "pub_year": "2024",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11478968/"
            }
        },
        {
            "title": "Ascle\u2014A Python Natural Language Processing Toolkit for Medical Text Generation: Development and Evaluation Study",
            "abstract": {
                "Background": "Medical texts present significant domain-specific challenges, and manually curating these texts is a time-consuming and labor-intensive process. To address this, natural language processing (NLP) algorithms have been developed to automate text processing. In the biomedical field, various toolkits for text processing exist, which have greatly improved the efficiency of handling unstructured text. However, these existing toolkits tend to emphasize different perspectives, and none of them offer generation capabilities, leaving a significant gap in the current offerings.",
                "Objective": "This study aims to describe the development and preliminary evaluation of Ascle. Ascle is tailored for biomedical researchers and clinical staff with an easy-to-use, all-in-one solution that requires minimal programming expertise. For the first time, Ascle provides 4 advanced and challenging generative functions: question-answering, text summarization, text simplification, and machine translation. In addition, Ascle integrates 12 essential NLP functions, along with query and search capabilities for clinical databases.",
                "Methods": "We fine-tuned 32 domain-specific language models and evaluated them thoroughly on 27 established benchmarks. In addition, for the question-answering task, we developed a retrieval-augmented generation (RAG) framework for large language models that incorporated a medical knowledge graph with ranking techniques to enhance the reliability of generated answers. Additionally, we conducted a physician validation to assess the quality of generated content beyond automated metrics.",
                "Results": "The fine-tuned models and RAG framework consistently enhanced text generation tasks. For example, the fine-tuned models improved the machine translation task by 20.27 in terms of BLEU score. In the question-answering task, the RAG framework raised the ROUGE-L score by 18% over the vanilla models. Physician validation of generated answers showed high scores for readability (4.95/5) and relevancy (4.43/5), with a lower score for accuracy (3.90/5) and completeness (3.31/5).",
                "Conclusions": "This study introduces the development and evaluation of Ascle, a user-friendly NLP toolkit designed for medical text generation. All code is publicly available through the Ascle GitHub repository. All fine-tuned language models can be accessed through Hugging Face."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.2196/60601",
                "pmid": "39361955",
                "pmc": "PMC11487205",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/39361955/"
            }
        },
        {
            "title": "Assessing the Quality of Patient Education Materials on Cardiac Catheterization From Artificial Intelligence Chatbots: An Observational Cross-Sectional Study",
            "abstract": {
                "title_content_0": "Background",
                "title_content_1": "Health literacy empowers patients to participate in their own healthcare. Personal health literacy is one\u2019s ability to find, understand, and use information/resources to make well-informed health decisions. Artificial intelligence (AI) has become a source for the acquisition of health-related information through large language model (LLM)-driven chatbots. Assessment of the readability and quality of health information produced by these chatbots has been the subject of numerous studies to date. This study seeks to assess the quality of patient education materials on cardiac catheterization produced by AI chatbots.",
                "title_content_2": "Methodology",
                "title_content_3": "We asked a set of 10 questions about cardiac catheterization to four chatbots: ChatGPT (OpenAI, San Francisco, CA), Microsoft Copilot (Microsoft Corporation, Redmond, WA), Google Gemini (Google DeepMind, London, UK), and Meta AI (Meta, New York, NY). The questions and subsequent answers were utilized to make patient education materials on cardiac catheterization. The quality of these materials was assessed using two validated instruments for patient education materials: DISCERN and the Patient Education Materials Assessment Tool (PEMAT).",
                "title_content_4": "Results",
                "title_content_5": "The overall DISCERN scores were 4.5 for ChatGPT, 4.4 for Microsoft Copilot and Google Gemini, and 3.8 for Meta AI. ChatGPT, Microsoft Copilot, and Google Gemini tied for the highest reliability score at 4.6, while Meta AI had the lowest with 4.2. ChatGPT had the highest quality score at 4.4, while Meta AI had the lowest with 3.4. ChatGPT and Google Gemini had Understandability scores of 100%, while Meta AI had the lowest with 82%. ChatGPT, Microsoft Copilot, and Google Gemini all had Actionability scores of 75%, while Meta AI had one of 50%.",
                "title_content_6": "Conclusions",
                "title_content_7": "ChatGPT produced the most reliable and highest quality materials, followed closely by Google Gemini. Meta AI produced the lowest quality materials. Given the easy accessibility that chatbots provide patients and the high-quality responses that we obtained, they could be a reliable source for patients to obtain information about cardiac catheterization."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.7759/cureus.69996",
                "pmid": "39445289",
                "pmc": "PMC11498076",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/39445289/"
            }
        },
        {
            "title": "Appropriateness and readability of Google Bard and ChatGPT-3.5 generated responses for surgical treatment of glaucoma",
            "abstract": {
                "Aim": "To evaluate the appropriateness and readability of the medical knowledge provided by ChatGPT-3.5 and Google Bard, artificial-intelligence-powered conversational search engines, regarding surgical treatment for glaucoma.",
                "Methods": "In this retrospective, cross-sectional study, 25 common questions related to the surgical management of glaucoma were asked on ChatGPT-3.5 and Google Bard. Glaucoma specialists graded the responses\u2019 appropriateness, and different scores assessed readability.",
                "Results": "Appropriate answers to the posed questions were obtained in 68% of the responses with Google Bard and 96% with ChatGPT-3.5. On average, the responses generated by Google Bard had a significantly lower proportion of sentences, having more than 30 and 20 syllables (23% and 52% respectively) compared to ChatGPT-3.5 (66% and 82% respectively), as noted by readability. Google Bard had significantly (p<0.0001) lower readability grade scores and significantly higher \u201cFlesch Reading ease score\u201d, implying greater ease of readability amongst the answers generated by Google Bard.",
                "Discussion": "Many patients and their families turn to LLM chatbots for information, necessitating clear and accurate content. Assessments of online glaucoma information have shown variability in quality and readability, with institutional websites generally performing better than private ones. We found that ChatGPT-3.5, while precise, has lower readability than Google Bard, which is more accessible but less precise. For example, the Flesch Reading Ease Score was 57.6 for Google Bard and 22.6 for ChatGPT, indicating Google Bard\u2019s content is easier to read. Moreover, the Gunning Fog Index scores suggested that Google Bard\u2019s text is more suitable for a broader audience. ChatGPT\u2019s knowledge is limited to data up to 2021, whereas Google Bard, trained with real-time data, offers more current information. Further research is needed to evaluate these tools across various medical topics.",
                "Conclusion": "The answers generated by ChatGPT-3.5\u2122 AI are more accurate than the ones given by Google Bard. However, comprehension of ChatGPT-3.5\u2122 answers may be difficult for the public with glaucoma. This study emphasized the importance of verifying the accuracy and clarity of online information that glaucoma patients rely on to make informed decisions about their ocular health. This is an exciting new area for patient education and health literacy."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.22336/rjo.2024.45",
                "pmid": null,
                "pmc": "PMC11503238",
                "pub_year": "2024",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11503238/"
            }
        },
        {
            "title": "Implementation of Outpatient Automated Stewardship Information System (OASIS\u00a9) Audit and Feedback in Two Healthcare Systems",
            "abstract": {
                "title_content_0": "Background: Combating antibiotic resistance, exacerbated by widespread unnecessary outpatient antibiotic prescriptions, necessitates innovative stewardship solutions. Audit and feedback reports are effective but often resource heavy. We introduced a free, open-source system, Outpatient Automated Stewardship Information System (OASIS\u00a9), for automating the creation and distribution of recurring audit and feedback reports to clinicians to improve antibiotic prescribing. Methods: We used mixed methods to evaluate implementation of OASIS\u00a9 across 11 clinics at Denver Health and Hospital Authority (DHHA) and Children\u2019s Hospital Colorado (CHCO) from July 2022 to August 2023. Both sites host their own Epic\u00ae electronic healthcare and enterprise data warehouse systems. R statistical software was utilized to retrieve and process the data needed to create individual[HCM1] clinician audit and feedback reports with peer comparison. Reports were provided for 1) antibiotics prescribed for respiratory diagnoses, 2) antibiotics prescribed for respiratory diagnoses where antibiotics are never indicated, 3) first-line antibiotic prescribing for acute otitis media (AOM), and 4) five-day duration of antibiotics for children two years and older with AOM. Feedback reports for each metric were emailed to clinicians for three consecutive months. The primary outcome was adaptations needed to implement OASIS\u00a9. Secondary outcomes included fidelity (measured by email readership), time to set up and maintain the program, and barriers and facilitators to implementation (assessed by four qualitative interviews with OASIS\u00a9 stakeholders). Results: The most significant adaptations made pertained to the automation of OASIS\u00a9 reports for organizations not using R for data retrieval and reporting, setting up OASIS\u00a9 specific email addresses, and validating clinician fidelity via read receipts. Fidelity was higher at DHHA (91-100%) compared to CHCO (10-30%). When interviewed, data analysts expressed that time for initial setup ranged from 1-6 hours. After reporting was automated, the estimated monthly time to send reports was 10 minutes. Views on setup complexity were split, but all recognized the readability of the reports and OASIS\u00a9\u2019s value for improving prescribing behaviors. The greatest barriers to implementation included obtaining analytic resources for initial setup and the need to download additional R packages. No interviewee had prior experience creating audit and feedback reports. Conclusions: Implementing OASIS\u00a9 requires addressing system diversity and knowledge gaps in outpatient informatics and antibiotic stewardship. Despite these challenges, the tool proved efficient and beneficial for monitoring and reporting antimicrobial prescribing. This free tool could likely be effectively disseminated to other health systems given the limited time and resources required for adaptations, setup, and monitoring. [HCM1]I"
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1017/ash.2024.175",
                "pmid": null,
                "pmc": "PMC11504877",
                "pub_year": "2024",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11504877/"
            }
        },
        {
            "title": "The Challenges for EU User Testing Policies for Patient Information Leaflets",
            "abstract": {
                "title_content_0": "Patient information leaflets (PILs) are essential tools in healthcare, providing crucial information about medication use. In the European Union, the European Medicines Agency (EMA) oversees the regulation and standardisation of PILs to ensure their readability and accessibility. However, challenges persist in ensuring these documents are comprehensible and user-friendly. This study employs a qualitative analytical approach, reviewing existing literature and regulatory documents to identify gaps in the EU user testing policies for PILs. It focuses on the diversity of participant samples, the independence of the testing process, and the robustness of user testing protocols. Findings indicate that current user testing practices often lack diversity and may be biased when pharmaceutical companies conduct their own tests. Additionally, there is a lack of user testing protocols for translated PILs, potentially compromising their accuracy and cultural relevance. To improve the efficacy of PILs, it is essential to include diverse and representative samples in user testing, mandate independent third-party evaluations, implement protocols for user testing on translated PILs, and ensure continuous updates to guidelines based on the latest best practices in health communication. These measures will enhance patient safety and understanding of medication information."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.3390/ijerph21101301",
                "pmid": null,
                "pmc": "PMC11507276",
                "pub_year": "2024",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11507276/"
            }
        },
        {
            "title": "Behavioral Nudging With Generative AI for Content Development in SMS Health Care Interventions: Case Study",
            "abstract": {
                "Background": "Brief message interventions have demonstrated immense promise in health care, yet the development of these messages has suffered from a dearth of transparency and a scarcity of publicly accessible data sets. Moreover, the researcher-driven content creation process has raised resource allocation issues, necessitating a more efficient and transparent approach to content development.",
                "Objective": "This research sets out to address the challenges of content development for SMS interventions by showcasing the use of generative artificial intelligence (AI) as a tool for content creation, transparently explaining the prompt design and content generation process, and providing the largest publicly available data set of brief messages and source code for future replication of our process.",
                "Methods": "Leveraging the pretrained large language model GPT-3.5 (OpenAI), we generate a collection of messages in the context of medication adherence for individuals with type 2 diabetes using evidence-derived behavior change techniques identified in a prior systematic review. We create an attributed prompt designed to adhere to content (readability and tone) and SMS (character count and encoder type) standards while encouraging message variability to reflect differences in behavior change techniques.",
                "Results": "We deliver the most extensive repository of brief messages for a singular health care intervention and the first library of messages crafted with generative AI. In total, our method yields a data set comprising 1150 messages, with 89.91% (n=1034) meeting character length requirements and 80.7% (n=928) meeting readability requirements. Furthermore, our analysis reveals that all messages exhibit diversity comparable to an existing publicly available data set created under the same theoretical framework for a similar setting.",
                "Conclusions": "This research provides a novel approach to content creation for health care interventions using state-of-the-art generative AI tools. Future research is needed to assess the generated content for ethical, safety, and research standards, as well as to determine whether the intervention is successful in improving the target behaviors."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.2196/52974",
                "pmid": "39405108",
                "pmc": "PMC11522651",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/39405108/"
            }
        },
        {
            "title": "Developing a Comprehensive Inventory to Define Harm Reduction Housing",
            "abstract": {
                "Background": "The City of Boston has faced unprecedented challenges with substance use amidst changes to the illicit drug supply and increased visibility of homelessness. Among its responses, Boston developed six low threshold harm reduction housing (HRH) sites geared towards supporting the housing needs of people who use drugs (PWUD) and addressing health and safety concerns around geographically concentrated tent encampments. HRH sites are transitional supportive housing that adhere to a \u201chousing first\u201d approach where abstinence is not required and harm reduction services and supports are co-located. Despite the importance of HRH, the specific characteristics and operations of these sites are not well understood. This study sought to address this gap by cataloging the common features of Boston\u2019s HRH sites to generate a comprehensive inventory tool for evaluating implementation of harm reduction strategies at transitional housing locations.",
                "Methods": "We collected data between June and September 2023 and included semi-structured qualitative interviews with HRH staff (n = 19), ethnographic observations and photos at six HRH sites. Candidate inventory components were derived through triangulation of the data. Two expert medical staff unaffiliated with data collection reviewed a draft inventory measuring awareness and utility of HRH inventory components. We then pilot tested the inventory with 3 HRH residents across two sites for readability and reliability. Inventory performance was further tested in a survey of 106 residents.",
                "Results": "HRH staff identified best practices, resources, and policies in HRH sites that were further contextualized with ethnographic field notes. Common to all were overdose prevention protocols, behavioral policies, security measures, and harm reduction supplies distribution. The initial 44-item inventory of services, policies and site best practices was further refined with expert and participant feedback and application, then finalized to generate a 32-item inventory. Residents identified and valued harm reduction services; medical supports were highly valued but less utilized.",
                "Conclusion": "The HRH inventory comprehensively assesses harm reduction provision and residents\u2019 awareness and perceived helpfulness of HRH operational components. Characterizing the critical components of HRH through this tool will aid in standardizing the concept and practice of HRH for PWUD and may assist other cities in planning and implementing HRH."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.21203/rs.3.rs-4999367/v1",
                "pmid": "39483918",
                "pmc": "PMC11527206",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/39483918/"
            }
        },
        {
            "title": "Generative artificial intelligence writing open notes: A mixed methods assessment of the functionality of GPT 3.5 and GPT 4.0",
            "abstract": {
                "Background": "Worldwide, patients are increasingly being offered access to their full online clinical records including the narrative reports written by clinicians (so-called \u201copen notes\u201d). Against these developments, there is growing interest in the use of generative artificial intelligence (AI) such as OpenAI's ChatGPT to co-assist clinicians with patient-facing documentation.",
                "Objective": "This study aimed to explore the effectiveness of OpenAI's ChatGPT 3.5 and GPT 4.0 in generating three patient-facing clinical notes from fictional general practice narrative reports.",
                "Methods": "On 1 October 2023 and 1 November 2023, we used ChatGPT 3.5 and 4.0 to generate notes for three validated fictional general practice notes, using a prompt in the style of a British primary care note for three commonly presented conditions: (1) type 2 diabetes, (2) major depressive disorder, and (3) a differential diagnosis for suspected bowel cancer. Outputs were analyzed for reading ease, sentiment analysis, empathy, and medical fidelity.",
                "Results": "ChatGPT 3.5 and 4.0 wrote longer notes than the original, and embedded more second person pronouns, with ChatGPT 3.5 scoring higher on both. ChatGPT expanded abbreviations, but readability metrics showed that the notes required a higher reading proficiency, with ChatGPT 3.5 demanding the most advanced level. Across all notes, ChatGPT offered higher signatures of empathy across cognitive, compassion/sympathy, and prosocial cues. Medical fidelity ratings varied across all three cases with ChatGPT 4.0 rated superior.",
                "Conclusions": "While ChatGPT improved sentiment and empathy metrics in the transformed notes, compared to the original they also required higher reading proficiency and omitted details impacting medical fidelity."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1177/20552076241291384",
                "pmid": null,
                "pmc": "PMC11528788",
                "pub_year": "2024",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11528788/"
            }
        },
        {
            "title": "Assessment of the Quality and Readability of Online Resources on Corneal Transplantation",
            "abstract": {
                "title_content_0": "Purpose: The aim of this study is to evaluate the quality and readability of corneal transplant websites.",
                "title_content_1": "Methods:\u00a0We examined internet resources related to corneal transplantation. The search terms \"cornea transplantation,\"\u00a0\"keratoplasty,\"\u00a0\"keratoprothesis,\"\u00a0and \"eye transplantation\" were searched on www.google.com. A total of 200 websites were scanned, with 50 websites for each search term. In the final evaluation, 113 websites were included in the study. DISCERN and the Journal of the American Medical Association (JAMA)\u00a0scales were used to evaluate the websites' quality. The Flesch-Kincaid Grade Level (FKGL) and Simple Measure of Gobbledygook (SMOG) were used to evaluate the readability of the information. The presence of the Health On the Net Foundation Code of Conduct (HONCode) certification on websites was examined in terms of quality.",
                "title_content_2": "Results: The total DISCERN score was 42.47 \u00b1 17.06, and the score for the JAMA\u00a0was 1.58 \u00b1 1.52. The FKGL and SMOG scores were found to be 9.19 \u00b1 2.08 and 8.20 \u00b1 5.23, respectively. Most of the websites we examined related to corneal transplantation were profit websites. These websites had low JAMA and DISCERN scores but relatively high readability levels. They were sites that provided low-quality information and had more financial bias and conflict of interest. Journal and book websites were found to have higher readability scores.\u00a0Journal and book-sourced websites with higher FKGL and SMOG scores were more difficult to read.",
                "title_content_3": "Conclusion: Most of the websites exhibit financial bias, and as a result, they contain low-quality information. Online information must be more regulated for patients to easily access accurate, high-quality information."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.7759/cureus.70819",
                "pmid": "39493116",
                "pmc": "PMC11531918",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/39493116/"
            }
        },
        {
            "title": "Improving Biomedical Science Literacy and Patient-Directed Knowledge of Tuberculosis (TB): A Cross-Sectional Infodemiology Study Examining Readability of Patient-Facing TB Information",
            "abstract": {
                "Background": "Tuberculosis (TB) continues be the leading cause of death globally due to an infectious agent. There is a paucity of data describing the readability of patient-facing TB information for service users. The aim of this study was to calculate the readability of multiple global TB information sources.",
                "Methods": "Information on tuberculosis (n = 150 sources) included nine categories, Patient-facing information: WHO publications (n = 17), International governments (n = 19), Hospitals (n = 10), Non-government organisations (NGOs)/charities (n = 20), Cochrane Plain Language Summaries (n = 20); LabTestsOnlineUK (n = 4) and Scientific-facing information: Clinical trials (n = 20), Cochrane abstracts (n = 20), Scientific abstracts (n = 20). Readability was calculated using Readable software, defined by (i) Flesch Reading Ease (FRE), (ii) Flesch-Kincaid Grade Level (FKGL), (iii) Gunning Fog Index and (iv) SMOG Index and two text metrics [words/sentence, syllables/word].",
                "Results": "Mean readability values for TB information for the FRE and FKGL were 35.6 \u00b1 1.6 (standard error of mean (SEM)) (US Target \u226560; UK Target \u226590) and 12.3 \u00b1 0.3 (US Target \u22648; UK Target \u22646), respectively, with mean words per sentence and syllables per word of 17.2 and 1.8, respectively. Cochrane Plain Language Summaries had similar readability scores to their matching scientific abstract (p = 0.15). LabTestsOnlineUK yielded a mean FRE score of 51.5 \u00b1 1.2, a mean FKGL score of 10.2 \u00b1 0.5 and text metric scores of 16.7 \u00b1 2.3 and 1.6, for words per sentence and syllables per word, respectively. In descending order, TB information from international governments, hospitals and LabTestsOnlineUK were the most readable (FRE = 57.9, 54.1 and 51.5, respectively), whereas scientific abstracts and Cochrane abstracts were the most difficult to read (13.0 and 30.2, respectively).",
                "Conclusion": "Patient-facing TB information analysed had poor readability. Effective communication of biomedical science concepts and information relating to TB is vital for service users to enhance their health literacy of tuberculosis, thereby promoting better clinical outcomes. Biomedical scientists are important custodians of scientific information for their service user populations, including other healthcare professionals within the TB multidisciplinary (MDT) team and patient service users. When preparing TB information, this should be checked and modified in real time employing readability calculators, to align with health readability targets."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.3389/bjbs.2024.13566",
                "pmid": "39502459",
                "pmc": "PMC11534592",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/39502459/"
            }
        },
        {
            "title": "Assessing the Clinical Appropriateness and Practical Utility of ChatGPT as an Educational Resource for Patients Considering Minimally Invasive Spine Surgery",
            "abstract": {
                "title_content_0": "Introduction",
                "title_content_1": "Minimally invasive spine surgery (MISS) has evolved over the last three decades as a less invasive alternative to traditional spine surgery, offering benefits such as smaller incisions, faster recovery, and lower complication rates. With patients frequently seeking information about MISS online, the comprehensibility and accuracy of this information are crucial. Recent studies have shown that much of the online material regarding spine surgery exceeds the recommended readability levels, making it difficult for patients to understand. This study explores the clinical appropriateness and readability of responses generated by Chat Generative Pre-Trained Transformer (ChatGPT) to frequently asked questions (FAQs) about MISS.",
                "title_content_2": "Methods",
                "title_content_3": "A set of 15 FAQs was formulated based on clinical expertise and existing literature on MISS. Each question was independently inputted into ChatGPT five times, and the generated responses were evaluated by three neurosurgery attendings for clinical appropriateness. Appropriateness was judged based on accuracy, readability, and patient accessibility. Readability was assessed using seven standardized readability tests, including the Flesch-Kincaid Grade Level and Flesch Reading Ease (FRE) scores. Statistical analysis was performed to compare readability scores across preoperative, postoperative, and intraoperative/technical question categories.",
                "title_content_4": "Results",
                "title_content_5": "The mean readability scores for preoperative, postoperative, and intraoperative/technical questions were 15\u00b12.8, 16\u00b13, and 15.7\u00b13.2, respectively, significantly exceeding the recommended sixth- to eighth-grade reading level for patient education (p=0.017). Differences in readability across individual questions were also statistically significant (p<0.001). All responses required a reading level above 11th grade, with a majority indicating college-level comprehension. Although preoperative and postoperative questions generally elicited clinically appropriate responses, 50% of intraoperative/technical questions yielded either \"inappropriate\" or \"unreliable\" responses, particularly for inquiries about radiation exposure and the use of lasers in MISS.",
                "title_content_6": "Conclusions",
                "title_content_7": "While ChatGPT is proficient in providing clinically appropriate responses to certain FAQs about MISS, it frequently produces responses that exceed the recommended readability level for patient education. This limitation suggests that its utility may be confined to highly educated patients, potentially exacerbating existing disparities in patient comprehension. Future AI-based patient education tools must prioritize clear and accessible communication, with oversight from medical professionals to ensure accuracy and appropriateness. Further research comparing ChatGPT's performance with other AI models could enhance its application in patient education across medical specialties."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.7759/cureus.71105",
                "pmid": null,
                "pmc": "PMC11548952",
                "pub_year": "2024",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11548952/"
            }
        },
        {
            "title": "Large language models in patient education: a scoping review of applications in medicine",
            "abstract": {
                "Introduction": "Large Language Models (LLMs) are sophisticated algorithms that analyze and generate vast amounts of textual data, mimicking human communication. Notable LLMs include GPT-4o by Open AI, Claude 3.5 Sonnet by Anthropic, and Gemini by Google. This scoping review aims to synthesize the current applications and potential uses of LLMs in patient education and engagement.",
                "Materials and methods": "Following the PRISMA-ScR checklist and methodologies by Arksey, O\u2019Malley, and Levac, we conducted a scoping review. We searched PubMed in June 2024, using keywords and MeSH terms related to LLMs and patient education. Two authors conducted the initial screening, and discrepancies were resolved by consensus. We employed thematic analysis to address our primary research question.",
                "Results": "The review identified 201 studies, predominantly from the United States (58.2%). Six themes emerged: generating patient education materials, interpreting medical information, providing lifestyle recommendations, supporting customized medication use, offering perioperative care instructions, and optimizing doctor-patient interaction. LLMs were found to provide accurate responses to patient queries, enhance existing educational materials, and translate medical information into patient-friendly language. However, challenges such as readability, accuracy, and potential biases were noted.",
                "Discussion": "LLMs demonstrate significant potential in patient education and engagement by creating accessible educational materials, interpreting complex medical information, and enhancing communication between patients and healthcare providers. Nonetheless, issues related to the accuracy and readability of LLM-generated content, as well as ethical concerns, require further research and development. Future studies should focus on improving LLMs and ensuring content reliability while addressing ethical considerations."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.3389/fmed.2024.1477898",
                "pmid": "39534227",
                "pmc": "PMC11554522",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/39534227/"
            }
        },
        {
            "title": "Ensuring Accuracy and Equity in Vaccination Information From ChatGPT and CDC: Mixed-Methods Cross-Language Evaluation",
            "abstract": {
                "Background": "In the digital age, large language models (LLMs) like ChatGPT have emerged as important sources of health care information. Their interactive capabilities offer promise for enhancing health access, particularly for groups facing traditional barriers such as insurance and language constraints. Despite their growing public health use, with millions of medical queries processed weekly, the quality of LLM-provided information remains inconsistent. Previous studies have predominantly assessed ChatGPT\u2019s English responses, overlooking the needs of non\u2013English speakers in the United States. This study addresses this gap by evaluating the quality and linguistic parity of vaccination information from ChatGPT and the Centers for Disease Control and Prevention (CDC), emphasizing health equity.",
                "Objective": "This study aims to assess the quality and language equity of vaccination information provided by ChatGPT and the CDC in English and Spanish. It highlights the critical need for cross-language evaluation to ensure equitable health information access for all linguistic groups.",
                "Methods": "We conducted a comparative analysis of ChatGPT\u2019s and CDC\u2019s responses to frequently asked vaccination-related questions in both languages. The evaluation encompassed quantitative and qualitative assessments of accuracy, readability, and understandability. Accuracy was gauged by the perceived level of misinformation; readability, by the Flesch-Kincaid grade level and readability score; and understandability, by items from the National Institutes of Health\u2019s Patient\u00a0Education\u00a0Materials Assessment Tool (PEMAT) instrument.",
                "Results": "The study found that both ChatGPT and CDC provided mostly accurate and understandable (eg, scores over 95 out of 100) responses. However, Flesch-Kincaid grade levels often exceeded the American Medical Association\u2019s recommended levels, particularly in English (eg, average grade level in English for ChatGPT=12.84, Spanish=7.93, recommended=6). CDC responses outperformed ChatGPT in readability across both languages. Notably, some Spanish responses appeared to be direct translations from English, leading to unnatural phrasing. The findings underscore the potential and challenges of using ChatGPT for health care access.",
                "Conclusions": "ChatGPT holds potential as a health information resource but requires improvements in readability and linguistic equity to be truly effective for diverse populations. Crucially, the default user experience with ChatGPT, typically encountered by those without advanced language and prompting skills, can significantly shape health perceptions. This is vital from a public health standpoint, as the majority of users will interact with LLMs in their most accessible form. Ensuring that default responses are accurate, understandable, and equitable is imperative for fostering informed health decisions across diverse communities."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.2196/60939",
                "pmid": "39476380",
                "pmc": "PMC11561424",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/39476380/"
            }
        },
        {
            "title": "\u201cDeveloping the tool SDM:KOMPASS. Supporting shared decision making implementation processes\u201d",
            "abstract": {
                "title_content_0": "Shared decision-making (SDM) involves patients in choosing their treatment or care options. SDM enhances patient engagement and treatment satisfaction. SDM has proved difficult to implement and sustain in routine clinical practice, hence a supportive tool is needed. This quality improvement study focuses on the development of a generic tool, labeled SDM:KOMPASS, which is intended to support hospital settings by facilitating the visualization of their formative progress and the setting of goals for the SDM implementation into routine clinical practice. The main objective of the present paper is to describe the development of this generic tool. A six-step development process was performed to develop a tool and investigate the tool\u2019s overall perceived usability. Qualitative methods, such as observations, individual and focus group interviews, provided insights. A 10-item quantitative survey gauged informants\u2019 immediate attitudes towards the tool. Purposefully sampled informants (N = 20), including healthcare professionals and patients, contributed diverse perspectives regarding; 1) The tool\u2019s readability and clarity, 2) the construct\u2019s domains and content, and 3) the tool\u2019s perceived usability. In alignment with real-world challenges, SDM:KOMPASS emerges as a potentially valuable resource for healthcare organizations embedding SDM. The six-step development process revealed how the tool SDM:KOMPASS has potential to enhance SDM implementation\u2019s manageability, goal-setting, and focus. Professionals engaged in strategic implementation within somatic and mental hospital departments find the tool potentially beneficial and feasible. The tool shows promise and usability but requires careful attention due to its comprehensiveness. The next step is to alpha test the tool in clinical practice."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1371/journal.pone.0312990",
                "pmid": "39556538",
                "pmc": "PMC11573207",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/39556538/"
            }
        },
        {
            "title": "Accuracy of Prospective Assessments of 4 Large Language Model Chatbot Responses to Patient Questions About Emergency Care: Experimental Comparative Study",
            "abstract": {
                "Background": "Recent surveys indicate that 48% of consumers actively use generative artificial intelligence (AI) for health-related inquiries. Despite widespread adoption and the potential to improve health care access, scant research examines the performance of AI chatbot responses regarding emergency care advice.",
                "Objective": "We assessed the quality of AI chatbot responses to common emergency care questions. We sought to determine qualitative differences in responses from 4 free-access AI chatbots, for 10 different serious and benign emergency conditions.",
                "Methods": "We created 10 emergency care questions that we fed into the free-access versions of ChatGPT 3.5 (OpenAI), Google Bard, Bing AI Chat (Microsoft), and Claude AI (Anthropic) on November 26, 2023. Each response was graded by 5 board-certified emergency medicine (EM) faculty for 8 domains of percentage accuracy, presence of dangerous information, factual accuracy, clarity, completeness, understandability, source reliability, and source relevancy. We determined the correct, complete response to the 10 questions from reputable and scholarly emergency medical references. These were compiled by an EM resident physician. For the readability of the chatbot responses, we used the Flesch-Kincaid Grade Level of each response from readability statistics embedded in Microsoft Word. Differences between chatbots were determined by the chi-square test.",
                "Results": "Each of the 4 chatbots\u2019 responses to the 10 clinical questions were scored across 8 domains by 5 EM faculty, for 400 assessments for each chatbot. Together, the 4 chatbots had the best performance in clarity and understandability (both 85%), intermediate performance in accuracy and completeness (both 50%), and poor performance (10%) for source relevance and reliability (mostly unreported). Chatbots contained dangerous information in 5% to 35% of responses, with no statistical difference between chatbots on this metric (P=.24). ChatGPT, Google Bard, and Claud AI had similar performances across 6 out of 8 domains. Only Bing AI performed better with more identified or relevant sources (40%; the others had 0%-10%). Flesch-Kincaid Reading level was 7.7-8.9 grade for all chatbots, except ChatGPT at 10.8, which were all too advanced for average emergency patients. Responses included both dangerous (eg, starting cardiopulmonary resuscitation with no pulse check) and generally inappropriate advice (eg, loosening the collar to improve breathing without evidence of airway compromise).",
                "Conclusions": "AI chatbots, though ubiquitous, have significant deficiencies in EM patient advice, despite relatively consistent performance. Information for when to seek urgent or emergent care is frequently incomplete and inaccurate, and patients may be unaware of misinformation. Sources are not generally provided. Patients who use AI to guide health care decisions assume potential risks. AI chatbots for health should be subject to further research, refinement, and regulation. We strongly recommend proper medical consultation to prevent potential adverse outcomes."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.2196/60291",
                "pmid": "39496149",
                "pmc": "PMC11574488",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/39496149/"
            }
        },
        {
            "title": "Written communication of whole genome sequencing results in the NHS Genomic Medicine Service: a multi-centre service evaluation",
            "abstract": {
                "title_content_0": "Whole genome sequencing (WGS) is being used in diagnostic testing for certain clinical indications within the NHS Genomic Medicine Service (GMS) in England. Letter writing is an integral part of delivering results. However, no national guidelines for writing results from WGS exist. This multi-centre service evaluation used mixed methods to understand the content and readability of letters returning diagnostic, variant of uncertain significance (VUS), and no-finding results to paediatric rare disease patients. Eight Regional Genetics Services (response rate 47%) in England provided a total of 37 letters returning diagnostic (n\u2009=\u200913), VUS (n\u2009=\u200910), and no-finding (n\u2009=\u200914) results. Diagnostic and VUS results were usually delivered during an appointment; no-finding results were typically delivered by letter only. Letters were diverse in which content topics they covered and level of detail. No-finding letters (14/14) explained the result but were less likely to cover other topics. Diagnostic letters discussed the result (13/13), the condition (13/13), clinical genetics follow-up (13/13), clinical management (10/13), and adapting to the result (9/13). VUS letters explained the result (10/10), diagnostic uncertainty (10/10), and clinical genetics follow-up (10/10). Uncertainty was a common component of letters (33/37), irrespective of the result. Reanalysis or review after one or more years was suggested in 6/13 diagnostic, 7/10 VUS, and 6/14 no-finding letters. The mean reading level of letters corresponded to 15\u201317 years. Understanding how WGS results are conveyed to families during appointments, as well as how families interpret that information, is needed to provide a more comprehensive overview of results communication and inform best practices."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1038/s41431-024-01636-5",
                "pmid": "38806663",
                "pmc": "PMC11576903",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/38806663/"
            }
        },
        {
            "title": "Readability Metrics in Patient Education: Where Do We Innovate?",
            "abstract": {
                "title_content_0": "The increasing use of digital applications in healthcare has led to a greater need for patient education materials. These materials, often in the form of pamphlets, booklets, and handouts, are designed to supplement physician\u2013patient communication and aim to improve patient outcomes. However, the effectiveness of these materials can be hindered by variations in patient health literacy. Readability, a measure of text comprehension, is a key factor influencing how well patients understand these educational materials. While there has been growing interest in readability assessment in medicine, many studies have demonstrated that digital texts do not frequently meet the recommended sixth-to-eighth grade reading level. The purpose of this opinion article is to review readability from the perspective of studies in pediatric medicine, internal medicine, preventative medicine, and surgery. This article aims to communicate that while readability is important, it tends to not fully capture the complexity of health literacy or effective patient communication. Moreover, a promising avenue to improve readability may be in generative artificial intelligence, as there are currently limited tools with similar effectiveness."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.3390/clinpract14060183",
                "pmid": "39585011",
                "pmc": "PMC11586978",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/39585011/"
            }
        },
        {
            "title": "Analysis of internet educational websites on tobacco cessation: A content analysis",
            "abstract": {
                "Background": "There have been significant changes in the lifestyles of individuals in the past few decades, which has led to increased morbidity and mortality worldwide. Both smoking and chewing forms of tobacco are highly prevalent, especially in India, and are implicated as causes of diseases, including oropharyngeal carcinomas. Effective tobacco cessation techniques and sources can help overcome addiction and reduce the disease burden in society. The aim was to evaluate the quality and readability of contents of various sources on an internet website about tobacco cessation.",
                "Objectives": "i) To evaluate the readability of internet content (Google) regarding tobacco cessation using Flesch\u2013Kincaid readability tests and the quality of internet content (Google) by using the JAMA benchmark, HONcode and DISCERN questionnaire.",
                "Methods": "A content review was employed to screen the content of the Google search engine for educational tobacco cessation websites, and the top 50 websites were selected according to criteria and reviewed by two reviewers. The readability of the internet content (Google) regarding tobacco cessation was evaluated using Flesch\u2013Kincaid readability tests. The quality of the screened sites was evaluated by using the JAMA (Journal of the American Medical Association) benchmark, HONcode (The Health on the Net Code of Conduct) and DISCERN (Discerning the Quality of Information for Choosing Treatments) questionnaire, and the readability and quality of the screened websites were correlated using the above instruments.",
                "Results": "FK readability ease was found to be 49% standard and 30% easy. The FK grade test found that 33% of the content could be easily understood by < 5\nth grade. All 4 JAMA benchmarks were met by 23% of websites, and authorship was the least fulfilled criterion. Correlation analysis revealed a significant association between FK ease score and FK grade score.",
                "Conclusions": "The Read-ability Ease and Read-ability Grade Levels of the websites related to tobacco cessation were not standard, and few websites fulfilled the JAMA benchmarks and had HONcode certification."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.12688/f1000research.146843.2",
                "pmid": null,
                "pmc": "PMC11607478",
                "pub_year": "2024",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11607478/"
            }
        },
        {
            "title": "Navigating pancreas transplant perceptions: assessing public sentiment and strategies using AI-driven analysis",
            "abstract": {
                "Background": "Pancreas transplantation, a crucial treatment for diabetes, is underutilized due to its invasiveness, strict criteria, organ scarcity, and limited centers. This highlights the need for enhanced public education and awareness through digital health platforms.",
                "Methods": "We utilized Google's AI-driven, consensus-based model and Claude AI 3.0 Opus by Anthropic to analyze public perceptions of pancreas transplantation. The top 10 websites identified by Google as of April-May 2024 were reviewed, focusing on sentiment, consensus, content readability, and complexity to develop strategies for better public engagement and understanding using digital health technologies.",
                "Results": "The top 10 websites, originating from the US and UK, showed a neutral and professional tone, targeting medical professionals and patients. Complex content was updated between 2021 and 2024, with a readability level suitable for high school to early college students. AI-driven analysis revealed strategies to increase public interest and understanding, including incorporating patient stories, simplifying medical jargon, utilizing visual aids, emphasizing quality of life improvements, showcasing research progress, facilitating patient outreach, promoting community engagement, partnering with influencers, and regularly updating content through digital health platforms.",
                "Conclusion": "To increase interest in pancreas transplantation in the era of connected health, we recommend integrating real patient experiences, simplifying medical content, using visual explanations, emphasizing post-transplant quality-of-life improvements, highlighting recent research, providing outreach opportunities, encouraging community connections, partnering with influencers, and keeping information current through digital health technologies. These methods aim to make pancreas transplantation more accessible and motivating for a diverse audience, supporting informed decision-making."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.3389/fdgth.2024.1453341",
                "pmid": null,
                "pmc": "PMC11638235",
                "pub_year": "2024",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11638235/"
            }
        },
        {
            "title": "Evaluating the Appropriateness, Consistency, and Readability of ChatGPT in Critical Care Recommendations",
            "abstract": {
                "title_content_0": "Background: We assessed 2 versions of the large language model (LLM) ChatGPT\u2014versions 3.5 and 4.0\u2014in generating appropriate, consistent, and readable recommendations on core critical care topics. Research Question: How do successive large language models compare in terms of generating appropriate, consistent, and readable recommendations on core critical care topics? Design and Methods: A set of 50 LLM-generated responses to clinical questions were evaluated by 2 independent intensivists based on a 5-point Likert scale for appropriateness, consistency, and readability. Results: ChatGPT 4.0 showed significantly higher median appropriateness scores compared to ChatGPT 3.5 (4.0 vs 3.0, P\u2009<\u2009.001). However, there was no significant difference in consistency between the 2 versions (40% vs 28%, P\u2009=\u20090.291). Readability, assessed by the Flesch-Kincaid Grade Level, was also not significantly different between the 2 models (14.3 vs 14.4, P\u2009=\u20090.93). Interpretation: Both models produced \u201challucinations\u201d\u2014misinformation delivered with high confidence\u2014which highlights the risk of relying on these tools without domain expertise. Despite potential for clinical application, both models lacked consistency producing different results when asked the same question multiple times. The study underscores the need for clinicians to understand the strengths and limitations of LLMs for safe and effective implementation in critical care settings. Registration:\nhttps://osf.io/8chj7/"
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1177/08850666241267871",
                "pmid": "39118320",
                "pmc": "PMC11639400",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/39118320/"
            }
        },
        {
            "title": "Exploring the Role of Large Language Models in Melanoma: A Systematic Review",
            "abstract": {
                "title_content_0": "Objective: This systematic review evaluates the current applications, advantages, and challenges of large language models (LLMs) in melanoma care. Methods: A systematic search was conducted in PubMed and Scopus databases for studies published up to 23 July 2024, focusing on the application of LLMs in melanoma. The review adhered to PRISMA guidelines, and the risk of bias was assessed using the modified QUADAS-2 tool. Results: Nine studies were included, categorized into subgroups: patient education, diagnosis, and clinical management. In patient education, LLMs demonstrated high accuracy, though readability often exceeded recommended levels. For diagnosis, multimodal LLMs like GPT-4V showed capabilities in distinguishing melanoma from benign lesions, but accuracy varied, influenced by factors such as image quality and integration of clinical context. Regarding management advice, ChatGPT provided more reliable recommendations compared to other LLMs, but all models lacked depth for individualized decision-making. Conclusions: LLMs, particularly multimodal models, show potential in improving melanoma care. However, current applications require further refinement and validation. Future studies should explore fine-tuning these models on large, diverse dermatological databases and incorporate expert knowledge to address limitations such as generalizability across different populations and skin types."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.3390/jcm13237480",
                "pmid": null,
                "pmc": "PMC11642440",
                "pub_year": "2024",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11642440/"
            }
        },
        {
            "title": "Utilization of Artificial Intelligence to Improve Equitable Healthcare Access for Breast Implant Patients",
            "abstract": {
                "title": "Abstract",
                "title_content_0": "Recently, mandated FDA patient decision checklists were developed with the goal of improving the informed decision-making process for patients considering breast implants. However, these checklists are written at reading levels far higher than recommended by the National Institutes of Health and the American Medical Association. This study aims to improve the accessibility, and therefore, the utility of the mandated FDA patient literature for the average breast implant patient using the assistance of artificial intelligence (AI). Patient decision checklists were obtained from the 3 most utilized breast implant manufacturers in the United States\u2014Allergan, Mentor, and Sientra. A novel patient decision checklist was synthesized by AI, written at the sixth grade reading level, using these checklists as source material. The AI-assisted checklist was edited by plastic surgeons for both formatting and content. The overall readability of Allergan, Mentor, and Sientra patient checklists correlated with the college reading level. These documents were of a statistically significantly higher reading level than the AI-assisted checklist, which was written at the recommended sixth grade level. Text composition analysis similarly demonstrated substantial differences between the AI-assisted and FDA-mandated literature. The currently mandated breast implant patient checklists are written at a college reading level and are inaccessible to the average patient. The authors propose a new patient decision checklist, generated with the assistance of AI, to improve healthcare access within plastic surgery. This simplified material can be used as an adjunct to the current checklists to improve shared decision making."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1093/asjof/ojae093",
                "pmid": null,
                "pmc": "PMC11646116",
                "pub_year": "2024",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11646116/"
            }
        },
        {
            "title": "Quality and readability of web-based Arabic health information on COVID-19: an infodemiological study",
            "abstract": {
                "Background": "This study sought to assess the quality and readability of web-based Arabic health information on COVID-19.",
                "Methods": "Three search engines were searched on 13 April 2020 for specific Arabic terms on COVID-19. The first 100 consecutive websites from each engine were analyzed for eligibility, which resulted in a sample of 36 websites. These websites were subjected to quality assessments using the Journal of the American Medical Association (JAMA) benchmarks tool, the DISCERN tool, and Health on the Net Foundation Code of Conduct (HONcode) certification. The readability of the websites was assessed using an online readability calculator.",
                "Results": "Among the 36 eligible websites, only one (2.7%) was HONcode certified. No website attained a high score based on the criteria of the DISCERN tool; the mean score of all websites was 31.5 \u00b1 12.55. As regards the JAMA benchmarks results, a mean score of 2.08 \u00b1 1.05 was achieved by the websites; however, only four (11.1%) met all the JAMA criteria. The average grade levels for readability were 7.2 \u00b1 7.5, 3.3 \u00b1 0.6 and 93.5 \u00b1 19.4 for the Flesch Kincaid Grade Level, Simple Measure of Gobbledygook, and Flesch Reading Ease scales, respectively.",
                "Conclusion": "Almost all of the most easily accessible web-based Arabic health information on COVID-19 does not meet recognized quality standards regardless of the level of readability and ability to be understood by the general population of Arabic speakers.",
                "Supplementary Information": "The online version contains supplementary material available at 10.1186/s12889-021-10218-9."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1186/s12889-021-10218-9",
                "pmid": "33461516",
                "pmc": "PMC7812558",
                "pub_year": "2021",
                "url": "https://pubmed.ncbi.nlm.nih.gov/33461516/"
            }
        },
        {
            "title": "The Brain Injury Screening Tool (BIST): Tool development, factor structure and validity",
            "abstract": {
                "title_content_0": "Currently health care pathways (the combination and order of services that a patient receives to manage their injury) following a mild traumatic brain injury vary considerably. Some clinicians lack confidence in injury recognition, management and knowing when to refer. A clinical expert group developed the Brain Injury Screening Tool (BIST) to provide guidance on health care pathways based on clinical indicators of poor recovery. The tool aims to facilitate access to specialist services (if required) to improve longer term prognosis. The tool was developed using a three-step process including: 1) domain mapping; 2) item development and 3) item testing and review. An online retrospective survey of 114 adults (>16 years) who had experienced a mild brain injury in the past 10 years was used to determine the initial psychometric properties of the 15-item symptom scale of the BIST. Participants were randomised to complete the BIST and one of two existing symptom scales; the Rivermead Post-concussion Symptom Questionnaire (RPQ) or the Sports Concussion Assessment Test (SCAT-5) symptom scale to determine concurrent validity. Participant responses to the BIST symptom scale items were used to determine scale reliability using Cronbach\u2019s alpha. A principal components analysis explored the underlying factor structure. Spearman\u2019s correlation coefficients determined concurrent validity with the RPQ and SCAT-5 symptom scales. The 15 items were found to require a reading age of 6\u20138 years old using readability statistics. High concurrent validity was shown against the RPQ (r = 0.91) and SCAT-5 (r = 0.90). The BIST total symptom scale (\u03b1 = 0.94) and the three factors identified demonstrated excellent internal consistency: physical/emotional (\u03b1 = 0.90), cognitive (\u03b1 = 0.92) and vestibular-ocular (\u03b1 = 0.80). This study provides evidence to support the utility, internal consistency, factor structure and concurrent validity of the BIST. Further research is warranted to determine the utility of the BIST scoring criteria and responsiveness to change in patients."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1371/journal.pone.0246512",
                "pmid": "33539482",
                "pmc": "PMC7861451",
                "pub_year": "2021",
                "url": "https://pubmed.ncbi.nlm.nih.gov/33539482/"
            }
        },
        {
            "title": "Quality and readability of web-based Arabic health information on periodontal disease",
            "abstract": {
                "Background": "Currently, the Internet seems to be a helpful tool for obtaining information about everything that we think about, including diseases, their prevention and treatment approaches. However, doubts exist regarding the quality and readability of such information. This study sought to assess the quality and readability of web-based Arabic information on periodontal disease.",
                "Methods": "In this infodemiological study, the Google, Yahoo!, and Bing search engines were searched using specific Arabic terms on periodontal disease. The first 100 consecutive websites from each engine were obtained. The eligible websites were categorized as commercial, health/professional, journalism, and other. The following tools were applied to assess the quality of the information on the included websites: the Health on the Net Foundation Code of Conduct (HONcode), the Journal of the American Medical Association (JAMA) benchmarks, and the DISCERN tool. The readability was assessed using an online readability tool.",
                "Results": "Of the 300 websites, 89 were eligible for quality and readability analyses. Only two websites (2.3%) were HONcode certified. Based on the DISCERN tool, 43 (48.3%) websites had low scores. The mean score of the JAMA benchmarks was 1.6\u2009\u00b1\u20091.0, but only 3 (3.4%) websites achieved \u201cyes\u201d responses for all four JAMA criteria. Based on the DISCERN tool, health/professional websites revealed the highest quality of information compared to other website categories. Most of the health/professional websites revealed moderate-quality information, while 55% of the commercial websites, 66% of journalism websites, and 43% of other websites showed poor quality information. Regarding readability, most of the analyzed websites presented simple and readable written content.",
                "Conclusions": "Aside from readable content, Arabic health information on the analyzed websites on periodontal disease is below the required level of quality."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1186/s12911-021-01413-0",
                "pmid": "33541345",
                "pmc": "PMC7863442",
                "pub_year": "2021",
                "url": "https://pubmed.ncbi.nlm.nih.gov/33541345/"
            }
        },
        {
            "title": "Evaluation of MicroScan Bacterial Identification Panels for Low-Resource Settings",
            "abstract": {
                "title_content_0": "Bacterial identification is challenging in low-resource settings (LRS). We evaluated the MicroScan identification panels (Beckman Coulter, Brea, CA, USA) as part of M\u00e9decins Sans Fronti\u00e8res\u2019 Mini-lab Project. The MicroScan Dried Overnight Positive ID Type 3 (PID3) panels for Gram-positive organisms and Dried Overnight Negative ID Type 2 (NID2) panels for Gram-negative organisms were assessed with 367 clinical isolates from LRS. Robustness was studied by inoculating Gram-negative species on the Gram-positive panel and vice versa. The ease of use of the panels and readability of the instructions for use (IFU) were evaluated. Of species represented in the MicroScan database, 94.6% (185/195) of Gram-negative and 85.9% (110/128) of Gram-positive isolates were correctly identified up to species level. Of species not represented in the database (e.g., Streptococcus suis and Bacillus spp.), 53.1% out of 49 isolates were incorrectly identified as non-related bacterial species. Testing of Gram-positive isolates on Gram-negative panels and vice versa (n = 144) resulted in incorrect identifications for 38.2% of tested isolates. The readability level of the IFU was considered too high for LRS. Inoculation of the panels was favorably evaluated, whereas the visual reading of the panels was considered error-prone. In conclusion, the accuracy of the MicroScan identification panels was excellent for Gram-negative species and good for Gram-positive species. Improvements in stability, robustness, and ease of use have been identified to assure adaptation to LRS constraints."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.3390/diagnostics11020349",
                "pmid": "33669829",
                "pmc": "PMC7922174",
                "pub_year": "2021",
                "url": "https://pubmed.ncbi.nlm.nih.gov/33669829/"
            }
        },
        {
            "title": "Written Informed Consent\u2014Translating into Plain Language. A Pilot Study",
            "abstract": {
                "title_content_0": "Background: Informed consent is important in clinical practice, as a person\u2019s written consent is required prior to many medical interventions. Many informed consent forms fail to communicate simply and clearly. The aim of our study was to create an easy-to-understand form. Methods: Our assessment of a Polish-language plastic surgery informed consent form used the Polish-language comprehension analysis program (jasnopis.pl, SWPS University) to assess the readability of texts written for people of various education levels; and this enabled us to modify the form by shortening sentences and simplifying words. The form was re-assessed with the same software and subsequently given to 160 adult volunteers to assess the revised form\u2019s degree of difficulty or readability. Results: The first software analysis found the language was suitable for people with a university degree or higher education, and after revision and re-assessment became suitable for persons with 4\u20136 years of primary school education and above. Most study participants also assessed the form as completely comprehensible. Conclusions: There are significant benefits possible for patients and practitioners by improving the comprehensibility of written informed consent forms."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.3390/healthcare9020232",
                "pmid": "33672624",
                "pmc": "PMC7924197",
                "pub_year": "2021",
                "url": "https://pubmed.ncbi.nlm.nih.gov/33672624/"
            }
        },
        {
            "title": "HyperBeta: characterizing the structural dynamics of proteins and self-assembling peptides",
            "abstract": {
                "title_content_0": "Self-assembling processes are ubiquitous phenomena that drive the organization and the hierarchical formation of complex molecular systems. The investigation of assembling dynamics, emerging from the interactions among biomolecules like amino-acids and polypeptides, is fundamental to determine how a mixture of simple objects can yield a complex structure at the nano-scale level. In this paper we present HyperBeta, a novel open-source software that exploits an innovative algorithm based on hyper-graphs to efficiently identify and graphically represent the dynamics of \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\beta$$\\end{document}\u03b2-sheets formation. Differently from the existing tools, HyperBeta directly manipulates data generated by means of coarse-grained molecular dynamics simulation tools (GROMACS), performed using the MARTINI force field. Coarse-grained molecular structures are visualized using HyperBeta \u2019s proprietary real-time high-quality 3D engine, which provides a plethora of analysis tools and statistical information, controlled by means of an intuitive event-based graphical user interface. The high-quality renderer relies on a variety of visual cues to improve the readability and interpretability of distance and depth relationships between peptides. We show that HyperBeta is able to track the \\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\beta$$\\end{document}\u03b2-sheets formation in coarse-grained molecular dynamics simulations, and provides a completely new and efficient mean for the investigation of the kinetics of these nano-structures. HyperBeta will therefore facilitate biotechnological and medical research where these structural elements play a crucial role, such as the development of novel high-performance biomaterials in tissue engineering, or a better comprehension of the molecular mechanisms at the basis of complex pathologies like Alzheimer\u2019s disease."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1038/s41598-021-87087-0",
                "pmid": "33833280",
                "pmc": "PMC8032683",
                "pub_year": "2021",
                "url": "https://pubmed.ncbi.nlm.nih.gov/33833280/"
            }
        },
        {
            "title": "The Readability of Patient Education Materials Pertaining to Gastrointestinal Procedures",
            "abstract": {
                "Introduction": "Due to the ubiquity and ease of access of Internet, patients are able to access online health information more easily than ever. The American Medical Association recommends that patient education materials be targeted at or below the 6th grade level in order to accommodate a wider audience. In this study, we evaluate the difficulty of educational materials pertaining to common GI procedures; we analyze on the readability of online education materials for colonoscopy, flexible sigmoidoscopy, and esophagogastroduodenoscopy (EGD).",
                "Methods": "Google search was performed using keywords of \u201ccolonoscopy,\u201d \u201csigmoidoscopy,\u201d and \u201cEGD\u201d with \u201cpatient information\u201d at the end of each search term. The texts from a total of 18 studies, 6 for each of the procedures, were then saved. Each study was also subdivided into \u201cIntroduction,\u201d \u201cPreparation,\u201d \u201cComplications,\u201d and if available, \u201cAlternatives.\u201d Furthermore, medical terminology that was properly explained, proper nouns, medication names, and other copyright text were removed in order to prevent inflation of the difficulty. Five validated readability tests were used to analyze each study and subsections: Coleman-Liau, New Dale-Chall, Flesch-Kincaid, Gunning Fog, SMOG.",
                "Results": "Studies on colonoscopy, flexible sigmoidoscopy, and EGD had median readability grades of 9.7, 10.2, and 11.0, respectively. Analysis of the subsections revealed that the \u201cAlternative\u201d subsection was the most difficult to comprehend with a readability score of 11.4, whereas the \u201cIntroduction\u201d subsection was the easiest to comprehend with a readability score of 9.5.",
                "Conclusion": "Despite modifications to the studies that improved the readability scores, patient education materials were still significantly above the recommended 6th grade level across all websites. This study emphasizes that clear and simple language is warranted in order to create information that is suitable for most patients."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1155/2021/7532905",
                "pmid": "33869107",
                "pmc": "PMC8035024",
                "pub_year": "2021",
                "url": "https://pubmed.ncbi.nlm.nih.gov/33869107/"
            }
        },
        {
            "title": "Transjugular intrahepatic portosystemic shunt (TIPS) procedure: an assessment of the quality and readability of online information",
            "abstract": {
                "Purpose": "Transjugular intrahepatic portosystemic shunt (TIPS) procedure is an established procedure carried out by interventional radiologists to achieve portal decompression and to manage the complications of portal hypertension. The aim of this study was to evaluate the quality and readability of information available online for TIPS procedure.",
                "Methods": "Websites were identified using the search terms \u201cTIPS procedure\u201d, \u201cTIPSS procedure\u201d, \u201ctransjugular intrahepatic portosystemic shunt procedure\u201d, with the first 25 pages from the three most popular search engines (Google, Bing and Yahoo) being selected for evaluation with a total of 225. Each Website was grouped by authorship into one of five categories: (1) Physician, (2) Academic, (3) For-profit, (4) Non-profit (including government and public health), or (5) Other (discussion/social media). Readability of each Website was assessed using the Flesch-Reading Ease score, Flesch\u2013Kincaid grade level, Gunning-Fog Index, Coleman\u2013Liau and SMOG index. Quality was calculated using the DISCERN instrument, the Journal of the American Medical Association (JAMA) benchmark criteria and the presence of Health on the Net (HON) code certification.",
                "Results": "After disregarding duplicate and non-accessible Websites a total of 81 were included. The mean DISCERN score assessing the quality of information provided by Websites was \u201cgood\u201d (59.3\u2009\u00b1\u200910.2) with adherence to the JAMA Benchmark being 54.3%. Websites with HON-code certification were statistically significantly higher in terms of DISCERN (p\u2009=\u20090.034) and JAMA scores (p\u2009=\u20090.003) compared to HON-code negative sites. The readability scores of Websites ranged from 10 to 12th grade across calculators. Thirty-two out of the 81 Websites were targeted towards patients (39.5%), 46 towards medical professionals (56.8%) and 3 were aimed at neither (3.7%). The medical professional aimed Websites were statistically significantly more difficulty to read across all readability formulas (all p\u2009<\u20090.001).",
                "Conclusion": "While quality of online information available to patients is \u201cgood\u201d, the average readability for information on the internet for TIPS is set far above the recommended 7th-grade level. Academic Websites were of the highest quality, yet most challenging for the general public to read. These findings call for the production of high-quality and comprehensible content around TIPS procedure, where physicians can reliably direct their patients for information."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1186/s12911-021-01513-x",
                "pmid": "33952225",
                "pmc": "PMC8101024",
                "pub_year": "2021",
                "url": "https://pubmed.ncbi.nlm.nih.gov/33952225/"
            }
        },
        {
            "title": "The Value of Web-Based Patient Education Materials on Transarterial Chemoembolization: Systematic Review",
            "abstract": {
                "Background": "Thousands of web searches are performed related to transarterial chemoembolization (TACE), given its palliative role in the treatment of liver cancer.",
                "Objective": "This study aims to assess the reliability, quality, completeness, readability, understandability, and actionability of websites that provide information on TACE for patients.",
                "Methods": "The five most popular keywords pertaining to TACE were searched on Google, Yahoo, and Bing. General website characteristics and the presence of Health On the Net Foundation code certification were documented. Website assessment was performed using the following scores: DISCERN, Journal of the American Medical Association, Flesch-Kincaid Grade Level, Flesch Reading Ease Score, and the Patient Education Materials Assessment Tool. A novel TACE content score was generated to evaluate website completeness.",
                "Results": "The search yielded 3750 websites. In total, 81 website entities belonging to 78 website domains met the inclusion criteria. A medical disclaimer was not provided on 28% (22/78) of website domains. Health On the Net code certification was present on 12% (9/78) of website domains. Authorship was absent on 88% (71/81) of websites, and sources were absent on 83% (67/81) of websites. The date of publication or of the last update was not listed on 58% (47/81) of websites. The median DISCERN score was 47.0 (IQR 40.5-54.0). The median TACE content score was 35 (IQR 27-43). The median readability grade level was in the 11th grade. Overall, 61% (49/81) and 16% (13/81) of websites were deemed understandable and actionable, respectively. Not-for-profit websites fared significantly better on the Journal of the American Medical Association, DISCERN, and TACE content scores.",
                "Conclusions": "The content referring to TACE that is currently available on the web is unreliable, incomplete, difficult to read, understandable but not actionable, and characterized by low overall quality. Websites need to revise their content to optimally educate consumers and support shared decision-making.",
                "Trial Registration": "PROSPERO International Prospective Register of Systematic Reviews CRD42020202747; https://www.crd.york.ac.uk/prospero/display_record.php?ID=CRD42020202747"
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.2196/25357",
                "pmid": "33960948",
                "pmc": "PMC8140383",
                "pub_year": "2021",
                "url": "https://pubmed.ncbi.nlm.nih.gov/33960948/"
            }
        },
        {
            "title": "Development and Content Validation of an Instrument to Measure Medication Self-Management in Older Adults",
            "abstract": {
                "title_content_0": "Background: For older adults, the capacity to self-manage medications may be limited by several factors. However, currently available tools do not permit a comprehensive assessment of such limitations. The Domain Specific Limitation in Medication Management Capacity (DSL-MMC) was developed to address this need. This study aimed to establish the face and content validity of the DSL-MMC. Methods: The DSL-MMC tool consisted of 4 domains and 12 sub-domains with 42 items including: 1. physical abilities (vision, dexterity, hearing); 2. cognition (comprehension, memory, executive functioning); 3. medication regimen complexity (dosing regimen, non-oral administration, polypharmacy); and 4. access/caregiver (prescription refill, new prescription, caregiver). Pharmacists assessed each item for relevance, importance, readability, understandability, and representation. Items with content validity index (CVI) scores of <0.80 for relevance were examined for revision or removal. Results: Twelve pharmacists participated in the study. CVI scores for relevance and importance of domains were 1.0; of the sub-domains, two were below 0.80. Among the 42 items, 35 (83%) and 30 (71%) maintained CVI scores above 0.80 for relevance and importance, respectively. Five items were removed, three were merged and seven were modified due to low CVI scores and/or feedback. Conclusion: The DSL-MMC has been validated for content."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.3390/pharmacy9020078",
                "pmid": "33920490",
                "pmc": "PMC8167785",
                "pub_year": "2021",
                "url": "https://pubmed.ncbi.nlm.nih.gov/33920490/"
            }
        },
        {
            "title": "An objective analysis of quality and readability of online information on COVID-19",
            "abstract": {
                "title_content_0": "High quality, readable health information is vital to mitigate the impact of the COVID-19 pandemic. The aim of this study was to assess the quality and readability of online COVID-19 information using 6 validated tools. This is a cross-sectional study. \u201cCOVID-19\u201d was searched across the three most popular English language search engines. Quality was evaluated using the DISCERN score, Journal of the American Medical Association benchmark criteria and Health On the Net Foundation Code of Conduct. Readability was assessed using the Flesch Reading Ease Score, Flesch-Kincaid Grade Level and Gunning-Fog Index. 41 websites were suitable for analysis. 9.8% fulfilled all JAMA criteria. Only one website was HONCode certified. Mean DISCERN score was 47.8/80 (\u201cfair\u201d). This was highest in websites published by a professional society/medical journal/healthcare provider. Readability varied from an 8th\u00a0to 12th\u00a0grade level. The overall quality of online COVID-19 information was \u201cfair\u201d. Much of this information was above the recommended 5th to 6th grade level, impeding access for many."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1007/s12553-021-00574-2",
                "pmid": "34189011",
                "pmc": "PMC8222704",
                "pub_year": "2021",
                "url": "https://pubmed.ncbi.nlm.nih.gov/34189011/"
            }
        },
        {
            "title": "A Web Interface for Antibiotic Prescription Recommendations in Primary Care: User-Centered Design Approach",
            "abstract": {
                "Background": "Antibiotic misuse is a serious public health problem worldwide. National health authorities release clinical practice guidelines (CPGs) to guide general practitioners (GPs) in their choice of antibiotics. However, despite the large-scale dissemination of CPGs, GPs continue to prescribe antibiotics that are not recommended as first-line treatments. This nonadherence to recommendations may be due to GPs misunderstanding the CPGs. A web interface displaying antibiotic prescription recommendations and their justifications could help to improve the comprehensibility and readability of CPGs, thereby increasing the adoption of recommendations regarding antibiotic treatment.",
                "Objective": "This study aims to design and evaluate a web interface for antibiotic prescription displaying both the recommended antibiotics and their justifications in the form of antibiotic properties.",
                "Methods": "A web interface was designed according to the same principles as e-commerce interfaces and was assessed by 117 GPs. These GPs were asked to answer 17 questions relating to the usefulness, user-friendliness, and comprehensibility and readability of the interface, and their satisfaction with it. Responses were recorded on a 4-point Likert scale (ranging from \u201cabsolutely disagree\u201d to \u201cabsolutely agree\u201d). At the end of the evaluation, the GPs were allowed to provide optional, additional free comments.",
                "Results": "The antibiotic prescription web interface consists of three main sections: a clinical summary section, a filter section, and a recommended antibiotics section. The majority of GPs appreciated the clinical summary (90/117, 76.9%) and filter (98/117, 83.8%) sections, whereas 48.7% (57/117) of them reported difficulty reading some of the icons in the recommended antibiotics section. Overall, 82.9% (97/117) of GPs found the display of drug properties useful, and 65.8% (77/117) reported that the web interface improved their understanding of CPG recommendations.",
                "Conclusions": "The web interface displaying antibiotic recommendations and their properties can help doctors understand the rationale underlying CPG recommendations regarding antibiotic treatment, but further improvements are required before its implementation into a clinical decision support system."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.2196/25741",
                "pmid": "34114958",
                "pmc": "PMC8235275",
                "pub_year": "2021",
                "url": "https://pubmed.ncbi.nlm.nih.gov/34114958/"
            }
        },
        {
            "title": "Decision aids linked to evidence summaries and clinical practice guidelines: results from user-testing in clinical encounters",
            "abstract": {
                "Background": "Tools for shared decision-making (e.g. decision aids) are intended to support health care professionals and patients engaged in clinical encounters involving shared decision-making. However, decision aids are hard to produce, and onerous to update. Consequently, they often do not reflect best current evidence, and show limited uptake in practice. In response, we initiated the Sharing Evidence to Inform Treatment decisions (SHARE-IT) project. Our goal was to develop and refine a new generation of decision aids that are generically produced along digitally structured guidelines and evidence summaries.",
                "Methods": "Applying principles of human-centred design and following the International Patient Decision Aid Standards (IPDAS) and GRADE methods for trustworthy evidence summaries we developed a decision aid prototype in collaboration with the Developing and Evaluating Communication strategies to support Informed Decisions and practice based on Evidence project (DECIDE). We iteratively user-tested the prototype in clinical consultations between clinicians and patients. Semi-structured interviews of participating clinicians and patients were conducted. Qualitative content analysis of both user-testing sessions and interviews was performed and results categorized according to a revised Morville\u2019s framework of user-experience. We made it possible to produce, publish and use these decision aids in an electronic guideline authoring and publication platform (MAGICapp).",
                "Results": "Direct observations and analysis of user-testing of 28 clinical consultations between physicians and patients informed four major iterations that addressed readability, understandability, usability and ways to cope with information overload. Participants reported that the tool supported natural flow of the conversation and induced a positive shift in consultation habits towards shared decision-making. We integrated the functionality of SHARE-IT decision aids in MAGICapp, which has since generated numerous decision aids.",
                "Conclusion": "Our study provides a proof of concept that encounter decision aids can be generically produced from GRADE evidence summaries and clinical guidelines. Online authoring and publication platforms can help scale up production including continuous updating of electronic encounter decision aids, fully integrated with evidence summaries and clinical practice guidelines.",
                "Supplementary Information": "The online version contains supplementary material available at 10.1186/s12911-021-01541-7."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1186/s12911-021-01541-7",
                "pmid": "34187484",
                "pmc": "PMC8240084",
                "pub_year": "2021",
                "url": "https://pubmed.ncbi.nlm.nih.gov/34187484/"
            }
        },
        {
            "title": "Creation and Worldwide Utilisation of New COVID-19 Online Information Hub for Genetics Health Professionals, Patients and Families",
            "abstract": {
                "title_content_0": "The current COVID-19 pandemic has unfortunately resulted in many significant concerns for individuals with genetic disorders and their relatives, regarding the viral infection and, particularly, its specific implications and additional advisable precautions for individuals affected by genetic disorders. To address this, the resulting requirement for guidance and information for the public and for genetics professionals was discussed among colleagues nationally, on the ScotGEN Steering Committee, and internationally on the Education Committee of the European Society of Human Genetics (ESHG). It was agreed that the creation of an online hub of genetics-related COVID-19 information resources would be particularly helpful. The proposed content, divided into a web page for professionals and a page for patients, was discussed with, and approved by, genetics professionals. The hub was created and provided online at www.scotgen.org.uk and linked from the ESHG\u2019s educational website for genetics and genomics, at www.eurogems.org. The new hub provides links, summary information and representative illustrations for a wide range of selected international resources. The resources for professionals include: COVID-19 research related hubs provided by Nature, Science, Frontiers, and PubMed; clinical guidelines; the European Centre for Disease Prevention and Control; the World Health Organisation; and molecular data sources including coronavirus 3D protein structures. The resources for patients and families include links to many accessible sources of support and relevant information. Since the launch of the pages, the website has received visits from over 50 countries worldwide. Several genetics consultants have commented on usefulness, clarity, readability, and ease of navigation. Visits have originated most frequently in the United Kingdom, Kuwait, Hong Kong, Moldova, United States, Philippines, France, and Qatar. More links have been added since the launch of the hub to include additional international public health and academic resources. In conclusion, an up-to-date online hub has been created and made freely available for healthcare professionals, patients, relatives and the public, providing categorised easily navigated links to a range of worldwide resources related to COVID-19. These pages are receiving a rapidly growing number of return visits and the authors continue to maintain and update the pages\u2019 content, incorporating new developments in this field of enormous worldwide importance."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.3389/fgene.2021.621683",
                "pmid": "34305996",
                "pmc": "PMC8296805",
                "pub_year": "2021",
                "url": "https://pubmed.ncbi.nlm.nih.gov/34305996/"
            }
        },
        {
            "title": "Spanish Translation and Cultural Adaptation of the Fibromyalgia Knowledge Questionnaire",
            "abstract": {
                "title_content_0": "Introduction: Fibromyalgia (FM) translates into a reduction in the quality of life of people who suffer from it, being a chronic disease of unknown etiology. One of the most widespread treatments includes the combination of patient education, along with other components. At the educational level, the Fibromyalgia Knowledge Questionnaire (FKQ) is a tool that assesses knowledge of fibromyalgia. Objective: To obtain the translation and cultural adaptation of the FKQ questionnaire into Spanish, as well as its readability, in addition to knowing the relationship between knowledge of the disease and the level of disability. Method: In phase one, a translation-back translation and an evaluation of the readability of the questionnaire was carried out from INFLESZ, while in phase two, the questionnaire was passed to women with FM to detect their knowledge of the disease. A total of 49 women participated, with a mean age of 54.48 years. Results: The Spanish version of the FKQ questionnaire was rated by the participants in all its items as \u201cclear and understandable\u201d. The readability obtained by the questionnaire was similar to its original version, with both totals being in the \u201cnormal\u201d range, following the INFLESZ ranges. Regarding the patients\u2019 knowledge about FM, the component in which the highest score was obtained was physical activity (80% correct), while the one that obtained the worst score was knowledge about medication (50% correct). In addition, an inverse correlation was obtained between the FKQ and the FIQ (Fibromyalgia Impact Questionnaire) (r = \u22120.438; p < 0.01). Conclusions: The FKQ has been translated and culturally adapted, obtaining a correct understanding by the participants, as well as a degree of readability similar to the original questionnaire. Furthermore, it was obtained that, the lower the level of knowledge of the sick person, the greater the disability."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.3390/ijerph18147678",
                "pmid": "34300129",
                "pmc": "PMC8305758",
                "pub_year": "2021",
                "url": "https://pubmed.ncbi.nlm.nih.gov/34300129/"
            }
        },
        {
            "title": "Development and Validation of an Italian Adaptation of the Psychosocial Aspects of Hereditary Cancer Questionnaire",
            "abstract": {
                "title_content_0": "Individuals that attend cancer genetic counseling may experience test-related psychosocial problems that deserve clinical attention. In order to provide a reliable and valid first-line screening tool for these issues, Eijzenga and coworkers developed the Psychosocial Aspects of Hereditary Cancer (PAHC) questionnaire. The aim of this work was to develop an Italian adaptation of the PAHC (I-PACH). This prospective multicenter observational study included three stages: (1) development of a provisional version of the I-PAHC; (2) pilot studies aimed at testing item readability and revising the questionnaire; and (3) a main study aimed at testing the reliability and validity of the final version of the I-PAHC with the administration of a battery comprising measures of depression, anxiety, worry, stress, and life problems to 271 counselees from four cancer genetic clinics. Adapting the original PAHC to the Italian context involved adding two further domains and expanding the emotions domain to include positive emotions. While most of the items were found to be easy to understand and score, some required revision to improve comprehensibility; others were considered irrelevant or redundant and therefore deleted. The final version showed adequate reliability and validity. The I-PAHC provides comprehensive content coverage of cancer genetic-specific psychosocial problems, is well accepted by counselees, and can be considered a sound assessment tool for psychosocial issues related to cancer genetic counseling and risk assessment in Italy."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.3389/fpsyg.2021.697300",
                "pmid": "34354641",
                "pmc": "PMC8329440",
                "pub_year": "2021",
                "url": "https://pubmed.ncbi.nlm.nih.gov/34354641/"
            }
        },
        {
            "title": "A Policy-Driven Approach to Secure Extraction of COVID-19 Data From Research Papers",
            "abstract": {
                "title_content_0": "The entire scientific and academic community has been mobilized to gain a better understanding of the COVID-19 disease and its impact on humanity. Most research related to COVID-19 needs to analyze large amounts of data in very little time. This urgency has made Big Data Analysis, and related questions around the privacy and security of the data, an extremely important part of research in the COVID-19 era. The White House OSTP has, for example, released a large dataset of papers related to COVID research from which the research community can extract knowledge and information. We show an example system with a machine learning-based knowledge extractor which draws out key medical information from COVID-19 related academic research papers. We represent this knowledge in a Knowledge Graph that uses the Unified Medical Language System (UMLS). However, publicly available studies rely on dataset that might have sensitive data. Extracting information from academic papers can potentially leak sensitive data, and protecting the security and privacy of this data is equally important. In this paper, we address the key challenges around the privacy and security of such information extraction and analysis systems. Policy regulations like HIPAA have updated the guidelines to access data, specifically, data related to COVID-19, securely. In the US, healthcare providers must also comply with the Office of Civil Rights (OCR) rules to protect data integrity in matters like plasma donation, media access to health care data, telehealth communications, etc. Privacy policies are typically short and unstructured HTML or PDF documents. We have created a framework to extract relevant knowledge from the health centers\u2019 policy documents and also represent these as a knowledge graph. Our framework helps to understand the extent to which individual provider policies comply with regulations and define access control policies that enforce the regulation rules on data in the knowledge graph extracted from COVID-related papers. Along with being compliant, privacy policies must also be transparent and easily understood by the clients. We analyze the relative readability of healthcare privacy policies and discuss the impact. In this paper, we develop a framework for access control decisions that uses policy compliance information to securely retrieve COVID data. We show how policy compliance information can be used to restrict access to COVID-19 data and information extracted from research papers."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.3389/fdata.2021.701966",
                "pmid": "34458724",
                "pmc": "PMC8387600",
                "pub_year": "2021",
                "url": "https://pubmed.ncbi.nlm.nih.gov/34458724/"
            }
        },
        {
            "title": "Predicting Health Material Accessibility: Development of Machine Learning Algorithms",
            "abstract": {
                "Background": "Current health information understandability research uses medical readability formulas to assess the cognitive difficulty of health education resources. This is based on an implicit assumption that medical domain knowledge represented by uncommon words or jargon form the sole barriers to health information access among the public. Our study challenged this by showing that, for readers from non-English speaking backgrounds with higher education attainment, semantic features of English health texts that underpin the knowledge structure of English health texts, rather than medical jargon, can explain the cognitive accessibility of health materials among readers with better understanding of English health terms yet limited exposure to English-based health education environments and traditions.",
                "Objective": "Our study explores multidimensional semantic features for developing machine learning algorithms to predict the perceived level of cognitive accessibility of English health materials on health risks and diseases for young adults enrolled in Australian tertiary institutes. We compared algorithms to evaluate the cognitive accessibility of health information for nonnative English speakers with advanced education levels yet limited exposure to English health education environments.",
                "Methods": "We used 113 semantic features to measure the content complexity and accessibility of original English resources. Using 1000 English health texts collected from Australian and international health organization websites rated by overseas tertiary students, we compared machine learning (decision tree, support vector machine [SVM], ensemble tree, and logistic regression) after hyperparameter optimization (grid search for the best hyperparameter combination of minimal classification errors). We applied 5-fold cross-validation on the whole data set for the model training and testing, and calculated the area under the operating characteristic curve (AUC), sensitivity, specificity, and accuracy as the measurement of the model performance.",
                "Results": "We developed and compared 4 machine learning algorithms using multidimensional semantic features as predictors. The results showed that ensemble classifier (LogitBoost) outperformed in terms of AUC (0.858), sensitivity (0.787), specificity (0.813), and accuracy (0.802). Support vector machine (AUC 0.848, sensitivity 0.783, specificity 0.791, and accuracy 0.786) and decision tree (AUC 0.754, sensitivity 0.7174, specificity 0.7424, and accuracy 0.732) followed. Ensemble classifier (LogitBoost), support vector machine, and decision tree achieved statistically significant improvement over logistic regression in AUC, sensitivity, specificity, and accuracy. Support vector machine reached statistically significant improvement over decision tree in AUC and accuracy. As the best performing algorithm, ensemble classifier (LogitBoost) reached statistically significant improvement over decision tree in AUC, sensitivity, specificity, and accuracy.",
                "Conclusions": "Our study shows that cognitive accessibility of English health texts is not limited to word length and sentence length as had been conventionally measured by medical readability formulas. We compared machine learning algorithms based on semantic features to explore the cognitive accessibility of health information for nonnative English speakers. The results showed the new models reached statistically increased AUC, sensitivity, and accuracy to predict health resource accessibility for the target readership. Our study illustrated that semantic features such as cognitive ability\u2013related semantic features, communicative actions and processes, power relationships in health care settings, and lexical familiarity and diversity of health texts are large contributors to the comprehension of health information; for readers such as international students, semantic features of health texts outweigh syntax and domain knowledge."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.2196/29175",
                "pmid": "34468321",
                "pmc": "PMC8444043",
                "pub_year": "2021",
                "url": "https://pubmed.ncbi.nlm.nih.gov/34468321/"
            }
        },
        {
            "title": "The impact of a computerized physician order entry system implementation on 20 different criteria of medication documentation\u2014a before-and-after study",
            "abstract": {
                "Background": "The medication process is complex and error-prone. To avoid medication errors, a medication order should fulfil certain criteria, such as good readability and comprehensiveness. In this context, a computerized physician order entry (CPOE) system can be helpful. This study aims to investigate the distinct effects on the quality of prescription documentation of a CPOE system implemented on general wards in a large tertiary care hospital.",
                "Methods": "In a retrospective analysis, the prescriptions of two groups of 160 patients each were evaluated, with data collected before and after the introduction of a CPOE system. According to nationally available recommendations on prescription documentation, it was assessed whether each prescription fulfilled the established 20 criteria for a safe, complete, and actionable prescription. The resulting fulfilment scores (prescription-Fscores) were compared between the pre-implementation and the post-implementation group and a multivariable analysis was performed to identify the effects of further covariates, i.e., the prescription category, the ward, and the number of concurrently prescribed drugs. Additionally, the fulfilment of the 20 criteria was assessed at an individual criterion-level (denoted criteria-Fscores).",
                "Results": "The overall mean prescription-Fscore increased from 57.4%\u2009\u00b1\u200912.0% (n\u2009=\u20091850 prescriptions) before to 89.8%\u2009\u00b1\u20097.2% (n\u2009=\u20091592 prescriptions) after the implementation (p\u2009<\u20090.001). At the level of individual criteria, criteria-Fscores significantly improved in most criteria (n\u2009=\u200914), with 6 criteria reaching a total score of 100% after CPOE implementation. Four criteria showed no statistically significant difference and in two criteria, criteria-Fscores deteriorated significantly. A multivariable analysis confirmed the large impact of the CPOE implementation on prescription-Fscores which was consistent when adjusting for the confounding potential of further covariates.",
                "Conclusions": "While the quality of prescription documentation generally increases with implementation of a CPOE system, certain criteria are difficult to fulfil even with the help of a CPOE system. This highlights the need to accompany a CPOE implementation with a thorough evaluation that can provide important information on possible improvements of the software, training needs of prescribers, or the necessity of modifying the underlying clinical processes.",
                "Supplementary Information": "The online version contains supplementary material available at 10.1186/s12911-021-01607-6."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1186/s12911-021-01607-6",
                "pmid": "34635100",
                "pmc": "PMC8504043",
                "pub_year": "2021",
                "url": "https://pubmed.ncbi.nlm.nih.gov/34635100/"
            }
        },
        {
            "title": "Open notes sounds great, but will a provider\u2019s documentation\nchange? An exploratory study of the effect of open notes on oncology\ndocumentation",
            "abstract": {
                "title": "Abstract",
                "Objective": "The effects of shared clinical notes on patients, care partners, and\nclinicians (\u201copen notes\u201d) were first studied as a\ndemonstration project in 2010. Since then, multiple studies have shown\nclinicians agree shared progress notes are beneficial to patients, and\npatients and care partners report benefits from reading notes. To determine\nif implementing open notes at a hematology/oncology practice changed\nproviders\u2019 documentation style, we assessed the length and\nreadability of clinicians\u2019 notes before and after open notes\nimplementation at an academic medical center in Boston, MA, USA.",
                "Materials and Methods": "We analyzed 143\u00a0888 notes from 60 hematology/oncology clinicians\nbefore and after the open notes debut at Beth Israel Deaconess Medical\nCenter, from January 1, 2012 to September 1, 2016. We measured the\nproviders\u2019 (medical doctor/nurse practitioner) documentation styles\nby analyzing character length, the number of addenda, note entry mode\n(dictated vs typed), and note readability. Measurements used 5 different\nreadability formulas and were assessed on notes written\nbefore and after the introduction of\nopen notes on November 25, 2013.",
                "Results": "After the introduction of open notes, the mean length of progress notes\nincreased from 6174 characters to 6648 characters\n(P\u2009<\u2009.001), and the mean character\nlength of the \u201cassessment and plan\u201d (A&P) increased\nfrom 1435 characters to 1597 characters\n(P\u2009<\u2009.001). The Average Grade Level\nReadability of progress notes decreased from 11.50 to 11.33, and overall\nreadability improved by 0.17\n(P\u2009=\u2009.01). There were no\nstatistically significant changes in the length or readability of\n\u201cInitial Notes\u201d or Letters, inter-doctor communication, nor\nin the modality of the recording of any kind of note.",
                "Conclusions": "After the implementation of open notes, progress notes and A&P\nsections became both longer and easier to read. This suggests clinician\ndocumenters may be responding to the perceived pressures of a transparent\nmedical records environment."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1093/jamiaopen/ooab051",
                "pmid": "34661067",
                "pmc": "PMC8518311",
                "pub_year": "2021",
                "url": "https://pubmed.ncbi.nlm.nih.gov/34661067/"
            }
        },
        {
            "title": "Readability of patient education materials related to radiation safety: What are the implications for patient-centred radiology care?",
            "abstract": {
                "Background": "Increasing numbers of patients and carers rely on online resources for healthcare information. Radiation safety can be misunderstood by patients and clinicians and lead to patient anxiety. We aimed to assess the readability of online patient educational materials (PEMs) related to radiation safety.",
                "Methods": "A total of 84 articles pertaining to radiation safety from 14 well-known online resources were identified. PEMs were then analysed using Readability Studio Professional Edition Version 2019. Readability was assessed using eight different instruments: the Flesch-Kincaid Reading Grade Level, Raygor Estimate, SMOG, Coleman\u2013Liau, Fry, FORCAST, Gunning Fog, and Flesch Reading Ease Score formula. The mean reading grade level (RGL) of each article was compared to the 6th and 8th grade reading level using 1-sample t-tests.",
                "Results": "The cumulative mean RGL for all 84 articles was 13.3 (range\u2009=\u20098.6\u201317.4), and none were written at or below the 6th or 8th grade level. The cumulative mean RGL exceeded the 6th grade reading level by an average of 7.3 levels (95% CI, 6.8\u20137.8; p\u2009<\u20090.001) and the 8th grade level by an average of 5.3 grade levels (95% CI, 4.8\u20135.8; p\u2009<\u20090.001). The mean Flesch Reading Ease Score was 39/100 (\u2018difficult\u2019).",
                "Conclusion": "Currently available online PEMs related to radiation safety are still written at higher than recommended reading levels. Radiation safety is a topic in which the specialist training of radiologists is crucial in providing guidance to patients. Addressing the readability of online PEMs can improve radiology-patient communication and support the shift to a patient-centred model of practice.",
                "Supplementary Information": "The online version contains supplementary material available at 10.1186/s13244-021-01094-3."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1186/s13244-021-01094-3",
                "pmid": "34674063",
                "pmc": "PMC8531160",
                "pub_year": "2021",
                "url": "https://pubmed.ncbi.nlm.nih.gov/34674063/"
            }
        },
        {
            "title": "A Comprehensive Evaluation of the Readability of Online Healthcare Materials Regarding Distal Radius Fractures",
            "abstract": {
                "title_content_0": "Introduction",
                "title_content_1": "Distal radius fractures (DRFs) are among the most common upper limb fractures reviewed in the emergency and orthopaedic departments. Approximately 40% of these fractures are unstable and require fixation to improve limb function. Confronted with an impending operation, many patients will access the internet, looking for information and reassurance. Previous studies have suggested that orthopaedic healthcare websites are beyond the comprehension of their target audience.",
                "title_content_2": "Objective",
                "title_content_3": "To assess the readability of healthcare websites regarding DRFs.",
                "title_content_4": "Methods",
                "title_content_5": "The terms distal radius fracture, broken wrist and wrist fracture were searched on Google and Bing. Of 101 websites initially considered, 52 unique websites underwent evaluation using readability software. Websites were assessed using two common methods for assessing readabilty; the Reading Grade Level (RGL) and the Flesch Reading Ease Score (FRES). In line with recommended guidelines and previous studies, an RGL of sixth grade and under and a FRES score above 65 was considered acceptable.",
                "title_content_6": "Results",
                "title_content_7": "The mean score for the FRES index was 56.67 (SD: \u00b1 19.6), which resulted in the majority of pieces assessed being classified as \u2018fairly difficult to read'. The mean RGL was 8.61 (SD: \u00b1 2.86); 17.3% of the websites assessed fulfilled the criteria of having an RGL of six or less. One way T-tests comparing the FRES and RGL mean scores against the acceptable standards showed that they failed to meet the acceptable indexes (FRES: P<0.004; 95% CI: -13.8 to -2.8; RGL: P<0.0001; CI: 1.8-3.4). ANOVA testing showed no significant difference based on category (FRES: P=0.791; RGL: P=0.101).",
                "title_content_8": "Conclusion",
                "title_content_9": "The level of comprehension required for online healthcare education materials related to distal radius fractures exceeds the recommended guidelines. Improving the readability content of these websites would enhance the internet\u2019s usability as an educational tool as well as improve patient post-operative outcomes."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.7759/cureus.18188",
                "pmid": "34707959",
                "pmc": "PMC8532190",
                "pub_year": "2021",
                "url": "https://pubmed.ncbi.nlm.nih.gov/34707959/"
            }
        },
        {
            "title": "Automatic Diagnosis of Mental Healthcare Information Actionability: Developing Binary Classifiers",
            "abstract": {
                "title_content_0": "We aimed to develop a quantitative instrument to assist with the automatic evaluation of the actionability of mental healthcare information. We collected and classified two large sets of mental health information from certified mental health websites: generic and patient-specific mental healthcare information. We compared the performance of the optimised classifier with popular readability tools and non-optimised classifiers in predicting mental health information of high actionability for people with mental disorders. sensitivity of the classifier using both semantic and structural features as variables achieved statistically higher than that of the binary classifier using either semantic (p < 0.001) or structural features (p = 0.0010). The specificity of the optimized classifier was statistically higher than that of the classifier using structural variables (p = 0.002) and the classifier using semantic variables (p = 0.001). Differences in specificity between the full-variable classifier and the optimised classifier were statistically insignificant (p = 0.687). These findings suggest the optimised classifier using as few as 19 semantic-structural variables was the best-performing classifier. By combining insights of linguistics and statistical analyses, we effectively increased the interpretability and the diagnostic utility of the binary classifiers to guide the development, evaluation of the actionability and usability of mental healthcare information."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.3390/ijerph182010743",
                "pmid": "34682483",
                "pmc": "PMC8536017",
                "pub_year": "2021",
                "url": "https://pubmed.ncbi.nlm.nih.gov/34682483/"
            }
        },
        {
            "title": "The Role of Physicians in Digitalizing Health Care Provision: Web-Based Survey Study",
            "abstract": {
                "Background": "Digitalization affects all areas of society, including the health care sector. However, the digitalization of health care provision is progressing slowly compared to other sectors. In the professional and political literature, physicians are partially portrayed as digitalization sceptics. Thus, the role of physicians in this process requires further investigation. The theory of \u201cdigital natives\u201d suggests a lower hurdle for younger generations to engage with digital technologies.",
                "Objective": "The objective of this study was to investigate the role of physicians in the process of digitalizing health care provision in Germany and to assess the age factor.",
                "Methods": "We conducted a large-scale study to assess the role of this professional group in the progress of the digital transformation of the German health care sector. Therefore, in an anonymous online survey, we inquired about the current digital penetration of the personal working environment, expectations, attitude toward, and concerns regarding digitalization. Based on these data, we studied associations with the nominal variable age and variations across 2 age groups.",
                "Results": "The 1274 participants included in the study generally showed a high affinity towards digitalization with a mean of 3.88 on a 5-point Likert scale; 723 respondents (56.75%) stated they personally use mobile apps in their everyday working life, with a weak tendency to be associated with the respondents\u2019 age (\u03b7=0.26). Participants saw the most noticeable existing benefits through digitalization in data quality and readability (882/1274, 69.23%) and the least in patient engagement (213/1274, 16.72%). Medical practitioners preponderantly expect further improvements through increased digitalization across almost all queried areas but the most in access to medical knowledge (1136/1274, 89.17%), treatment of orphan diseases (1016/1274, 79.75%), and medical research (1023/1274, 80.30%).",
                "Conclusions": "Respondents defined their role in the digitalization of health care provision as ambivalent: \u201cscrutinizing\u201d on the one hand but \u201cactive\u201d and \u201copen\u201d on the other. A gap between willingness to participate and digital sovereignty was indicated. Thus, education on digitalization as a means to support health care provision should not only be included in the course of study but also in the continuing process of further and advanced training."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.2196/31527",
                "pmid": "34545813",
                "pmc": "PMC8663562",
                "pub_year": "2021",
                "url": "https://pubmed.ncbi.nlm.nih.gov/34545813/"
            }
        },
        {
            "title": "Readability and Comprehension of Printed Patient Education Materials",
            "abstract": {
                "title_content_0": "Background: Health literacy, a recently determined construct plays an important role in how individuals are able to manage their health. A useful approach for the assessment of health literacy is to measure the comprehension of available patient education materials (PEMs).",
                "title_content_1": "Objective: We aimed at assessing the usefulness of PEMS available in Hungarian by testing comprehension of selected PEMs in different groups of users.",
                "title_content_2": "Methods: Comprehension of patient education materials in the domain of healthcare was tested by selecting PEMs and creating questions based on their text in 3 dimensions of health literacy: understand, process/appraise, apply/use. Twenty questions were created that could be answered without pre-existing knowledge by reading the appropriate text taken from PEMs. Comprehension was examined in four groups: laypersons, non-professional healthcare workers, 1st year healthcare students, and 5th year medical students. Readability indices were calculated for the same texts to which questions were created.",
                "title_content_3": "Results: Laypersons answered <50% of the PEMs-based questions correctly. Non-professional healthcare workers performed better with 57% of right answers but significantly worse than healthcare students or medical students. Those with at least high school qualification (maturity exam) showed significantly higher comprehension compared to those with lower educational attainment. Persons in good or very good health also had significantly better comprehension than those in less favorable health. All readability indices showed that comprehension of the tested PEMs required at least 10 years of schooling or more. Therefore, these PEMS are difficult to understand for persons with less than high school level of education.",
                "title_content_4": "Conclusion: Rephrasing of the investigated patient educational materials would be recommended so that they better fit the educational attainment of the Hungarian population. Evaluation of the readability and comprehensibility of other PEMs also seems warranted."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.3389/fpubh.2021.725840",
                "pmid": "34917569",
                "pmc": "PMC8670754",
                "pub_year": "2021",
                "url": "https://pubmed.ncbi.nlm.nih.gov/34917569/"
            }
        },
        {
            "title": "Laparoscopic Cholecystectomy: Evaluation of Web-Based Information",
            "abstract": {
                "title_content_0": "Introduction",
                "title_content_1": "Laparoscopic cholecystectomy (LC), the gold standard treatment for symptomatic gallstone disease, is the most common procedure performed by general surgeons worldwide. The internet remains to be a popular source of medical information. Our aim was to evaluate the quality and readability of information available on the web for patients undergoing LC and to compare the information provided by the National Health Service (NHS) and non-NHS websites.",
                "title_content_2": "Methods",
                "title_content_3": "We searched for the keywords \u2018laparoscopic cholecystectomy\u2019 using the three most popular search engines (Google, Yahoo and MSN) and looked at the first 50 websites only. The readability of each document was assessed using the Flesch Reading Ease (FRE) score. We checked Health on the Net Foundation Code of Conduct (HONcode) certification status, whether the sites had been checked by an expert and when the information was last updated.",
                "title_content_4": "Results",
                "title_content_5": "Fifty-five of the possible 150 sites were analysed thus excluding repetitions (n=65), irrelevant content (n=26) or inaccessible links (n=3). Only seven of those were HONcode-certified. The mean FRE score was 46 (range 0-68, SD=16.13). There were 13 NHS sites and 42 non-NHS sites. The mean FRE score for the NHS sites was significantly better compared to the non-NHS sites [58.31 (SD=5.01) vs 42.21 (SD=16.35); p=0.001]. Fifty-four per cent (54%) of the analysed websites had been checked by a medical expert and 22% were updated within the last year.",
                "title_content_6": "Conclusions",
                "title_content_7": "This study highlights the poor quality and readability of information on medical websites. The information provided by NHS sites have significantly better readability compared to non-NHS sites."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.7759/cureus.20897",
                "pmid": "35004076",
                "pmc": "PMC8722460",
                "pub_year": "2022",
                "url": "https://pubmed.ncbi.nlm.nih.gov/35004076/"
            }
        },
        {
            "title": "An Analysis of US Academic Medical Center Websites: Usability Study",
            "abstract": {
                "Background": "Health care organizations are tasked with providing web-based health resources and information. Usability refers to the ease of user experience on a website. In this study, we conducted a usability analysis of academic medical centers in the United States, which, to the best of our knowledge, has not been previously carried out.",
                "Objective": "The primary aims of the study were to the following: (1) adapt a preexisting usability scoring methodology to academic medical centers; (2) apply and test this methodology on a sample set of academic medical center websites; and (3) make recommendations from these results on potential areas of improvements for our sample of academic medical center websites.",
                "Methods": "All website usability testing took place from June 1, 2020, to December 15, 2020. We replicated a methodology developed in previous literature and applied it to academic medical centers. Our sample included 73 US academic medical centers. Usability was split into four broad categories: accessibility (the ability of those with low levels of computer literacy to access and navigate the hospital\u2019s website); marketing (the ability of websites to be found through search engines and the relevance of descriptions to the links provided); content quality (grammar, frequency of information updates, material relevancy, and readability); and technology (download speed, quality of the programming code, and website infrastructure). Using these tools, we scored each website in each category. The composite of key factors in each category contributed to an overall \u201cgeneral usability\u201d score for each website. An overall score was then calculated by applying a weighted percentage across all factors and was used for the final \u201coverall usability\u201d ranking.",
                "Results": "The category with the highest average score was technology, with a 0.82 (SD 0.068, SE 0.008). The lowest-performing category was content quality, with an average of 0.22 (SD 0.069, SE 0.008). As these numbers reflect weighted percentages as an integer, the higher the score, the greater the overall usability in that category.",
                "Conclusions": "Our data suggest that technology, on average, was the highest-scored variable among academic medical center websites. Because website functionality is essential to a user\u2019s experience, it is justified that academic medical centers invest in optimal website performance. The overall lowest-scored variable was content quality. A potential reason for this may be that academic medical center websites are usually larger in size, making it difficult to monitor the increased quantity of content. An easy way to improve this variable is to conduct more frequent website audits to assess readability, grammar, and relevance. Marketing is another area in which these organizations have potential for improvement. Our recommendation is that organizations utilize search engine optimization techniques to improve their online visibility and discoverability."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.2196/27750",
                "pmid": "34932015",
                "pmc": "PMC8734930",
                "pub_year": "2021",
                "url": "https://pubmed.ncbi.nlm.nih.gov/34932015/"
            }
        },
        {
            "title": "Predicting the impact of online news articles \u2013 is information necessary?",
            "abstract": {
                "title_content_0": "We exploit the Twitter platform to create a dataset of news articles derived from tweets concerning COVID-19, and use the associated tweets to define a number of popularity measures. The focus on (potentially) biomedical news articles allows the quantity of biomedically valid information (as extracted by biomedical relation extraction) to be included in the list of explored features. Aside from forming part of a systematic correlation exploration, the features \u2013 ranging from the semantic relations through readability measures to the article\u2019s digital content \u2013 are used within a number of machine learning classifier and regression algorithms. Unsurprisingly, the results support that for more complex articles (as determined by a readability measure) more sophisticated syntactic structure may be expected. A weak correlation is found with information within an article suggesting that other factors, such as numbers of videos, have a notable impact on the popularity of a news article. The best popularity prediction performance is obtained using a random forest machine learning algorithm, and the feature describing the quantity of biomedical information is in the top 3 most important features in almost a third of the experiments performed. Additionally, this feature is found to be more valuable than the widely used named entity recognition."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1007/s11042-021-11621-5",
                "pmid": "35035262",
                "pmc": "PMC8742664",
                "pub_year": "2022",
                "url": "https://pubmed.ncbi.nlm.nih.gov/35035262/"
            }
        },
        {
            "title": "Audit on nursing notes in a psychiatry in-patient setting",
            "abstract": {
                "Aims": "We aimed to assess the accessibility and informativeness of the content of daily nursing notes through an audit, and improve deficiencies identified.",
                "Background": "Nursing notes are an important source of observation findings, of in-ward psychiatry patients.There can be variations in the quality of the notes as well as information contained within.A basic level of clarity and information within all notes will be helpful in using these to inform the management of patients.",
                "Method": "An audit was carried-out in a ward treating working-age patients for psychiatric illnesses.Setting standards - standard required of a daily progress note was decided after discussion in multi-disciplinary team meeting (MDT). Clear language and information on; mental-state, medication, meals, physical health, personal care, activities, risks and use of leave, were identified as requirements.Retrospective audit - First audit cycle was carried-out by assessing the notes two weeks retrospectively. The assessment instrument used a qualitative measurement of the readability of the notes as well as quantitative assessment of the contents.Intervention - The standards set during the MDT, as well as a suggested format for recording notes, were communicated to the staff through email. Follow-up meetings with individual staff members and MDT, to evaluate staff satisfaction and new suggestions to improve the format were held. Difficulties staff encountered when implementing the format were discussed and resolved.Second audit cycle - Following implementation of the intervention, the notes were again assessed using the same instrument.",
                "Conclusion": "Difficulty in accessing information from the notes was noted in the first audit cycle. The average score for accessibility of information when scored on Likert scale + 3 to -3, was 1. Use of language scored 2 on average. On the second audit cycle, accessibility had increased to 3 on average while language score remained 2.Quantitative measurement was done for presence of information on; mental state, medication, meals, physical health, personal care, activities, risks and use of time away from ward. All of these parameters showed an increase in the post-intervention second audit cycle. Information on taking meals, medication, and physical health was present 100% of the time in the second cycle. Most improvement was in information on personal care which showed a five-fold increase, from 17% to 89%In conclusion, standard for nursing notes arrived via discussion and consensus in MDT, has been successful in improving the accessibility and information within nursing notes."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1192/bjo.2021.511",
                "pmid": null,
                "pmc": "PMC8770045",
                "pub_year": "2021",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC8770045/"
            }
        },
        {
            "title": "A prescription of information \u2013 promoting symptom self-management in people with functional neurological disorder (FND)",
            "abstract": {
                "Aims": "Functional Neurological Disorder (FND) is known to be associated with high healthcare resource utilisation and poor quality of life. Patients\u2019 understanding of the disorder is considered instrumental in improving prognosis.We produced a symptom self-management patient education strategy with a booklet and FND symptoms recording template in a community neuropsychiatry setting. We embedded this psychoeducation intervention in a post-nursing triage model of care.",
                "Method": "A co-production cycle of patient education material was implemented as part of a Quality Improvement Project (QIP) at East Kent Neuropsychiatry Service. Year 4 medical students completed their first QIP cycle involving 4 students, 2 multidisciplinary team members and 4 patients with functional neurological presentations. An FND leaflet and symptom recording template was produced and reviewed using feedback domains such as leaflet readability, perceived usefulness, and template design. The revised version of leaflet was then pilot-tested in second QIP cycle via email or post to 12 patients awaiting their group psychology or neuropsychiatry appointments for treatment of FND. The uptake and impact of leaflet was assessed using telephone-based structured feedback collection.",
                "Result": "The first QIP cycle included 10 participants and generated qualitative knowledge domains, providing examples of different types of FND presentations and a biological-psychological-social model explaining onset and/or recurrence of FND symptoms. Group patient feedback and co-production input allowed inclusion of the patient voice and a re-design of leaflet and symptom recording template.The second QIP cycle involved 12 participants: feedback was collected two weeks after circulation of patient education material. Only 5 participants (42%) had read and used their education leaflet and template during this period. Patients described the booklet as useful overall, but thought it to be more useful at the point of diagnosis and referral to neuropsychiatry. Qualitatively, patients wished there to be more emphasis on FND being explained as \u201cless psychiatric, more a neuropsychiatric problem\u201d, and that it would be \u201cvery good for someone who had just been diagnosed\u201d. 80% of responders rated the leaflet quality 8/10 or above. These respondents felt that the leaflet had helped them understand their condition better than they did previously. Usefulness of an additional self-formulation flowchart was rated as 8/10 or below by all patients - with several finding it difficult to use.",
                "Conclusion": "Our QIP supports the need for early patient education when discussing diagnosis of FND. The finding of 42% uptake within two weeks of leaflet dispatch is encouraging."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1192/bjo.2021.493",
                "pmid": null,
                "pmc": "PMC8770079",
                "pub_year": "2021",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC8770079/"
            }
        },
        {
            "title": "Co-Designing a New Yoga-Based Mindfulness Intervention for Survivors of Stroke: A Formative Evaluation",
            "abstract": {
                "title_content_0": "Movement-based mindfulness interventions (MBI) are complex, multi-component interventions for which the design process is rarely reported. For people with stroke, emerging evidence suggests benefits, but mainstream programs are generally unsuitable. We aimed to describe the processes involved and to conduct a formative evaluation of the development of a novel yoga-based MBI designed for survivors of stroke. We used the Medical Research Council complex interventions framework and principles of co-design. We purposefully approached health professionals and consumers to establish an advisory committee for developing the intervention. Members collaborated and iteratively reviewed the design and content of the program, formatted into a training manual. Four external yoga teachers independently reviewed the program. Formative evaluation included review of multiple data sources and documentation (e.g., formal meeting minutes, focus group discussions, researcher observations). The data were synthesized using inductive thematic analysis. Three broad themes emerged: (a) MBI content and terminology; (b) manual design and readability; and (c) barriers and enablers to deliver the intervention. Various perspectives and feedback on essential components guided finalizing the program. The design phase of a novel yoga-based MBI was strengthened by interdisciplinary, consumer contributions and peer review. The 12-week intervention is ready for testing among survivors of stroke."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.3390/neurolint14010001",
                "pmid": null,
                "pmc": "PMC8788460",
                "pub_year": "2021",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC8788460/"
            }
        },
        {
            "title": "Optimizing Readability and Format of Plain Language Summaries for Medical Research Articles: Cross-sectional Survey Study",
            "abstract": {
                "Background": "Plain language summaries (PLSs) are intended to provide readers with a clear, nontechnical, and easily understandable overview of medical and scientific literature; however, audience preferences for specific PLS formats have yet to be fully explored.",
                "Objective": "This study aims to evaluate the preferred readability level and format for PLSs of medical research articles of different disease states via a web-based survey of audiences of different age groups.",
                "Methods": "Articles describing phase III clinical trials published in top-level, peer-reviewed journals between May 2016 and May 2018 were identified for 3 chronic disease states representing a range of adult patient age groups: (1) psoriasis, a skin disease representative of younger patients; (2) multiple sclerosis (MS), a neurological disease representative of middle-aged patients; and (3) rheumatoid arthritis (RA), a painful joint disease representative of older patients. Four PLSs were developed for each research article, of which 3 were text-only summaries (written with high, medium, and low complexity) and 1 was an infographic. To evaluate each of the 4 PLS formats, a 20-question open survey (specific to one of the 3 diseases) was sent to a representative sample selected via UK-based patient association websites, Twitter, and Facebook patient groups. A weighted-average calculation was applied to respondents\u2019 ranked preferences for each PLS format.",
                "Results": "For all 3 articles, the weighted-average preference scores showed that infographic (psoriasis 2.91, MS 2.71, and RA 2.78) and medium-complexity text-based PLS (reading age 14-17 years, US Grade 9-11; psoriasis 2.90; MS 2.47; RA 2.77) were the two most preferred PLS formats.",
                "Conclusions": "Audience preferences should be accounted for when preparing PLSs to accompany peer-reviewed original research articles. Oversimplified text can be viewed negatively, and graphical summaries or medium-complexity text-based summaries appear to be the most popular.",
                "Plain Language Summary": "Patients and caregivers should have the chance to read about medical research in a format they can understand. However, we do not know much about the formats that people with different illnesses or ages prefer. Researchers wanted to find out more about this. They selected 3 medical articles about illnesses that affect different age groups: psoriasis (younger patients), multiple sclerosis (middle-aged patients), and rheumatoid arthritis (older patients). They created 4 summaries of each article. One was a graphical summary, and the other 3 were words-only summaries of high, medium, and low complexity. Then, the researchers posted surveys on UK patient group websites and Facebook patient groups to ask people what they thought of the summaries. The surveys were taken by 167 people. These people were patients with psoriasis, multiple sclerosis, or rheumatoid arthritis, or their caregivers. Most were women, and about half had a university degree. For each illness, most people preferred the graphical summary. Among the word-only summaries, most people preferred the medium-complexity wording written for a reading age of 14 to 17 years. People felt that the graphical and medium-complexity summaries were clear and concise, while the others used jargon or were too simple. Authors of medical articles should remember these results when writing summaries for patients. More research is needed about the preferences of other people, such as those with other illnesses. (See Multimedia Appendix 1 for the graphical summary of the plain language summary.)"
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.2196/22122",
                "pmid": "35014966",
                "pmc": "PMC8790687",
                "pub_year": "2022",
                "url": "https://pubmed.ncbi.nlm.nih.gov/35014966/"
            }
        },
        {
            "title": "Online Patient Education Materials Related to Lipoprotein(a): Readability Assessment",
            "abstract": {
                "Background": "Lipoprotein(a) (Lp(a)) is a highly proatherogenic lipid fraction that is a clinically significant risk modifier. Patients wanting to learn more about Lp(a) are likely to use online patient educational materials (OPEMs). However, the readability of OPEMs may exceed the health literacy of the public.",
                "Objective": "This study aims to assess the readability of OPEMs related to Lp(a). We hypothesized that the readability of these online materials would exceed the sixth grade level recommended by the American Medical Association.",
                "Methods": "Using an online search engine, we queried the top 20 search results from 10 commonly used Lp(a)-related search terms to identify a total of 200 websites. We excluded duplicate websites, advertised results, research journal articles, or non\u2013patient-directed materials, such as those intended only for health professionals or researchers. Grade level readability was calculated using 5 standard readability metrics (automated readability index, SMOG index, Coleman-Liau index, Gunning Fog score, Flesch-Kincaid score) to produce robust point (mean) and interval (CI) estimates of readability. Generalized estimating equations were used to model grade level readability by each search term, with the 5 readability scores nested within each OPEM.",
                "Results": "A total of 27 unique websites were identified for analysis. The average readability score for the aggregated results was a 12.2 (95% CI 10.9798-13.3978) grade level. OPEMs were grouped into 6 categories by primary source: industry, lay press, research foundation and nonprofit organizations, university or government, clinic, and other. The most readable category was OPEMs published by universities or government agencies (9.0, 95% CI 6.8-11.3). The least readable OPEMs on average were the ones published by the lay press (13.0, 95% CI 11.2-14.8). All categories exceeded the sixth grade reading level recommended by the American Medical Association.",
                "Conclusions": "Lack of access to readable OPEMs may disproportionately affect patients with low health literacy. Ensuring that online content is understandable by broad audiences is a necessary component of increasing the impact of novel therapeutics and recommendations regarding Lp(a)."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.2196/31284",
                "pmid": "35014955",
                "pmc": "PMC8790699",
                "pub_year": "2022",
                "url": "https://pubmed.ncbi.nlm.nih.gov/35014955/"
            }
        },
        {
            "title": "78696 A Qualitative Cross-Sectional Study of Leadership in a Pandemic: What do Students Value?",
            "abstract": {
                "title_content_0": "ABSTRACT IMPACT: This real-world study of what students value in crisis leadership fills an important gap in the literature and may inform future leadership development programs in undergraduate medical education. OBJECTIVES/GOALS: Leadership training is of growing importance and prevalence in medical education. The COVID-19 pandemic provides a unique insight into the qualities students value in leaders. Our qualitative study examined these leadership themes and provides a grounding for future development of leadership programs. METHODS/STUDY POPULATION: A conventional qualitative approach was used in order to allow open expression of ideas related to leadership in a pandemic. The authors developed a 5 free-text question survey instrument aimed to uncover student perceptions of leadership both during the current pandemic and in crises in general. A participant pilot was performed in order to ensure readability and ease of understanding. We used thematic analysis to examine the content of the survey responses, and inductive coding of the responses allowed identification of emerging themes. Medical students at the University of Michigan were surveyed. RESULTS/ANTICIPATED RESULTS: In total, 162 students completed the survey. The demographic characteristics of participants are shown in Table\u00a01. Median age was 25 years old (range, 22-39). There was good representation from the 4 classes in the medical school with 20-30% from each medical school class and 5% of dual degree students. Thematic analysis demonstrated that students value personal characteristics of excellence in their leaders with an orientation towards helping other people. Students believe that leaders must know how to interpret and use information and then that these leaders must be able to communicate expertly to guide organizations. The final theme that emerged is that effective leaders must commit to decisive action. DISCUSSION/SIGNIFICANCE OF FINDINGS: This study took place at a time of unprecedented crises and response examples were grounded in this real-world practice of leadership. These results and themes that emerged fill a critical gap and may facilitate future curriculum development for medical students and trainees."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1017/cts.2021.568",
                "pmid": null,
                "pmc": "PMC8827770",
                "pub_year": "2021",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC8827770/"
            }
        },
        {
            "title": "Online Information on Painful Sexual Dysfunction in Women: Quality Analysis of Websites in SPANISH about Dyspareunia, Vaginismus and Vulvodynia",
            "abstract": {
                "title_content_0": "The objective of this study was to evaluate the content, quality, and readability of websites containing information on dyspareunia, vaginismus, and vulvodynia in Spanish. Web pages were retrieved entering the terms \u201cdyspareunia\u201d, \u201cvaginismus\u201d, and \u201cvulvodynia\u201d in Google, Yahoo!, and Bing search engines. Two researchers employed the DISCERN and Berm\u00fadez-Tamayo questionnaires to analyze the content and quality of the websites, and the INFLESZ scale to evaluate their readability. IBM SPSS\u00ae version 25 statistical software was employed for data analysis. The internet search yielded 262 websites, 91 of which were included after applying the selection criteria. Websites with information on dyspareunia obtained median scores of 24 (30\u201321) in the DISCERN, 38 (41.0\u201335.5) in the Berm\u00fadez-Tamayo, and 55.3 (57.2\u201350.9) in the INFLESZ tools. The results for websites on vaginismus revealed median scores of 23.5 (30\u201320) in the DISCERN, 37 (42\u201335) in the Berm\u00fadez-Tamayo, and 52.9 (55.6\u201346.4) in the INFLESZ. Finally, the median scores for vulvodynia sites was 25.5 (30\u201320) in the DISCERN, 38 (43\u201333.7) in the Berm\u00fadez-Tamayo, and 54.2 (57.3\u201347.2) in the INFLESZ. These outcomes indicate that the quality of information in these websites is very low, while the overall quality of the web pages is moderate. Sites on vaginismus and vulvodynia were \u201csomewhat difficult\u201d to read, while readability was \u201cnormal\u201d for websites on dyspareunia. Healthcare professionals should be aware of the shortcomings of these websites and address them through therapeutic education with resources containing updated, quality information. This raises the need for health professionals to generate these resources themselves or for experts and/or scientific societies in the field to check the quality and timeliness of the contents, regardless of whether or not the websites are endorsed with quality seals."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.3390/ijerph19031506",
                "pmid": null,
                "pmc": "PMC8834735",
                "pub_year": "2022",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC8834735/"
            }
        },
        {
            "title": "How have governments supported citizens stranded abroad due to COVID-19 travel restrictions? A comparative analysis of the financial and health support in eleven countries",
            "abstract": {
                "Background": "In response to the continuing threat of importing novel coronavirus disease (COVID-19), many countries have implemented some form of border restriction. A repercussion of these restrictions has been that some travellers have found themselves stranded abroad unable to return to their country of residence, and in need for government support. Our analysis explores the COVID-19-related information and support options provided by 11 countries to their citizens stranded overseas due to travel restrictions. We also examined the quality (i.e., readability, accessibility, and useability) of the information that was available from selected governments\u2019 web-based resources.",
                "Methods": "Between June 18 to June 30, 2021, COVID-19-related webpages from 11 countries (Australia, New Zealand, Fiji, Canada, United States of America (USA), United Kingdom (UK), France, Spain, Japan, Singapore, and Thailand) were reviewed and content relating to information and support for citizens stuck overseas analysed. Government assistance-related data from each webpage was extracted and coded for the following themes: travel arrangements, health and wellbeing, finance and accommodation, information needs, and sources. Readability was examined using the Simplified Measure of Gobbledygook (SMOG) and the Flesch Kincaid readability tests; content \u2018accessibility\u2019 was measured using the Web Content Accessibility Guidelines (WCAG) Version 2.1; and content \u2018usability\u2019 assessed using the usability heuristics for website design tool.",
                "Results": "Ninety-eight webpages from 34 websites were evaluated. No country assessed covered all themes analysed. Most provided information and some level of support regarding repatriation options; border control and re-entry measures; medical assistance; and traveller registration. Only three countries provided information or support for emergency housing while abroad, and six provided some form of mental health support for their citizens. Our analysis of the quality of COVID-19-related information available on a subset of four countries\u2019 websites found poor readability and multiple accessibility and usability issues.",
                "Conclusion": "This study uniquely analyses government support for citizens stuck abroad during the COVID-19 pandemic. With large variance in the information and services available across the countries analysed, our results highlight gaps, inconsistencies, and potential inequities in support available, and raise issues pertinent to the quality, accessibility, and usability of information. This study will assist policymakers plan and communicate comprehensive support packages for citizens stuck abroad due to the COVID-19 situation and design future efforts to prepare for global public health emergencies.",
                "Supplementary Information": "The online version contains supplementary material available at 10.1186/s12879-022-07155-2."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1186/s12879-022-07155-2",
                "pmid": "35184735",
                "pmc": "PMC8858437",
                "pub_year": "2022",
                "url": "https://pubmed.ncbi.nlm.nih.gov/35184735/"
            }
        },
        {
            "title": "Coarse-Grained Neural Network Model of the Basal Ganglia to Simulate Reinforcement Learning Tasks",
            "abstract": {
                "title_content_0": "Computational models of the basal ganglia (BG) provide a mechanistic account of different phenomena observed during reinforcement learning tasks performed by healthy individuals, as well as by patients with various nervous or mental disorders. The aim of the present work was to develop a BG model that could represent a good compromise between simplicity and completeness. Based on more complex (fine-grained neural network, FGNN) models, we developed a new (coarse-grained neural network, CGNN) model by replacing layers of neurons with single nodes that represent the collective behavior of a given layer while preserving the fundamental anatomical structures of BG. We then compared the functionality of both the FGNN and CGNN models with respect to several reinforcement learning tasks that are based on BG circuitry, such as the Probabilistic Selection Task, Probabilistic Reversal Learning Task and Instructed Probabilistic Selection Task. We showed that CGNN still has a functionality that mirrors the behavior of the most often used reinforcement learning tasks in human studies. The simplification of the CGNN model reduces its flexibility but improves the readability of the signal flow in comparison to more detailed FGNN models and, thus, can help to a greater extent in the translation between clinical neuroscience and computational modeling."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.3390/brainsci12020262",
                "pmid": null,
                "pmc": "PMC8870197",
                "pub_year": "2022",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC8870197/"
            }
        },
        {
            "title": "The Quality and Reliability of Information in the Summaries of Product Characteristics",
            "abstract": {
                "title_content_0": "The Summary of Product Characteristics (SmPC) is an obligatory document concerning a medicine required (among other things) for the authorization of a medicinal product. The purpose of the SmPC is to provide product information to healthcare professionals. A necessary condition for this is to ensure that the SmPC is clear and precise. However, neither European nor national legislation obliges marketing authorization holders to review the SmPC in terms of its readability and understandability prior to the registration of a medicine. To date, research on SmPCs has focused on accuracy and completeness; however, the literature lacks information on the extent to which SmPCs meet the needs of healthcare professionals concerning the readability of the information they contain. The main objective of this article is to point out the lack of precision in the legal provisions for the preparation of SmPCs concerning the comprehensibility of the provisions. The article points to the lack of testing of the SmPC in terms of accessibility and transparency for healthcare professionals, highlighting that the document does not meet the needs of healthcare professionals in providing adequate information about medicines. It shows that the current rules and guidelines for the preparation of the registration dossier for a medicinal product are not entirely precise and contain numerous shortcomings."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.3390/ijerph19042185",
                "pmid": null,
                "pmc": "PMC8872284",
                "pub_year": "2022",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC8872284/"
            }
        },
        {
            "title": "Assessment of the Readability and Quality of Online Patient Education Material for Chronic Medical Conditions",
            "abstract": {
                "title_content_0": "Patient education materials (PEMs) were assessed from chronic health condition associations to determine their quality and if they were above the 6th grade reading level (GRL) recommended by the Centers for Disease Control and National Institutes of Health. PEMs from 55 associations were assessed for their GRL using ten readability scales and underwent a difficult word analysis. The associations had their quality assessed using two methods: the Journal of the American Medical Association (JAMA) Benchmarks and Health on the Net Foundation Code of Conduct certification (HONCode). Two thousand five hundred and ninety PEMs, collected between June and November 2021, were analyzed. The overall GRL average was 10.8 \u00b1 2.8, with a range of 0 to 19. Difficult word analysis showed that 15.8% \u00b1 4.8 contained complex words of 3 or more syllables and 25.7% \u00b1 6.3 contained words which were unfamiliar. No association displayed all four indicators of quality according to JAMA Benchmarks or held an up-to-date HONCode certification. The PEM readability continues to be written at a level above the recommended GRL. Additionally, the lack of quality indicators from the associations\u2019 websites may make it difficult for older adults to identify the sources as credible. This represents an opportunity to optimize materials that would be understood by a wider audience."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.3390/healthcare10020234",
                "pmid": null,
                "pmc": "PMC8872454",
                "pub_year": "2022",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC8872454/"
            }
        },
        {
            "title": "Readability of Korean-Language COVID-19 Information from the South Korean National COVID-19 Portal Intended for the General Public: Cross-sectional Infodemiology Study",
            "abstract": {
                "Background": "The coronavirus pandemic has increased reliance on the internet as a tool for disseminating information; however, information is useful only when it can be understood. Prior research has shown that web-based health information is not always easy to understand. It is not yet known whether the Korean-language COVID-19 information from the internet is easy for the general public to understand.",
                "Objective": "We aimed to evaluate the readability of Korean-language COVID-19 information intended for the general public from the national COVID-19 portal of South Korea.",
                "Methods": "A total of 122 publicly available COVID-19 information documents written in Korean were obtained from the South Korean national COVID-19 portal. We determined the level of readability (at or below ninth grade, 10th to 12th grade, college, or professional) of each document using a readability tool for Korean-language text. We measured the reading time, character count, word count, sentence count, and paragraph count for each document. We also evaluated the characteristics of difficult-to-read documents to modify the readability from difficult to easy.",
                "Results": "The median readability level was at a professional level; 90.2% (110/122) of the information was difficult to read. In all 4 topics, few documents were easy to read (overview: 5/12, 41.7%; prevention: 6/97, 6.2%; test: 0/5, 0%; treatment: 1/8, 12.5%; P=.006), with a median 11th-grade readability level for overview, a median professional readability level for prevention, and median college readability levels for test and treatment. Difficult-to-read information had the following characteristics in common: literacy style, medical jargon, and unnecessary detail.",
                "Conclusions": "In all 4 topics, most of the Korean-language COVID-19 web-based information intended for the general public provided by the national COVID-19 portal of South Korea was difficult to read; the median readability levels exceeded the recommended ninth-grade level. Readability should be a key consideration in developing public health documents, which play an important role in disease prevention and health promotion."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.2196/30085",
                "pmid": "35072633",
                "pmc": "PMC8896563",
                "pub_year": "2022",
                "url": "https://pubmed.ncbi.nlm.nih.gov/35072633/"
            }
        },
        {
            "title": "Patient-facing genetic and genomic mobile apps in the UK: a systematic review of content, functionality, and quality",
            "abstract": {
                "title_content_0": "Close relative (consanguineous) marriage is widely practised globally, and it increases the risk of genetic disorders. Mobile apps may increase awareness and education regarding the associated risks in a sensitive, engaging, and accessible manner. This systematic review of patient-facing genetic/genomic mobile apps explores content, function, and quality. We searched the NHS Apps Library and the UK Google Play and Apple App stores for patient-facing genomic/genetic smartphone apps. Descriptive information and information on content was extracted and summarized. Readability was examined using the Flesch\u2013Kincaid metrics. Two raters assessed each app, using the Mobile App Rating Scale (MARS) and the IMS Institute for Healthcare Informatics functionality score. A total of 754 apps were identified, of which 22 met the eligibility criteria. All apps intended to inform/educate users, while 32% analyzed genetic data, and 18% helped to diagnose genetic conditions. Most (68%) were clearly about genetics, but only 14% were affiliated with a medical/health body or charity, and only 36% had a privacy strategy. Mean reading scores were 35 (of 100), with the average reading age being equivalent to US grade 12 (UK year 13). On average, apps had 3.3 of the 11 IMS functionality criteria. The mean MARS quality score was 3.2\u2009\u00b1\u20090.7. Half met the minimum acceptability score (3 of 5). None had been formally evaluated. It was evident that there are few high-quality genomic/genetic patient-facing apps available in the UK. This demonstrates a need for an accessible, culturally sensitive, evidence-based app to improve genetic literacy within patient populations and specific communities.",
                "Supplementary Information": "The online version contains supplementary material available at 10.1007/s12687-022-00579-y."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1007/s12687-022-00579-y",
                "pmid": "35182377",
                "pmc": "PMC8941009",
                "pub_year": "2022",
                "url": "https://pubmed.ncbi.nlm.nih.gov/35182377/"
            }
        },
        {
            "title": "Assessment of the Readability of Web-Based Patient Education Material From Major Canadian Pediatric Associations: Cross-sectional Study",
            "abstract": {
                "Background": "Web-based patient education materials (PEMs) are frequently written above the recommended reading level in North America. Poor PEM readability limits the accessibility of medical information for individuals with average literacy levels or lower. Pediatric hospital and association websites have not only been shown to be a preferred source of information among caregivers but have also become a necessity during the COVID-19 pandemic. The readability of Canadian pediatric association websites has not yet been assessed.",
                "Objective": "The aim of this study is to determine if the content of PEMs from Canadian pediatric associations is written at a reading level that the majority of Canadians can understand.",
                "Methods": "A total of 258 PEMs were extracted from 10 Canadian pediatric associations and evaluated for their reading level using 10 validated readability scales. The PEMs underwent a difficult word analysis and comparisons between PEMs from different associations were conducted.",
                "Results": "Web-based PEMs were identified from 3 pediatric association websites, where the reading level (calculated as a grade level) was found to be an average of 8.8 (SD 1.8) for the Caring for Kids website, 9.5 (SD 2.2) for the Pediatric Endocrine Group website, and 13.1 (SD 2.1) for the Atlantic Pediatric Society website. The difficult word analysis identified that 19.9% (SD 6.6%) of words were unfamiliar, with 13.3% (SD 5.3%) and 31.9% (SD 6.1%) of words being considered complex (\u22653 syllables) and long (\u22656 letters), respectively.",
                "Conclusions": "The web-based PEMs were found to be written above the recommended seventh-grade reading level for Canadians. Consideration should be made to create PEMs at an appropriate reading level for both patients and their caregivers to encourage health literacy and ultimately promote preventative health behaviors and improve child health outcomes."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.2196/31820",
                "pmid": "35293875",
                "pmc": "PMC8968558",
                "pub_year": "2022",
                "url": "https://pubmed.ncbi.nlm.nih.gov/35293875/"
            }
        },
        {
            "title": "Quality and readability of online patient information on treatment for erectile dysfunction",
            "abstract": {
                "title": "Abstract",
                "Objectives": "To investigate the quality and readability of online patient information on treatment for erectile dysfunction using a Google search.",
                "Materials and methods": "The results of a Google search for \u201cerectile dysfunction treatment\u201d were reviewed. Webpages that contained written information on erectile dysfunction except those containing scientific publications and paywall protected webpages were included in further analysis. Typographic and treatment information were recorded. Readability was assessed using the Fleisch\u2010Kincaid grade level, the Gunning\u2010Fog index, the Coleman\u2010Liau index, and Simple Measure of Gobbledygook. Website quality was assessed using the DISCERN instrument, Journal of the American Medical Association (JAMA) benchmark criteria, and presence of Health on the net (HON) code certification. Website typography, discussed treatment types, readability scores, and quality measures were reported. Parametric and nonparametric statistical tests were used to compare the data as appropriate dependent on the normality of data.",
                "Results": "Eighty\u2010one webpages were included. Urologists and hospitals were the most common producers with 15 (18%) each. Seventy\u2010four (91%) webpages contained specific information on treatment for erectile dysfunction and 15 (19%) contained advertisements. Seventeen (21%) webpages were HON code certified. The median DISCERN score was 35 (IQR 26.5\u201044) out of 80. The mean combined readability score was 12.32 (SD 1.91). The median JAMA benchmark score was 1 (IQR 1\u20102) out of 4. Google rank had a small negative correlation with DISCERN score (\u03c4 = \u22120.16, P\u00a0=\u00a0.036). HON code certified webpages had higher DISCERN scores (median of 44 [IQR 35\u201058.5] vs 32.5 [IQR 25.25\u201042.25], U\u00a0=\u00a0832, Z\u00a0=\u00a06561, P\u00a0<\u00a0.001). A linear regression was used to predict DISCERN score based on meeting each JAMA benchmark criterion (F(2, 78) = 22.7, P\u00a0<\u00a0.001) R\n2 = 0.368, P\u00a0<\u00a0.001. Within this model the effects of meeting attribution (\u03b2\u00a0=\u00a011.09) and currency (\u03b2\u00a0=\u00a08.79) criterion were significant.",
                "Conclusions": "The quality of online information on treatment for erectile dysfunction is generally poor. However, easy to identify markers of quality like HON code certification, or meeting JAMA benchmark criterion for attribution and currency may help patients to navigate to better quality online information on treatment for erectile dysfunction. Webpages are written at senior high school level, above any recommendations for patient medical information. Health professionals should use validated instruments to assess the quality of online information on treatment for erectile dysfunction prior to publication to improve their utility for patients."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1002/bco2.87",
                "pmid": "35474701",
                "pmc": "PMC8988690",
                "pub_year": "2021",
                "url": "https://pubmed.ncbi.nlm.nih.gov/35474701/"
            }
        },
        {
            "title": "Masked emotions: Do face mask patterns and colors affect the recognition of emotions?",
            "abstract": {
                "title_content_0": "Previous research has shown that face masks impair the ability to perceive social information and the readability of emotions. These studies mostly explored the effect of standard medical, often white, masks on emotion recognition. However, in reality, many individuals prefer masks with different styles. We investigated whether the appearance of the mask (pattern: angular vs. curvy and color: black vs. white) affected the recognition of emotional states. Participants were asked to identify the emotions on faces covered by masks with different designs. The presence of masks resulted in decreasing accuracy and confidence and increasing reaction times, indicating that masks impair emotion recognition. There were no significant effects of angularity versus curvature or color on emotion recognition, which suggests that mask design may not impair the recognition beyond the effect of mere mask wearing. Besides, we found relationships between individual difference variables such as mask wearing attitudes, mask design preferences, individual traits and emotion recognition. The majority of participants demonstrated positive attitudes toward mask wearing and preferred non-patterned black and white masks. Preferences for white masks were associated with better emotion recognition of masked faces. In contrast, those with negative attitudes toward masks showed marginally poorer performance in emotion recognition for masked faces, and preferred patterned more than plain masks, perhaps viewing masks as a fashion item rather than a necessity. Moreover, preferences to wear patterned masks were negatively related to actual wearing\u00a0of masks indoors and perceived risks of COVID."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1186/s41235-022-00380-y",
                "pmid": "35394218",
                "pmc": "PMC8990494",
                "pub_year": "2022",
                "url": "https://pubmed.ncbi.nlm.nih.gov/35394218/"
            }
        },
        {
            "title": "Development and usability of a web-based patient-tailored tool to support adherence to urate-lowering therapy in gout",
            "abstract": {
                "Background": "The aim of this study is to develop and assess usability of a web-based patient-tailored tool to support adherence to urate-lowering therapy (ULT) among gout patients in a clinical setting.",
                "Methods": "The content of the tool was based on the Integrated Change (I-Change) model. This model combines various socio-cognitive theories and assumes behavioral change is a result of becoming aware of the necessity of change by integrating pre-motivational, motivational, and post-motivational factors. An expert group (five gout experts, three health services researchers, and one health behavior expert) was assembled that decided in three meetings on the tool\u2019s specific content (assessments and personalized feedback) using information from preparatory qualitative studies and literature reviews. Usability was tested by a think aloud approach and validated usability questionnaires.",
                "Results": "The I-Change Gout tool contains three consecutive sessions comprising 80 questions, 66 tailored textual feedback messages, and 40 tailored animated videos. Navigation through the sessions was determined by the patients\u2019 intention to adapt suboptimal ULT adherence. After the sessions, patients receive an overview of the personalized advices and plans to support ULT adherence. Usability testing among 20 gout patients that (ever) used ULT and seven healthcare professionals revealed an overall score for the tool of 8.4\u2009\u00b1\u20090.9 and 7.7\u2009\u00b1\u20091.0 (scale 1\u201310). Furthermore, participants reported a high intention to use and/or recommend the tool to others. Participants identified some issues for further improvement (e.g. redundant questions, technical issues, and text readability). If relevant, these were subsequently implemented in the I-Change Gout tool, to allow further testing among the following participants.",
                "Conclusion": "This study provides initial support for the usability by patients and healthcare professionals of the I-Change Gout tool to support ULT adherence behavior.",
                "Supplementary Information": "The online version contains supplementary material available at 10.1186/s12911-022-01833-6."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1186/s12911-022-01833-6",
                "pmid": null,
                "pmc": "PMC8991610",
                "pub_year": "2022",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC8991610/"
            }
        },
        {
            "title": "Evaluating the Impacts of Patient Engagement on Health Services Research Teams: Lessons from the Veteran Consulting Network",
            "abstract": {
                "Background": "Despite increasing commitment to patient engagement in research, evaluation of the impact of these efforts on research processes, products, and teams is limited.",
                "Objective": "To explore the impacts of engaging patients as consultants to research studies by examining the experiences, impacts, and lessons learned from a program facilitating patient engagement at a Veterans Health Administration research center.",
                "Design": "We developed a logic model to articulate the activities being implemented to support patient engagement and their anticipated outcomes. Then, we conducted qualitative, semi-structured interviews with participants in the local Veteran Consulting Network to qualitatively explore these outcomes.",
                "Participants": "Twelve researchers and eleven Veteran patients with experience working on at least one grant or funded study.",
                "Approach": "Interview transcripts were inductively coded using a consensus-based approach. Findings were synthesized using framework analysis and mapped back onto our logic model of expected patient engagement impacts.",
                "Key Results": "Patient engagement improved the perceived quality and relevance of research studies as patient consultants challenged researchers\u2019 assumptions about patient populations and clinical contexts and gave feedback that helped improve the feasibility of proposed grants, readability of study materials, comprehensiveness of study assessments, and cultural sensitivity and relevance of interventions. Patient engagement also had personal benefits to researchers and patients. Researchers reported improved communication skills and higher job satisfaction. Patients reported a sense of purpose and satisfaction from their work with greater awareness of and appreciation for research.",
                "Conclusions": "Engaging patients in research can have multiple benefits to the people and work involved. Our evaluation process can serve as a template for other organizations to plan for and assess the impact of their own patient engagement programs. Creating logic models and updating them based on feedback from program users make engagement goals explicit, help verify expected mechanisms to achieve impact, and facilitate organizational learning.",
                "Supplementary Information": "The online version contains supplementary material available at 10.1007/s11606-021-06987-z."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1007/s11606-021-06987-z",
                "pmid": "35349028",
                "pmc": "PMC8993982",
                "pub_year": "2022",
                "url": "https://pubmed.ncbi.nlm.nih.gov/35349028/"
            }
        },
        {
            "title": "Extract, transform, load framework for the conversion of health databases to OMOP",
            "abstract": {
                "title_content_0": "Common data models standardize the structures and semantics of health datasets, enabling reproducibility and large-scale studies that leverage the data from multiple locations and settings. The Observational Medical Outcomes Partnership Common Data Model (OMOP CDM) is one of the leading common data models. While there is a strong incentive to convert datasets to OMOP, the conversion is time and resource-intensive, leaving the research community in need of tools for mapping data to OMOP. We propose an extract, transform, load (ETL) framework that is metadata-driven and generic across source datasets. The ETL framework uses a new data manipulation language (DML) that organizes SQL snippets in YAML. Our framework includes a compiler that converts YAML files with mapping logic into an ETL script. Access to the ETL framework is available via a web application, allowing users to upload and edit YAML files via web editor and obtain an ETL SQL script for use in development environments. The structure of the DML maximizes readability, refactoring, and maintainability, while minimizing technical debt and standardizing the writing of ETL operations for mapping to OMOP. Our framework also supports transparency of the mapping process and reuse by different institutions."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1371/journal.pone.0266911",
                "pmid": null,
                "pmc": "PMC9000122",
                "pub_year": "2022",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC9000122/"
            }
        },
        {
            "title": "From the United Kingdom to Australia\u2014Adapting a Web-Based Self-management Education Program to Support the Management of Type 2 Diabetes: Tutorial",
            "abstract": {
                "title_content_0": "Diabetes self-management education and support can improve outcomes in people with diabetes. Providing health interventions via digital modes of delivery can extend the reach of programs delivered through traditional means. The web-based version of the Diabetes Education and Self-Management for Ongoing and Newly Diagnosed (MyDESMOND) is a digital diabetes education and support program for people with type 2 diabetes. The program was originally developed in the United Kingdom and is evidence-based, grounded in behavioral theory, and designed through a rigorous process of intervention mapping. As such, MyDESMOND was considered an ideal candidate for adaptation to the Australian setting. Program content and the digital platform were modified to suit the local context to increase the likelihood that the revised version of MyDESMOND will deliver similar outcomes to the original program. The aim of this paper is to describe the systematic processes undertaken to adapt the digital MyDESMOND diabetes education and support program for people with type 2 diabetes to the Australian setting. The adaptation involved a multidisciplinary group with a diverse range of skills and expertise\u2014a governance structure was established, a skilled project team was appointed, and stakeholder engagement was strategically planned. The adaptation of the program content included modifications to the clinical recommendations, the inclusion of local resources, practical changes, and revisions to optimize readability. A 2-stage independent review of the modified content was enacted. Digital adaptations were informed by relevant standards, local legislative requirements, and considerations of data sovereignty. The digital platform was extensively tested before deployment to the production setting. MyDESMOND is the first evidence-based digital diabetes education and support program for Australians with type 2 diabetes. This paper provides a road map for the adaptation of digital health interventions to new contexts."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.2196/26339",
                "pmid": "35442198",
                "pmc": "PMC9069279",
                "pub_year": "2022",
                "url": "https://pubmed.ncbi.nlm.nih.gov/35442198/"
            }
        },
        {
            "title": "Readability of English, German, and Russian Disease-Related Wikipedia Pages: Automated Computational Analysis",
            "abstract": {
                "Background": "Wikipedia is a popular encyclopedia for health- and disease-related information in which patients seek advice and guidance on the web. Yet, Wikipedia articles can be unsuitable as patient education materials, as investigated in previous studies that analyzed specific diseases or medical topics with a comparatively small sample size. Currently, no data are available on the average readability levels of all disease-related Wikipedia pages for the different localizations of this particular encyclopedia.",
                "Objective": "This study aimed to analyze disease-related Wikipedia pages written in English, German, and Russian using well-established readability metrics for each language.",
                "Methods": "Wikipedia database snapshots and Wikidata metadata were chosen as resources for data collection. Disease-related articles were retrieved separately for English, German, and Russian starting with the main concept of Human Diseases and Disorders (German: Krankheit; Russian: \u0417\u0430\u0431\u043e\u043b\u0435\u0432\u0430\u043d\u0438\u044f \u0447\u0435\u043b\u043e\u0432\u0435\u043a\u0430). In the case of existence, the corresponding International Classification of Diseases, Tenth Revision (ICD-10), codes were retrieved for each article. Next, the raw texts were extracted and readability metrics were computed.",
                "Results": "The number of articles included in this study for English, German, and Russian Wikipedia was n=6127, n=6024, and n=3314, respectively. Most disease-related articles had a Flesch Reading Ease (FRE) score <50.00, signaling difficult or very difficult educational material (English: 5937/6125, 96.93%; German: 6004/6022, 99.7%; Russian: 2647/3313, 79.9%). In total, 70% (7/10) of the analyzed articles could be assigned an ICD-10 code with certainty (English: 4235/6127, 69.12%; German: 4625/6024, 76.78%; Russian: 2316/3314, 69.89%). For articles with ICD-10 codes, the mean FRE scores were 28.69 (SD 11.00), 20.33 (SD 9.98), and 38.54 (SD 13.51) for English, German, and Russian, respectively. A total of 9 English ICD-10 chapters (11 German and 10 Russian) showed significant differences: chapter F (FRE 23.88, SD 9.95; P<.001), chapter E (FRE 25.14, SD 9.88; P<.001), chapter H (FRE 30.04, SD 10.57; P=.049), chapter I (FRE 30.05, SD 9.07; P=.04), chapter M (FRE 31.17, 11.94; P<.001), chapter T (FRE 32.06, SD 10.51; P=.001), chapter A (FRE 32.63, SD 9.25; P<.001), chapter B (FRE 33.24, SD 9.07; P<.001), and chapter S (FRE 39.02, SD 8.22; P<.001).",
                "Conclusions": "Disease-related English, German, and Russian Wikipedia articles cannot be recommended as patient education materials because a major fraction is difficult or very difficult to read. The authors of Wikipedia pages should carefully revise existing text materials for readers with a specific interest in a disease or its associated symptoms. Special attention should be given to articles on mental, behavioral, and neurodevelopmental disorders (ICD-10 chapter F) because these articles were most difficult to read in comparison with other ICD-10 chapters. Wikipedia readers should be supported by editors providing a short and easy-to-read summary for each article."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.2196/36835",
                "pmid": "35576562",
                "pmc": "PMC9152717",
                "pub_year": "2022",
                "url": "https://pubmed.ncbi.nlm.nih.gov/35576562/"
            }
        },
        {
            "title": "Evaluation of an online text simplification editor using manual and automated metrics for perceived and actual text difficulty",
            "abstract": {
                "title": "Abstract",
                "Objective": "Simplifying healthcare text to improve understanding is difficult but critical to improve health literacy. Unfortunately, few tools exist that have been shown objectively to improve text and understanding. We developed an online editor that integrates simplification algorithms that suggest concrete simplifications, all of which have been shown individually to affect text difficulty.",
                "Materials and Methods": "The editor was used by a health educator at a local community health center to simplify 4 texts. A controlled experiment was conducted with community center members to measure perceived and actual difficulty of the original and simplified texts. Perceived difficulty was measured using a Likert scale; actual difficulty with multiple-choice questions and with free recall of information evaluated by the educator and 2 sets of automated metrics.",
                "Results": "The results show that perceived difficulty improved with simplification. Several multiple-choice questions, measuring actual difficulty, were answered more correctly with the simplified text. Free recall of information showed no improvement based on the educator evaluation but was better for simplified texts when measured with automated metrics. Two follow-up analyses showed that self-reported education level and the amount of English spoken at home positively correlated with question accuracy for original texts and the effect disappears with simplified text.",
                "Discussion": "Simplifying text is difficult and the results are subtle. However, using a variety of different metrics helps quantify the effects of changes.",
                "Conclusion": "Text simplification can be supported by algorithmic tools. Without requiring tool training or linguistic knowledge, our simplification editor helped simplify healthcare related texts."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1093/jamiaopen/ooac044",
                "pmid": null,
                "pmc": "PMC9155254",
                "pub_year": "2022",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC9155254/"
            }
        },
        {
            "title": "Diabetes in the News: Readability Analysis of Malaysian Diabetes Corpus",
            "abstract": {
                "title_content_0": "This paper describes a study to evaluate the readability scores of Malaysian newspaper articles meant to create awareness of diabetes among the public. In contrast to patient-specific sources of information, mass media may potentially reach healthy people, thus preventing them from becoming part of the diabetes statistics. Articles published within a selected corpus from the years 2013 to 2018 and related to awareness regarding diabetes were sampled, and their readability was scored using Flesch Kinkaid Reading Ease (FKRE). Features of three articles ranked as the best and worst for readability were qualitatively analyzed. The average readability for the materials is low at 49.6 FKRE, which may impede the uptake of information contained in the articles. Feature analysis of articles with the best and worst readability indicates that medical practitioners may not be the best spokesperson to reach the public. It also indicates that simple sentence structures could help improve readability. There is still much room for improvement in attaining good public health literacy through mass media communication. Public health and media practitioners should be vigilant of the language aspects of their writing when reaching out to the public."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.3390/ijerph19116802",
                "pmid": null,
                "pmc": "PMC9180217",
                "pub_year": "2022",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC9180217/"
            }
        },
        {
            "title": "Development and Validation of a Clinical Practicum Assessment Tool for the NAACLS-Accredited Biomedical Science Program",
            "abstract": {
                "title_content_0": "Student perspectives on their final year clinical placements in biomedical sciences at Qatar University are assessed using the clinical practicum assessment tool (CPAT), which was developed in-house following accreditation body requirements. The tool, which we call the CPAT-Qatar University (CPAT-QU), covers the three clinical practicum domains: practicum content, preceptors, and competencies. Here, we validate this tool. The CPAT-QU has 27 Likert-scale questions and free-text open questions. CPAT-QU readability was calculated using the Flesch\u2013Kincaid Reading Ease (FKRE) instrument. Content validity was assessed using the average and universal average scale-level content validity indices (S-CVI/Average and S-CVI/UA). For construct validity, 50 employed graduates who had completed the practicum were consented for study participation, and the validity was calculated by a principal component analysis (PCA). Reliability was analyzed by Cronbach\u2019s alpha. The S-CVI/Average and S-CVI/UA were 0.90 and 0.59, respectively, indicating that an adequate proportion of the content was relevant. The PCA extracted two core components, which explained 63% of the variance in the CPAT-QU. Cronbach\u2019s alpha values for the items were within the acceptable range of 0.60\u20131.00, showing that internal consistency has a good level. CPAT-QU appears to be a useful tool for assessing student perspectives on their clinical placements; however, construct validity needs continuous improvement."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.3390/ijerph19116651",
                "pmid": null,
                "pmc": "PMC9180805",
                "pub_year": "2022",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC9180805/"
            }
        },
        {
            "title": "Online Patient Education Materials for Common Sports Injuries Are Written at Too-High of a Reading Level: A Systematic Review",
            "abstract": {
                "Purpose": "To determine the readability of online patient information for common sports injuries.",
                "Methods": "A systematic search of the literature using PubMed/MEDLINE, Embase, and the CINAHL databases was performed according to Preferred Reporting Items for Systematic Reviews and Meta-Analysis guidelines. Studies were included if they (1) were published between 2000 and September 2020, (2) were English-language publications and complete studies from peer-reviewed journals, (3) evaluated online information directed toward patients with common sports injuries.",
                "Results": "Eleven studies met inclusion criteria and were included. The mean Flesch-Kincaid Grade Level for online education information was 10.5, whereas the mean Flesch Reading Ease was 51.2, indicating existing health resources are written above the recommended readability grade level (no greater than a sixth-grade reading level). The mean DISCERN score was 41.5, indicating that the quality of information accessible to patients was fair. The accuracy of health content determined by the ACL-Specific Score was reported as moderate level (mean 8.85).",
                "Conclusions": "This study demonstrates that online patient information regarding common sports injuries the does not match the readability recommendations of the American Medical Association and National Institutes of health.",
                "Clinical Relevance": "Future health-related information should be written by qualified experts at a level that can be easily understood by patients of all health literacy levels. Surgeons should be more attentive to where patients get their information from and how they interpret it. Accurate, easy to understand educational tools can improve efforts to help patients identify misconceptions about treatment options, and to guide patients to choices that are consistent with their values."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1016/j.asmr.2021.12.017",
                "pmid": null,
                "pmc": "PMC9210373",
                "pub_year": "2022",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC9210373/"
            }
        },
        {
            "title": "Osteoporotic Vertebral Fractures: An Analysis of Readability and Quality of Web-Based information",
            "abstract": {
                "title_content_0": "Introduction",
                "title_content_1": "Vertebral compression fractures are among\u00a0the most common fragility fractures with significant morbidity and mortality. With an aging population, the incidence of these fractures is on the rise. In this age of social and electronic media, there is a plethora of online information available. While access to healthcare information has increased, most of these websites remain beyond the comprehension of their target audience.",
                "title_content_2": "Objective",
                "title_content_3": "To assess the readability and quality of online information regarding osteoporotic vertebral fractures.",
                "title_content_4": "Methods",
                "title_content_5": "A search for the terms osteoporotic vertebral fractures, osteoporotic spinal fractures, and fragility spinal fractures was performed using the top five search engines. Eighty-three websites were identified and analyzed. Quality assessment was done using the DISCERN and Journal of the American Medical Association (JAMA) tools while readability was analyzed using the Flesch Reading Ease Score (FRES), Flesch Kincaid Grade (FKG), and Gunning Fog Index (GFI).",
                "title_content_6": "Results",
                "title_content_7": "The mean DISCERN score was 39.55 while the mean JAMA was 2.2. Readability testing revealed a mean FRES score of 49.26 with 16 websites having a score of > 60, FKG 8.38, and\u00a0GFI of 9.51. 33 websites had an FKG score of 8 or below 8.",
                "title_content_8": "Conclusion",
                "title_content_9": "The above results indicate that web-based information relating to osteoporotic vertebral fractures is of variable quality and readability. Although 40 % of websites are at the eighth grade or below level, only 16 % of websites are above the FRES score of 60, which makes online information difficult to comprehend by an average patient."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.7759/cureus.26029",
                "pmid": null,
                "pmc": "PMC9288661",
                "pub_year": "2022",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC9288661/"
            }
        },
        {
            "title": "Assessing the Readability of Anesthesia-Related Patient Education Materials from Major Anesthesiology Organizations",
            "abstract": {
                "Introduction": "The National Institutes of Health (NIH), American Medical Association (AMA), and the US Department of Health and Human Services (USDHHS) recommend that patient education materials (PEMs) be written between the 4th to 6th grade reading level to ensure readability by the average American. In this study, we examine the reading levels of online patient education materials from major anesthesiology organizations.",
                "Methods": "Readability analysis of PEMs found on the websites of anesthesiology organizations was performed using the Flesch Reading Ease score, Flesch-Kincaid Grade Level, Simple Measure of Gobbledygook, Gunning Frequency of Gobbledygook, New Dale-Chall test, Coleman-Liau Index, New Fog Count, Raygor Readability Estimate, the FORCAST test, and the Fry Score.",
                "Results": "Most patient educational materials from the websites of the anesthesiology organizations evaluated were written at or above the 10th grade reading level.",
                "Conclusions": "Online patient education materials from the major anesthesiology societies are written at levels higher than an average American adult reading skill level and higher than recommended by National Institute of Health, American Medical Association, and US Department of Health and Human Services. Online resources should be revised to improve readability. Simplifying text, using shorter sentences and terms are strategies online resources can implement to improve readability. Future studies should incorporate comprehensibility, user-friendliness, and linguistic ease to further understand the implications on overall healthcare."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1155/2022/3284199",
                "pmid": null,
                "pmc": "PMC9300304",
                "pub_year": "2022",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC9300304/"
            }
        },
        {
            "title": "A Novel Decision Aid Improves Quality of Reproductive Decision-Making and Pregnancy Knowledge for Women with Inflammatory Bowel Disease",
            "abstract": {
                "Background": "Women with inflammatory bowel disease (IBD) with poor IBD-specific reproductive knowledge experience more childlessness and fear of IBD medications in pregnancy. The Pregnancy in IBD Decision Aid (PIDA), developed by an international multidisciplinary team, offers personalized online decision support regarding pregnancy in IBD.",
                "Aims": "Assess the impact of PIDA on quality of reproductive decision-making and pregnancy-related knowledge among preconception (PC) and pregnant patients with IBD, and evaluate acceptability to patients and clinicians.",
                "Methods": "PC and pregnant patients with IBD aged 18\u201345 completed questionnaires pre- and post-PIDA to assess quality of decision-making (Decisional Conflict Scale (DCS); Decision Self-Efficacy Scale (DSES) and IBD-in-pregnancy knowledge (Crohn's and Colitis Pregnancy Knowledge Score (CCPKnow)). Paired t test assessed for differences pre- and post-PIDA. Patients and clinicians completed acceptability surveys.",
                "Results": "DCS and DSES were completed by 74 patients (42 Crohn\u2019s disease, 32 ulcerative colitis); 41 PC and 33 pregnant. DCS improved significantly post-PIDA in PC patients regarding pregnancy planning (t(40)\u2009=\u20094.83, p\u2009<\u20090.0001, Cohen\u2019s dz\u2009=\u20090.75) and in pregnant patients regarding medication management (t(32)\u2009=\u20092.37, p\u2009=\u20090.0242, dz\u2009=\u20090.41). DSES for PC patients improved significantly post-PIDA (t(40)\u2009=\u2009-3.56, p\u2009=\u20090.001, dz\u2009=\u2009-0.56). CCPKnow improved significantly post-PIDA in PC (t(42)\u2009=\u20094.93, p\u2009<\u20090.0001, dz\u2009=\u2009-0.75) and pregnant patients (t(32)\u2009=\u20095.1, p\u2009<\u20090.0001, dz\u2009=\u2009-0.89). PIDA was deemed optimal for length, readability, and content amount and considered highly useful by patients (n\u2009=\u200973) and clinicians (n\u2009=\u200914).",
                "Conclusions": "Patients using PIDA developed an improved quality of reproductive decision-making and IBD-in-pregnancy knowledge. PIDA is an accessible tool that can empower women with IBD to make values-congruent, evidence-based decisions regarding pregnancy and may reduce voluntary childlessness.",
                "Supplementary Information": "The online version contains supplementary material available at 10.1007/s10620-022-07494-9."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1007/s10620-022-07494-9",
                "pmid": "35499712",
                "pmc": "PMC9352739",
                "pub_year": "2022",
                "url": "https://pubmed.ncbi.nlm.nih.gov/35499712/"
            }
        },
        {
            "title": "Validating the knowledge bank approach for personalized prediction of survival in acute myeloid leukemia: a reproducibility study",
            "abstract": {
                "title_content_0": "Reproducibility is not only essential for the integrity of scientific research but is also a prerequisite for model validation and refinement for the future application of predictive algorithms. However, reproducible research is becoming increasingly challenging, particularly in high-dimensional genomic data analyses with complex statistical or algorithmic techniques. Given that there are no mandatory requirements in most biomedical and statistical journals to provide the original data, analytical source code, or other relevant materials for publication, accessibility to these supplements naturally suggests a greater credibility of the published work. In this study, we performed a reproducibility assessment of the notable paper by Gerstung et al. (Nat Genet 49:332\u2013340, 2017) by rerunning the analysis using their original code and data, which are publicly accessible. Despite an open science setting, it was challenging to reproduce the entire research project; reasons included: incomplete data and documentation, suboptimal code readability, coding errors, limited portability of intensive computing performed on a specific platform, and an R computing environment that could no longer be re-established. We learn that the availability of code and data does not guarantee transparency and reproducibility of a study; paradoxically, the source code is still liable to error and obsolescence, essentially due to methodological and computational complexity, a lack of reproducibility checking at submission, and updates for software and operating environment. The complex code may also hide problematic methodological aspects of the proposed research. Building on the experience gained, we discuss the best programming and software engineering practices that could have been employed to improve reproducibility, and propose practical criteria for the conduct and reporting of reproducibility studies for future researchers.",
                "Supplementary Information": "The online version contains supplementary material available at 10.1007/s00439-022-02455-8."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1007/s00439-022-02455-8",
                "pmid": "35429300",
                "pmc": "PMC9360099",
                "pub_year": "2022",
                "url": "https://pubmed.ncbi.nlm.nih.gov/35429300/"
            }
        },
        {
            "title": "How Readable Are Consultant Psychiatrist Letters From the Mental Health Liaison Team?",
            "abstract": {
                "Aims": "To assess whether consultant discharge letters from the mental health liaison team are: 1. Written to patients as advised by NICE shared decision making guidance. 2. Easy to read using the Flesch Reading Ease Test as advised by the Academy of Medical Royal Colleges, which equates to a score of 60 to 70.",
                "Methods": "50 consultant discharge letters were collated from April to November 2021. Each letter was assessed whether they were written directly to a patient and scored according to their Flesch Reading Ease (FRE) and Flesch-Kincaid Grade Level (FKGL) via Microsoft Word.FRE scores a text from 0 to 100 from the average length of sentences and the number of syllables in words to indicate its difficulty to read. The higher the score achieved, the easier it is to read the text. It is a recommended tool by The Academy of Medical Royal Colleges\u2019 guidance on outpatient clinic letters, however, does not specify a target level of readability. A score of 60 to 70 equates to plain English easily understood by students aged 13 to 15 years and was concluded to be the equivocal score expressed in the guidance.4The FKGL presents a score as a U.S. grade level to indicate the level of education generally required to understand a text. Words per sentence and syllables per word are factored in to calculate the grade.5",
                "Results": "The median FRE was 50.9 (n = 50, IQR 8.9). Only one letter met the desired standard. The mean score was 50.6 (SD 6.4). This mean was significantly different from a hypothetical ideal mean of 65 (t(df) = 15.9(49), p < 0.0001) so could not, unfortunately, be explained by chance. The median FKGL was 10.1.",
                "Conclusion": "Overall, the letters were of greater difficulty than the desired score of both FRE and FKGL. Lay language and patient-directed writing will aid in improving scores."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1192/bjo.2022.438",
                "pmid": null,
                "pmc": "PMC9380079",
                "pub_year": "2022",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC9380079/"
            }
        },
        {
            "title": "Evaluation of Medical Information on Male Sexual Dysfunction on Baidu Encyclopedia and Wikipedia: Comparative Study",
            "abstract": {
                "Background": "Sexual dysfunction is a private set of disorders that may cause stigma for patients when discussing their private problems with doctors. They might also feel reluctant to initiate a face-to-face consultation. Internet searches are gradually becoming the first choice for people with sexual dysfunction to obtain health information. Globally, Wikipedia is the most popular and consulted validated encyclopedia website in the English-speaking world. Baidu Encyclopedia is becoming the dominant source in Chinese-speaking regions; however, the objectivity and readability of the content are yet to be evaluated.",
                "Objective": "Hence, we aimed to evaluate the reliability, readability, and objectivity of male sexual dysfunction content on Wikipedia and Baidu Encyclopedia.",
                "Methods": "The Chinese Baidu Encyclopedia and English Wikipedia were investigated. All possible synonymous and derivative keywords for the most common male sexual dysfunction, erectile dysfunction, premature ejaculation, and their most common complication, chronic prostatitis/chronic pelvic pain syndrome, were screened. Two doctors evaluated the articles on Chinese Baidu Encyclopedia and English Wikipedia. The Journal of the American Medical Association (JAMA) scoring system, DISCERN instrument, and Global Quality Score (GQS) were used to assess the quality of disease-related articles.",
                "Results": "The total DISCERN scores (P=.002) and JAMA scores (P=.001) for Wikipedia were significantly higher than those of Baidu Encyclopedia; there was no statistical difference between the GQS scores (P=.31) for these websites. Specifically, the DISCERN Section 1 score (P<.001) for Wikipedia was significantly higher than that of Baidu Encyclopedia, while the differences between the DISCERN Section 2 and 3 scores (P=.14 and P=.17, respectively) were minor. Furthermore, Wikipedia had a higher proportion of high total DISCERN scores (P<.001) and DISCERN Section 1 scores (P<.001) than Baidu Encyclopedia. Baidu Encyclopedia and Wikipedia both had low DISCERN Section 2 and 3 scores (P=.49 and P=.99, respectively), and most of these scores were low quality.",
                "Conclusions": "Wikipedia provides more reliable, higher quality, and more objective information than Baidu Encyclopedia. Yet, there are opportunities for both platforms to vastly improve their content quality. Moreover, both sites had similar poor quality content on treatment options. Joint efforts of physicians, physician associations, medical institutions, and internet platforms are needed to provide reliable, readable, and objective knowledge about diseases."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.2196/37339",
                "pmid": "35943768",
                "pmc": "PMC9399830",
                "pub_year": "2022",
                "url": "https://pubmed.ncbi.nlm.nih.gov/35943768/"
            }
        },
        {
            "title": "Computer-generated structured electronic medical records are preferable to conventional medical records for patients with acute abdominal pain - a prospective, double-blinded study",
            "abstract": {
                "Objectives": "Structured medical records improve readability and ensure the inclusion of information necessary for correct diagnosis and treatment. This is the first study to assess the quality of computer-generated structured medical records by comparing them to conventional medical records on patients with acute abdominal pain.",
                "Materials and methods": "A prospective double-blinded study was conducted in a tertiary referral center emergency department between January 2018 and June 2018. Patients were examined by emergency department physicians and by experience and inexperienced researcher. The researchers used a new electronical medical records system, which gathered data during the examination and the system generate structured medical records containing natural language. Conventional medical records dictated by physician and computer-generated medical records were compared by a group of independent clinicians.",
                "Results": "Ninety-nine patients were included. The overall quality of the computer-generated medical records was better than the quality of conventional human-generated medical records \u2013 the structure was similar or better in 99% of cases and the readability was similar or better in 86% of cases, p\u2009<\u20090.001. The quality of medical history, current illness, and findings of physical examinations were likewise better with the computer-generated recording. The results were similar when patients were examined by experienced or inexperienced researcher using the computer-generated recording.",
                "Discussion": "The quality of computer-generated structured medical records was superior to that of conventional medical records. The quality remained similar regardless of the researcher\u2019s level of experience. The system allows automatic risk scoring and easy access for quality control of patient care. We therefore consider that it would be useful in wider practice."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1007/s10916-022-01852-w",
                "pmid": "36008740",
                "pmc": "PMC9411218",
                "pub_year": "2022",
                "url": "https://pubmed.ncbi.nlm.nih.gov/36008740/"
            }
        },
        {
            "title": "Conclusiveness, linguistic characteristics and readability of Cochrane plain language summaries of intervention reviews: a cross-sectional study",
            "abstract": {
                "Background": "One of the most important formats to disseminate the evidence in health to different populations are Cochrane Plain Language Summaries (PLSs). PLSs should be written in a simplified language, easily understandable and providing clear message for the consumer. The aim of this study was to examine the extent to which PLSs are customized for lay persons, specifically by providing conclusive, comprehensible, and readable messages.",
                "Methods": "The study analyzed Cochrane PLSs of interventional studies (N\u2009=\u20094360) in the English language published from 1995 to 2019. We categorized the conclusiveness into one of the following categories: \u201cpositive\u201d, \u201cpositive inconclusive\u201d, \u201cno evidence\u201d, \u201cno opinion\u201d, \u201cnegative\u201d, \u201cnegative inconclusive\u201d, \u201cunclear\u201d, \u201cequal\u201d, \u201cequal inconclusive\u201d. Language characteristics were analyzed using Linguistic Inquiry and Word Count (LIWC) software. The level of readability was measured by SMOG (Simple Measure of Gobbledygook) index, indicating the number of years of education required to read the text. For each PLS, we also collected the following data: Cochrane Review Network, year of publication and number of authors.",
                "Results": "Most of the PLSs (80%) did not have a conclusive message. In 53% PLSs there was no concluding opinion about the studied intervention or the conclusion was unclear. The most frequent conclusiveness category was \u201cno opinion\u201d (30%), and its frequency increased over time. The conclusiveness categories were similarly dispersed across Cochrane Networks. PLSs were written in an objective style, with high levels of analytical tone and clout above neutral, but a lower relation to authenticity and tone. The median number of years of non-specific education needed to read the PLSs was 14.9 (IQR 13.8 to 16.1), indicating that the person needs almost 15\u2009years of general education to read the content with ease.",
                "Conclusion": "Most of the Cochrane PLSs provided no concluding opinion or unclear conclusion regarding the effects of analyzed intervention. Analysis of readability indicated that they may be difficult to read for the lay population without medical education. Our results indicate that PLSs may not be so plain, and that the writing of Cochrane PLSs requires more effort. Tools used in this study could improve PLSs and make them better suited for lay audiences.",
                "Supplementary Information": "The online version contains supplementary material available at 10.1186/s12874-022-01721-7."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1186/s12874-022-01721-7",
                "pmid": null,
                "pmc": "PMC9464378",
                "pub_year": "2022",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC9464378/"
            }
        },
        {
            "title": "Proximal Humerus Fracture: An Evaluation of the Readability and Value of Web-Based Knowledge",
            "abstract": {
                "title_content_0": "Background",
                "title_content_1": "In today\u2019s scientifically developed world, the majority of patients use different websites to explore sophisticated and varied health knowledge. Consequently, healthcare specialists remain concerned that patients may be betrayed. Currently, there is a scarcity of information on the importance and legibility of online health data on proximal humerus fractures. This study aimed to assess the readability and value of existing web-based evidence regarding fractures of the proximal humerus.",
                "title_content_2": "Methodology",
                "title_content_3": "A search of three keywords, namely, broken shoulder, proximal humerus fracture, and broken humerus, was performed using the top three internet search engines. The first five pages of every search browser were analyzed. After discarding duplicate websites, 80 websites were found to be suitable for the analysis. Website quality was scored using the Journal of the American Medical Association (JAMA) benchmark criteria and the DISCERN criteria. The presence or absence of the Health on the Net Foundation Code of Conduct (HON code) certification and author characteristics were noted. The degree of readability was measured using six unique parameters, namely, the Automated Readability Index, Flesch Reading Ease Score, SMOG Index, Coleman-Liau index, Flesch-Kincaid Grade Level, and Gunning-Fog Index.",
                "title_content_4": "Results",
                "title_content_5": "In total, 80 specific websites were fit for evaluation and analysis. On the DISCERN tool, six (7.5%) websites revealed a high score. Only 20 websites fulfilled all four JAMA benchmark criteria. Of the total 80, only 17 were HON code-certified websites. Readability was variable but the majority was at the college level.",
                "title_content_6": "Conclusions",
                "title_content_7": "The most important result of this study is the low value, readability, and clarity of online testimony regarding proximal humerus fractures."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.7759/cureus.27957",
                "pmid": null,
                "pmc": "PMC9465624",
                "pub_year": "2022",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC9465624/"
            }
        },
        {
            "title": "Analyzing Suicide Risk From Linguistic Features in Social Media: Evaluation Study",
            "abstract": {
                "Background": "Effective suicide risk assessments and interventions are vital for suicide prevention. Although assessing such risks is best done by health care professionals, people experiencing suicidal ideation may not seek help. Hence, machine learning (ML) and computational linguistics can provide analytical tools for understanding and analyzing risks. This, therefore, facilitates suicide intervention and prevention.",
                "Objective": "This study aims to explore, using statistical analyses and ML, whether computerized language analysis could be applied to assess and better understand a person\u2019s suicide risk on social media.",
                "Methods": "We used the University of Maryland Suicidality Dataset comprising text posts written by users (N=866) of mental health\u2013related forums on Reddit. Each user was classified with a suicide risk rating (no, low, moderate, or severe) by either medical experts or crowdsourced annotators, denoting their estimated likelihood of dying by suicide. In language analysis, the Linguistic Inquiry and Word Count lexicon assessed sentiment, thinking styles, and part of speech, whereas readability was explored using the TextStat library. The Mann-Whitney U test identified differences between at-risk (low, moderate, and severe risk) and no-risk users. Meanwhile, the Kruskal-Wallis test and Spearman correlation coefficient were used for granular analysis between risk levels and to identify redundancy, respectively. In the ML experiments, gradient boost, random forest, and support vector machine models were trained using 10-fold cross validation. The area under the receiver operator curve and F1-score were the primary measures. Finally, permutation importance uncovered the features that contributed the most to each model\u2019s decision-making.",
                "Results": "Statistically significant differences (P<.05) were identified between the at-risk (671/866, 77.5%) and no-risk groups (195/866, 22.5%). This was true for both the crowd- and expert-annotated samples. Overall, at-risk users had higher median values for most variables (authenticity, first-person pronouns, and negation), with a notable exception of clout, which indicated that at-risk users were less likely to engage in social posturing. A high positive correlation (\u03c1>0.84) was present between the part of speech variables, which implied redundancy and demonstrated the utility of aggregate features. All ML models performed similarly in their area under the curve (0.66-0.68); however, the random forest and gradient boost models were noticeably better in their F1-score (0.65 and 0.62) than the support vector machine (0.52). The features that contributed the most to the ML models were authenticity, clout, and negative emotions.",
                "Conclusions": "In summary, our statistical analyses found linguistic features associated with suicide risk, such as social posturing (eg, authenticity and clout), first-person singular pronouns, and negation. This increased our understanding of the behavioral and thought patterns of social media users and provided insights into the mechanisms behind ML models. We also demonstrated the applicative potential of ML in assisting health care professionals to assess and manage individuals experiencing suicide risk."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.2196/35563",
                "pmid": "36040781",
                "pmc": "PMC9472054",
                "pub_year": "2022",
                "url": "https://pubmed.ncbi.nlm.nih.gov/36040781/"
            }
        },
        {
            "title": "Phantom Study on the Robustness of MR Radiomics Features: Comparing the Applicability of 3D Printed and Biological Phantoms",
            "abstract": {
                "title_content_0": "The objectives of our study were to (a) evaluate the feasibility of using 3D printed phantoms in magnetic resonance imaging (MR) in assessing the robustness and repeatability of radiomic parameters and (b) to compare the results obtained from the 3D printed phantoms to metrics obtained in biological phantoms. To this end, three different 3D phantoms were printed: a Hilbert cube (5 \u00d7 5 \u00d7 5 cm3) and two cubic quick response (QR) code phantoms (a large phantom (large QR) (5 \u00d7 5 \u00d7 4 cm3) and a small phantom (small QR) (4 \u00d7 4 \u00d7 3 cm3)). All 3D printed and biological phantoms (kiwis, tomatoes, and onions) were scanned thrice on clinical 1.5 T and 3 T MR with 1 mm and 2 mm isotropic resolution. Subsequent analyses included analyses of several radiomics indices (RI), their repeatability and reliability were calculated using the coefficient of variation (CV), the relative percentage difference (RPD), and the interclass coefficient (ICC) parameters. Additionally, the readability of QR codes obtained from the MR images was examined with several mobile phones and algorithms. The best repeatability (CV \u2264 10%) is reported for the acquisition protocols with the highest spatial resolution. In general, the repeatability and reliability of RI were better in data obtained at 1.5 T (CV = 1.9) than at 3 T (CV = 2.11). Furthermore, we report good agreements between results obtained for the 3D phantoms and biological phantoms. Finally, analyses of the read-out rate of the QR code revealed better texture analyses for images with a spatial resolution of 1 mm than 2 mm. In conclusion, 3D printing techniques offer a unique solution to create textures for analyzing the reliability of radiomic data from MR scans."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.3390/diagnostics12092196",
                "pmid": null,
                "pmc": "PMC9497898",
                "pub_year": "2022",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC9497898/"
            }
        },
        {
            "title": "Validation of Three MicroScan\u00ae Antimicrobial Susceptibility Testing Plates Designed for Low-Resource Settings",
            "abstract": {
                "title_content_0": "Easy and robust antimicrobial susceptibility testing (AST) methods are essential in clinical bacteriology laboratories (CBL) in low-resource settings (LRS). We evaluated the Beckman Coulter MicroScan lyophilized broth microdilution panel designed to support M\u00e9decins Sans Fronti\u00e8res (MSF) CBL activity in difficult settings, in particular with the Mini-Lab. We evaluated the custom-designed MSF MicroScan Gram-pos microplate (MICPOS1) for Staphylococcus and Enterococcus species, MSF MicroScan Gram-neg microplate (MICNEG1) for Gram-negative bacilli, and MSF MicroScan Fastidious microplate (MICFAST1) for Streptococci and Haemophilus species using 387 isolates from routine CBLs from LRS against the reference methods. Results showed that, for all selected antibiotics on the three panels, the proportion of the category agreement was above 90% and the proportion of major and very major errors was below 3%, as per ISO standards. The use of the Prompt inoculation system was found to increase the MIC and the major error rate for some antibiotics when testing Staphylococci. The readability of the manufacturer\u2019s user manual was considered challenging for low-skilled staff. The inoculations and readings of the panels were estimated as easy to use. In conclusion, the three MSF MicroScan MIC panels performed well against clinical isolates from LRS and provided a convenient, robust, and standardized AST method for use in CBL in LRS."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.3390/diagnostics12092106",
                "pmid": null,
                "pmc": "PMC9497938",
                "pub_year": "2022",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC9497938/"
            }
        },
        {
            "title": "The Current Quality of Web-Based Information on the Treatment of Bipolar Disorder: A Systematic Search",
            "abstract": {
                "title_content_0": "Background: An important aspect of managing chronic disorders like bipolar disorder is to have access to relevant health information. This study investigates and compares the quality of information on the treatments of bipolar disorder that is available on English websites, as an international language, and on Italian websites, as a popular local language. Methods: A systematic review search was obtained from four search engines. We excluded unrelated materials, scientific papers, and duplicates. We analyzed popularity with PageRank; technological quality with Nibbler; readability with the Flesh Reading Ease test and Gulpease index; quality of information with the DISCERN scale, the JAMA benchmark criteria, and on the extent of adherence to the HONCode. Results: 35 English and 31 Italian websites were included. The English websites were found to have a higher level of quality information and technological quality than the Italian ones. Overall, the websites were found to be difficult to read, requiring a high level of education. Conclusions: These results can be important to inform guidelines for the improvement of health information and help users to reach a higher level of evidence on the websites. Users should find the benefits of treatment, support for shared decision-making, the sources used, the medical editor\u2019s supervision, and the risk of postponing treatment."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.3390/jcm11185427",
                "pmid": null,
                "pmc": "PMC9501527",
                "pub_year": "2022",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC9501527/"
            }
        },
        {
            "title": "Development and Validation of the Minnesota Assessment of Pharmacogenomic Literacy (MAPL)",
            "abstract": {
                "title_content_0": "Ensuring that patients have an adequate understanding of pharmacogenomic (PGx) test results is a critical component of implementing precision medicine into clinical care. However, no PGx-specific validated literacy assessment has yet been developed. To address this need, we developed and validated the Minnesota Assessment of Pharmacogenomic Literacy (MAPLTM). Foundational work included a scoping review of patient and general public attitudes and experiences with pharmacogenomic testing, three focus groups, readability assessments, and review by experts and members of the general public. This resulted in a 15-item assessment designed to assess knowledge in four domains: underlying concepts, limitations, benefits, and privacy. For validation, 646 participants completed the MAPL as a part of a larger survey about pharmacogenomic research and statewide PGx implementation. Two items were deemed to be \u201ctoo easy\u201d and dropped. The remaining 13 items were retained in the final MAPL with good internal reliability (Cronbach\u2019s alpha = 0.75). Confirmatory factor analysis validated the four-domain construct of MAPL and suggested good model performance and high internal validity. The estimated coefficient loadings across 13 questions on the corresponding domains are all positive and statistically significant (p < 0.05). The MAPL covers multiple knowledge domains of specific relevance to PGx and is a useful tool for clinical and research settings where quantitative assessment of PGx literacy is of value."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.3390/jpm12091398",
                "pmid": null,
                "pmc": "PMC9506235",
                "pub_year": "2022",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC9506235/"
            }
        },
        {
            "title": "Three dimensional analysis of hip joint reaction force using Q Hip Force (AQHF) software: Implication as a diagnostic tool",
            "abstract": {
                "title_content_0": "Assessment of hip joint reaction force (JRF) is one of the analytical methods that can enable an understanding of the healthy walking index and the propensity towards disease. In this study, we have designed software, Analysis Q Hip Force (AQHF), to analyze the data retrieved from the mathematical equations for calculating the JRF and ground reaction force (GRF) that act on the hip joint during the early part of the stance phase. The stance phase is considered the least stable sub-phase during walking on level ground, and the gait stability is sequentially minimized during walking on elevated ramps. We have calculated the JRF and GRF values of walking stances on varied inclinations. The data obtained from these calculations during walking on elevated ramps were exported from mathematical equations to Q Hip Force software as two separate values, namely the JRF data and GRF data of the hip joint. The Q Hip Force software stores the two reaction force data in a text file, which allows the import and easy readability of the analyzed data with the AQHF application. The input and output data from the AQHF software were used to investigate the effect of different walking ramps on the magnitude of the hip JRF and GRF. The result of this study demonstrates a significant correlation between the JRF/GRF values and healthy walking indices till a ramp elevation of 70\u00b0. The software is designed to calculate and extrapolate data to analyze the possibility of stress in the hip joint. The framework developed in this study shows promise for preclinical and clinical applications. Studies are underway to use the results of JRF and GRF values as a diagnostic and prognostic tools in different diseases."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1371/journal.pone.0273159",
                "pmid": null,
                "pmc": "PMC9512223",
                "pub_year": "2022",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC9512223/"
            }
        },
        {
            "title": "Comparison of online health information between different digital platforms for pelvic organ prolapse",
            "abstract": {
                "Purpose": "To identify differences in the content and quality of online health information for pelvic organ prolapse (POP) presented in social media and digital search engines to sustainably enhance patient guidance for adequate platforms for seeking online health information on POP.",
                "Methods": "The platforms Google search, Facebook, Instagram, LinkedIn, and YouTube were searched for the keyword \u201cpelvic organ prolapse\u201d. Results were categorized as useful, misleading, advertising, and personal experience. Data were categorized into healthcare professionals, professional organisations, industry, patients, and individuals. The readability score and Health On the Net (HON) code seal were analyzed for Google. Descriptive and univariate analysis was performed.",
                "Results": "The source with the highest quantity of useful content was YouTube whereas LinkedIn included mostly advertisement and misleading content. YouTube and Google provided the greatest variety of health information. Social media platforms identified emotional distress and sleep disturbances as a common side effect of POP which is limited considered in clinical practice and provide novel insights of bothersome symptoms related to the disease. The spectrum of different surgical techniques was limited in all platforms. Only 12 (40.0%) were HON-qualified websites with a mean readability score of 10.4 which is considered fairly difficult to read.",
                "Conclusion": "Besides Google search, YouTube was identified as a valuable online source for POP information. However, encompassing information of surgical techniques was limited in all platforms. Urogynecological association may contribute to improve patient information by providing online health information which is complete and easy to understand.",
                "Supplementary Information": "The online version contains supplementary material available at 10.1007/s00345-022-04129-6."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1007/s00345-022-04129-6",
                "pmid": "36006445",
                "pmc": "PMC9512708",
                "pub_year": "2022",
                "url": "https://pubmed.ncbi.nlm.nih.gov/36006445/"
            }
        },
        {
            "title": "Decision Support Tools for Coronary Artery Calcium Scoring in the Primary Prevention of Cardiovascular Disease Do Not Meet Health Literacy Needs: A Systematic Environmental Scan and Evaluation",
            "abstract": {
                "title_content_0": "A shared decision-making approach is considered optimal in primary cardiovascular disease (CVD) prevention. Evidence-based patient decision aids can facilitate this but do not always meet patients\u2019 health literacy needs. Coronary artery calcium (CAC) scans are increasingly used in addition to traditional cardiovascular risk scores, but the availability of high-quality decision aids to support shared decision-making is unknown. We used an environmental scan methodology to review decision support for CAC scans and assess their suitability for patients with varying health literacy. We systematically searched for freely available web-based decision support tools that included information about CAC scans for primary CVD prevention and were aimed at the public. Eligible materials were independently evaluated using validated tools to assess qualification as a decision aid, understandability, actionability, and readability. We identified 13 eligible materials. Of those, only one qualified as a decision aid, and one item presented quantitative information about the potential harms of CAC scans. None presented quantitative information about both benefits and harms of CAC scans. Mean understandability was 68%, and actionability was 48%. Mean readability (12.8) was much higher than the recommended grade 8 level. Terms used for CAC scans were highly variable. Current materials available to people considering a CAC scan do not meet the criteria to enable informed decision-making, nor do they meet the health literacy needs of the general population. Clinical guidelines, including CAC scans for primary prevention, must be supported by best practice decision aids to support decision-making."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.3390/ijerph191811705",
                "pmid": "36141978",
                "pmc": "PMC9517328",
                "pub_year": "2022",
                "url": "https://pubmed.ncbi.nlm.nih.gov/36141978/"
            }
        },
        {
            "title": "Nursing Teaching Curriculum Setting by Introducing Postcompetency Model under the Vision of Internet Informatization",
            "abstract": {
                "title_content_0": "The nursing curriculum is to be modernized to improve the student's skills in meeting the recent trends in medical and healthcare fields. The curriculum improvements are based on expert recommendations, authors, and informative data from different web sources. The challenging task is to improve the readability and understandability of the curriculum to real-time standards. Considering the above facts, this article introduces a refined curriculum with Internet information analysis (RC-IIA) method. The proposed method incorporates the distributed Internet, journal, and previous curriculum information within the active nursing syllabus. This prevents repetitions and less-informative content within the active curriculum. Besides, classification learning for knowledge-based representations is used within the curriculum to improve competency. Based on the refined information, a recommendation-based curriculum is preferred for varying information across different standards. The proposed analysis method relies on existing and distributed information across multiple curriculum providers for leveraging the visibility and prolonging the stealth of the nursing curriculum."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1155/2022/6164614",
                "pmid": null,
                "pmc": "PMC9534662",
                "pub_year": "2022",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC9534662/"
            }
        },
        {
            "title": "Lexical simplification benchmarks for English, Portuguese, and Spanish",
            "abstract": {
                "title_content_0": "Even in highly-developed countries, as many as 15\u201330% of the population can only understand texts written using a basic vocabulary. Their understanding of everyday texts is limited, which prevents them from taking an active role in society and making informed decisions regarding healthcare, legal representation, or democratic choice. Lexical simplification is a natural language processing task that aims to make text understandable to everyone by replacing complex vocabulary and expressions with simpler ones, while preserving the original meaning. It has attracted considerable attention in the last 20 years, and fully automatic lexical simplification systems have been proposed for various languages. The main obstacle for the progress of the field is the absence of high-quality datasets for building and evaluating lexical simplification systems. In this study, we present a new benchmark dataset for lexical simplification in English, Spanish, and (Brazilian) Portuguese, and provide details about data selection and annotation procedures, to enable compilation of comparable datasets in other languages and domains. As the first multilingual lexical simplification dataset, where instances in all three languages were selected and annotated using comparable procedures, this is the first dataset that offers a direct comparison of lexical simplification systems for three languages. To showcase the usability of the dataset, we adapt two state-of-the-art lexical simplification systems with differing architectures (neural vs. non-neural) to all three languages (English, Spanish, and Brazilian Portuguese) and evaluate their performances on our new dataset. For a fairer comparison, we use several evaluation measures which capture varied aspects of the systems' efficacy, and discuss their strengths and weaknesses. We find that a state-of-the-art neural lexical simplification system outperforms a state-of-the-art non-neural lexical simplification system in all three languages, according to all evaluation measures. More importantly, we find that the state-of-the-art neural lexical simplification systems perform significantly better for English than for Spanish and Portuguese, thus posing a question if such an architecture can be used for successful lexical simplification in other languages, especially the low-resourced ones."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.3389/frai.2022.991242",
                "pmid": null,
                "pmc": "PMC9536312",
                "pub_year": "2022",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC9536312/"
            }
        },
        {
            "title": "Do chiropractic interns use clinical practice guidelines when managing patients with neck pain in France? A feasibility study",
            "abstract": {
                "Background": "In France, we lack knowledge about factors influencing chiropractors\u2019 use of French guideline for managing neck pain and associated disorders (NAD). In particular, we know little about how chiropractic interns use clinical practice guidelines during their training.",
                "Objectives": "We aimed to determine the feasibility of conducting a cross-sectional study of chiropractic interns to determine their utilization and conformity with clinical practice guidelines when managing patients with NAD in France.",
                "Method": "We developed a web-based questionnaire which included 3 sections: (1) clinical vignettes; (2) determinants of practice and (3) socio-demographic and current practice information. The study was conducted in two phases. The first phase included 2 groups: chiropractors and students (interns). Ten chiropractors reviewed and made recommendations on content (especially clinical vignettes), readability, and flow of the survey. Fifteen interns were invited to pretest the proposed recruitment strategy and determine time needed to survey completion, assess completeness of data collection, and evaluate its readability and flow in students. Due to the low participation of students during the first phase, 20 additional interns were invited to participate and pilot the revised recruitment strategy during the second phase. A group of 20 interns were invited to participate and pilot the revised recruitment strategy during the second phase. Qualitative feedbacks about the recruitment strategy, the content of the questionnaire and the survey process were collected by phone to improve all these steps if necessary.",
                "Results": "We collected data from November 2020 to February 2021. In phase 1, 70% of chiropractors (7/10) reviewed the survey and one intern responded (7% participation rate). A revised recruitment strategy was designed and 70% of interns agreed to participate in phase 2. Time to complete the questionnaire was on average 48\u00a0m:22\u00a0s. Interns evaluated survey content as relevant, comprehensive, covering the range of 4 grades of NAD, and adapted to an intern sample. Five main modifications were recommended by (1) Adjusting survey support; (2) Enhancing communication strategy; (3) Considering interns\u2019 comments about the length of the questionnaire; (4) Modifying 2 determinants not adapted to a French context; (5) Adding a proposal when determinants deal with multidisciplinary management.",
                "Conclusion": "Conducting a web-based cross-sectional study of chiropractic interns to assess their utilization and conformity to clinical practice guideline is feasible.",
                "Supplementary Information": "The online version contains supplementary material available at 10.1186/s12998-022-00453-1."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1186/s12998-022-00453-1",
                "pmid": "36209083",
                "pmc": "PMC9548113",
                "pub_year": "2022",
                "url": "https://pubmed.ncbi.nlm.nih.gov/36209083/"
            }
        },
        {
            "title": "Development, Validity, and Reliability of the Perceived Telemedicine Importance, Disadvantages, and Barriers (PTIDB) Questionnaire for Egyptian Healthcare Professionals",
            "abstract": {
                "title_content_0": "Background: This study aimed to develop and investigate the psychometric properties of the Perceived Telemedicine Importance, Disadvantages, and Barriers (PTIDB) questionnaire for healthcare professionals (HCPs) in Egypt. This study was conducted in three phases: (1) development of the questionnaire, (2) preliminary testing of the questionnaire, and (3) investigation of its validity and reliability using a large survey. Methods: A cross-sectional survey was conducted over two months. A convenience sample of 691 HCPs and clerks from 22 governorates accessed the online survey. The construct validity was assessed using exploratory factor analysis (EFA), confirmatory factor analysis (CFA), and internal reliability. Results: The initial Eigenvalues showed that all 19 items of the questionnaire explained 56.0% of the variance in three factors. For Factor 1 (importance), eight items were loaded on one factor, with factor loading ranging from 0.61 to 0.78. For Factor 2 (disadvantages), seven items were loaded on one factor with factor loading ranging from 0.60 to 0.79. For Factor 3 (barriers), four items were loaded on one factor, with factor loading ranging from 0.60 to 0.86. The CFA showed that All loadings ranged from 0.4 to 1.0, with CFI = 0.93 and RMSEA = 0.061. All the factors had satisfactory reliability; 0.87 for \u2018\u2018Importance\u2019\u2019, 0.82 for \u2018\u2018Disadvantages\u2019\u2019, and 0.79 for \u2018\u2018Barriers\u2019\u2019. Conclusion: The PTIDB questionnaire has an acceptable level of validity and internal consistency, at a readability level of 12th grade. The retest reliability, however, still needs to be tested."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.3390/ijerph191912678",
                "pmid": null,
                "pmc": "PMC9566010",
                "pub_year": "2022",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC9566010/"
            }
        },
        {
            "title": "Evaluation of Online Written Medication Educational Resources for People Living With Heart Failure",
            "abstract": {
                "Background": "Patient educational resources on heart failure (HF) medications may improve patient understanding, which is critical\u00a0for informed decision-making and patient self-efficacy. The purpose of our study was to evaluate the quality and readability\u00a0of written medication educational resources available online.",
                "Methods": "Two investigators searched Google, Yahoo, and Bing for written patient educational resources that addressed at least one HF medication. We assessed educational quality using the Ensuring Quality Information for Patients (EQIP) tool (range 0 [worst] to 100 [best]), and we evaluated readability using the Flesch-Kincaid Grade Level.",
                "Results": "From 693 identified webpages, 39 HF medication educational resources met study eligibility. Among included resources, the median Ensuring Quality Information for Patients score was 61% (interquartile range 54%-68%), with 2 (5%) rated\u00a0as high quality (score \u2265 75%). The median Flesch-Kincaid\u00a0Grade\u00a0Level was 8 (interquartile range 8-12), with 4 (10%) resources meeting the recommended 6th-grade reading level.",
                "Conclusions": "Most HF medication educational resources available on the Internet are of acceptable educational quality, but could readily be improved. Most resources were beyond the recommended reading grade level for educational resources, limiting their utility for patients with a low literacy level."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1016/j.cjco.2022.07.004",
                "pmid": null,
                "pmc": "PMC9568683",
                "pub_year": "2022",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC9568683/"
            }
        },
        {
            "title": "Readability assessment of patient educational materials for pediatric spinal deformity from top academic orthopedic institutions",
            "abstract": {
                "Study design": "Cross-sectional analysis of patient educational materials from top pediatric orthopedic hospital websites.",
                "Objective": "To assess the readability of online educational materials of top pediatric orthopedic hospital websites for pediatric spinal deformity.",
                "Summary of background data": "The internet has become an increasingly popular source of health information for patients and their families. Healthcare experts recommend that the readability of online education materials be at or below a 6th-grade reading level. However, previous studies have demonstrated that the readability of online education materials on various orthopedic topics is too advanced for the average patient. To date, the readability of online education materials for pediatric spinal deformity has not been analyzed.",
                "Methods": "Online patient education materials from the top 25 pediatric orthopedic institutions, as ranked by the U.S. News and World Report hospitals for pediatric orthopedics, were accessed utilizing the following readability assessments: Flesch\u2013Kincaid (FK), Flesch Reading Ease, Gunning Fog Index, Coleman\u2013Liau Index, Simple Measure of the Gobbledygook Index (SMOG), Automated Readability Index, FORCAST, and the New Dale and Chall Readability. Correlations between academic institutional ranking, geographic location, and the use of concomitant multi-media modalities with FK scores were evaluated using a Spearman regression.",
                "Results": "Only 48% (12 of 25) of top pediatric orthopedic hospitals provided online information regarding pediatric spinal deformity at or below a 6th-grade reading level. The mean FK score was 9.0\u2009\u00b1\u20092.7, Flesch Reading Ease 50.8\u2009\u00b1\u200915.6, Gunning Fog Score 10.6\u2009\u00b1\u20093.1, Coleman\u2013Liau Index 11.6\u2009\u00b1\u20092.6, SMOG index 11.7\u2009\u00b1\u20092.0, Automated Readability Index 8.6\u2009\u00b1\u20092.8, and Dale\u2013Chall Readability Score 6.4\u2009\u00b1\u20091.4. There was no significant correlation between institutional ranking, geographic location, or use of multimedia with FK scores.",
                "Conclusion": "Online educational material for pediatric spinal deformity from top pediatric orthopedic institutional websites are associated with poor readability."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1007/s43390-022-00545-1",
                "pmid": "35819724",
                "pmc": "PMC9579064",
                "pub_year": "2022",
                "url": "https://pubmed.ncbi.nlm.nih.gov/35819724/"
            }
        },
        {
            "title": "Credibility, Accuracy, and Comprehensiveness of Readily Available Internet-Based Information on Treatment and Management of Peripheral Artery Disease and Intermittent Claudication: Review",
            "abstract": {
                "Background": "Peripheral artery disease (PAD) affects millions of people worldwide, and a core component of management of the condition is self-management. The internet is an important source of health information for many people. However, the content of websites regarding treatment recommendations for PAD has not been fully evaluated.",
                "Objective": "This study aimed to assess the credibility, accuracy, and comprehensiveness of websites found via a common search engine, by comparing the content to current guidelines for treatment and management of PAD and intermittent claudication (IC).",
                "Methods": "A review of websites from hospitals, universities, governments, consumer organizations, and professional associations in the United States and the United Kingdom was conducted. Website recommendations for the treatment of PAD and IC were coded in accordance with the guidelines of the National Institute for Health and Care Excellence (NICE) and the American Heart Association (AHA). Primary outcomes were website credibility (4-item Journal of the American Medical Association benchmark), website accuracy (in terms of the percentage of accurate recommendations), and comprehensiveness of website recommendations (in terms of the percentage of guideline recommendations that were appropriately covered). Secondary outcomes were readability (Flesch\u2013Kincaid grade level) and website quality (Health On the Net Foundation\u2019s code of conduct).",
                "Results": "After screening, 62 websites were included in this analysis. Only 45% (28/62) of websites met the credibility requirement by stating they were updated after the NICE guidelines were published. Declaration of authorship and funding and the presence of reference lists were less commonly reported. Regarding accuracy, 81% (556/685) of website recommendations were deemed accurate on following NICE\u2019s and the AHA\u2019s recommendations. Comprehensiveness was low, with an average of 40% (25/62) of guideline treatment recommendations being appropriately covered by websites. In most cases, readability scores revealed that the websites were too complex for web-based consumer health information.",
                "Conclusions": "Web-based information from reputable sources about the treatment and management of PAD and IC are generally accurate but have low comprehensiveness, credibility, and readability."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.2196/39555",
                "pmid": "36251363",
                "pmc": "PMC9623463",
                "pub_year": "2022",
                "url": "https://pubmed.ncbi.nlm.nih.gov/36251363/"
            }
        },
        {
            "title": "The Quality of Online Information for the Treatment of Knee Osteoarthritis: A Google Study",
            "abstract": {
                "title_content_0": "Introduction",
                "title_content_1": "Affecting more than 30 million adults annually, osteoarthritis (OA) is the most common joint disorder in the United States. A variety of management options for knee OA exists, including\u00a0physical therapy, weight loss, intra-articular corticosteroid injections, and total joint arthroplasty. With many treatments available, patients often utilize the internet to educate themselves about their condition and management options. The purpose of this study was to evaluate the quality, transparency, and readability of online information for the treatment of knee OA.",
                "title_content_2": "Methods",
                "title_content_3": "The search terms \u201cknee,\u201d \u201carthritis,\u201d and \u201ctreatment\u201d were entered into an incognito mode Google browser. Websites were classified by type (commercial, academic, nonacademic medical practice, government/patient advocacy, and other) and graded on content quality, transparency, and readability using the following scores, respectively: modified DISCERN, Journal of American Medical Association (JAMA) Benchmark, and Flesch-Kincaid (FK) grade level.",
                "title_content_4": "Results",
                "title_content_5": "Of the 95 websites evaluated, commercial (mean, 38.2) and academic (37.3) sites had the highest total DISCERN scores, which were significantly greater than nonacademic medical practice (31.8) and government/patient advocacy sites (33.4) (p\u22640.035). Nonacademic medical practice sites had the lowest mean total DISCERN (31.8) and JAMA (1.47) scores and the highest FK grade level readability (9.5). There was a significant positive correlation between mean total DISCERN and JAMA scores (r=0.46, p<0.001).",
                "title_content_6": "Conclusion",
                "title_content_7": "The mean overall quality of websites regarding the treatment of OA was good as evidenced by greater modified DISCERN scores. However, website quality ranged from poor to excellent, indicating that some websites are still missing key information patients may find useful when assessing treatment options online."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.7759/cureus.29995",
                "pmid": "36381839",
                "pmc": "PMC9636897",
                "pub_year": "2022",
                "url": "https://pubmed.ncbi.nlm.nih.gov/36381839/"
            }
        },
        {
            "title": "The Evaluation of the Informational Content, Readability, and Quality of Online Information Related to Vitiligo in the Arabic Language",
            "abstract": {
                "title_content_0": "Background",
                "title_content_1": "Vitiligo is a common skin condition worldwide. It is an autoimmune disorder characterized by losing functional melanocytes, leading to chronic patchy depigmentation. People use the internet to seek health information, which is becoming one of the most commonly utilized sources. In this study, we aim to evaluate online information seen by patients about vitiligo by assessing the quality, content, and readability of widely used medical websites.",
                "title_content_2": "Methodology",
                "title_content_3": "All searches were conducted on February 16, 2022. The most popular search engines, Google, Yahoo, and Bing, were used to find websites, using vitiligo written in Arabic as a search term. An online readability calculator tool was used for the readability assessment of all websites. Two board-certified dermatology consultants (AK\u00a0and LA) formulated a scoring sheet containing 19 questions based on commonly asked questions by patients in the dermatology clinics; 10 out of the 19 questions were designed to cover general information about vitiligo. In contrast, the other nine questions were designed to accommodate the management aspect of vitiligo. For the accountability assessment of each website, Journal of American Medical Association (JAMA) benchmarks were used. Statistical analysis has been performed using Statistical Package for the Social Sciences (SPSS) version 25 (IBM SPSS Statistics, Armonk, NY, USA).",
                "title_content_4": "Main measures",
                "title_content_5": "The following measures were used: a 19-question\u00a0sheet, JAMA benchmarks, the Coleman-Liau index, and the Automated Readability Index (ARI).",
                "title_content_6": "Results",
                "title_content_7": "In this study, we analyzed 21 websites. The interobserver reproducibility was 0.946 between AK and LA\u00a0(P\u2009<\u20090.001). For all websites, the mean (standard deviation (SD)) score of the questionnaire was 11.71 (3.45) (95% confidence interval (CI): 10.14-13.29) out of 19 possible points. Regarding all four JAMA benchmarks, no website achieved all benchmarks. Three of 21 websites (14.29%) completed three out of four JAMA benchmarks. No correlation was found between the content quality of the websites and JAMA benchmarks (r = 0.270, P = 0.237).",
                "title_content_8": "Conclusion",
                "title_content_9": "Online information about vitiligo in Arabic varies depending on the source, but overall, it is of low quality and written beyond the level of the general population. The \u201ctop 10 websites\u201d\u00a0outlined in our article may be used as a suggested reading list for vitiligo patients."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.7759/cureus.30497",
                "pmid": null,
                "pmc": "PMC9674206",
                "pub_year": "2022",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC9674206/"
            }
        },
        {
            "title": "The Risks Associated With Computed Tomography Scans: An Assessment of the Readability and Reliability of Online Text Available for Patient Information and Guidance",
            "abstract": {
                "title_content_0": "Introduction",
                "title_content_1": "Computed tomography (CT) scanning\u00a0has become a fundamental aspect of many diagnostic pathways and therapeutic procedures. However, it is not without risk. Many patients are unaware of the exposure to ionising radiation that is involved with undergoing a CT scan, as well as the associated increase in cancer risk with cumulative exposure. Controversy over which clinician is responsible for advising a patient regarding CT risk often means that patients are left uninformed and unaware. Those who choose to seek further guidance online do so with the risk of encountering poor quality, difficult-to-read medical text, which may leave them even more confused or misinformed.",
                "title_content_2": "Aim",
                "title_content_3": "This study aimed to assess the readability, quality, and accuracy of the information available to patients online regarding CT scans and their associated risks.",
                "title_content_4": "Method",
                "title_content_5": "An internet search of 180 websites was collated using three search terms, each entered into three search engines. The terms used were 'CT Risk', 'CT Harm', and 'Dangers of a CT Scan'. Unique websites generated were assessed for readability using four readability formulae: The Flesh-Kincaid Grade Level, the Flesch Reading Ease Score, the Gunning Fog Index, and the Simple Measure of Gobbledygook (SMOG) Index. The text on each website was also evaluated for quality and accuracy using the Discern tool. Mean readability scores were calculated and compared against the defined standard required for the information intended for the general public. A two-tailed t-test was also carried out to assess statistical significance.",
                "title_content_6": "Results",
                "title_content_7": "Of the 180 websites collated, 77 were unique.\u00a076.62% of websites (59/77) met the readability target for the Flesch-Kincaid Grade Level, and 66.23% (51/77) met the target for the Gunning Fog Index, which was for text to be readable at the Grade 8 level (or by children aged 13-14). 59.74% (46/77) met the target for the SMOG Index, which for healthcare information, was defined as Grade 6 Level, or children aged 11-12. Only 11.69% of websites (9/77) met the target for the Flesch Reading Ease score. 55.84%\u00a0of websites satisfied the pre-defined standard for three out of four readability criteria, however, only 11.69% satisfied all four criteria, limited by the scores obtained by the Flesch Reading Ease formula. The websites generated a mean Discern score of 3.58, meaning the average quality of the information was deemed to be 'fair', with no serious shortcomings.",
                "title_content_8": "Conclusion",
                "title_content_9": "More than 50% of websites satisfied three readability criteria simultaneously. However, there is still scope for improvement, both in terms of enhancing the readability of the remaining websites, and also ensuring that all websites review the proportion of polysyllabic words in the text, which is the emphasis of the Flesh Reading Ease Score.\u00a0In addition, physicians and radiologists\u00a0have a responsibility to inform patients of the risks associated with CT scans, and to direct them to supplementary good-quality information and resources."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.7759/cureus.30758",
                "pmid": null,
                "pmc": "PMC9700557",
                "pub_year": "2022",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC9700557/"
            }
        },
        {
            "title": "Medical Text Simplification Using Reinforcement Learning (TESLEA): Deep Learning\u2013Based Text Simplification Approach",
            "abstract": {
                "Background": "In most cases, the abstracts of articles in the medical domain are publicly available. Although these are accessible by everyone, they are hard to comprehend for a wider audience due to the complex medical vocabulary. Thus, simplifying these complex abstracts is essential to make medical research accessible to the general public.",
                "Objective": "This study aims to develop a deep learning\u2013based text simplification (TS) approach that converts complex medical text into a simpler version while maintaining the quality of the generated text.",
                "Methods": "A TS approach using reinforcement learning and transformer\u2013based language models was developed. Relevance reward, Flesch-Kincaid reward, and lexical simplicity reward were optimized to help simplify jargon-dense complex medical paragraphs to their simpler versions while retaining the quality of the text. The model was trained using 3568 complex-simple medical paragraphs and evaluated on 480 paragraphs via the help of automated metrics and human annotation.",
                "Results": "The proposed method outperformed previous baselines on Flesch-Kincaid scores (11.84) and achieved comparable performance with other baselines when measured using ROUGE-1 (0.39), ROUGE-2 (0.11), and SARI scores (0.40). Manual evaluation showed that percentage agreement between human annotators was more than 70% when factors such as fluency, coherence, and adequacy were considered.",
                "Conclusions": "A unique medical TS approach is successfully developed that leverages reinforcement learning and accurately simplifies complex medical paragraphs, thereby increasing their readability. The proposed TS approach can be applied to automatically generate simplified text for complex medical text data, which would enhance the accessibility of biomedical research to a wider audience."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.2196/38095",
                "pmid": "36399375",
                "pmc": "PMC9719064",
                "pub_year": "2022",
                "url": "https://pubmed.ncbi.nlm.nih.gov/36399375/"
            }
        },
        {
            "title": "Item format statistics and readability of extended matching questions as an effective tool to assess medical students",
            "abstract": {
                "title_content_0": "Testing based on multiple choice questions (MCQ) is one of the most established forms of assessment, not only in the medical field. Extended matching questions (EMQ) represent a specific type of MCQ designed to require higher levels of cognition, such as problem-solving. The purpose of this evaluation was to assess the suitability and efficiency of EMQ as an assessment method. EMQ were incorporated into the end-of-semester examination in internal medicine, in which 154 students participated, and compared with three established MCQ types. Item and examination quality were investigated, as well as readability and processing time. EMQ were slightly more difficult to score; however, both item discrimination and discrimination index were higher when compared to other item types. EMQ were found to be significantly longer and required more processing time, but readability was improved. Students judged EMQ as clearly challenging, but attributed significantly higher clinical relevance when compared to established MCQ formats. Using the Spearman-Brown prediction, only ten EMQ items would be needed to reproduce the Cronbach\u2019s alpha value of 0.75 attained for the overall examination. EMQ proved to be both efficient and suitable when assessing medical students, demonstrating powerful characteristics of reliability. Their expanded use in favor of common MCQ could save examination time without losing out on statistical quality."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1038/s41598-022-25481-y",
                "pmid": "36470965",
                "pmc": "PMC9723123",
                "pub_year": "2022",
                "url": "https://pubmed.ncbi.nlm.nih.gov/36470965/"
            }
        },
        {
            "title": "Visualizing knowledge evolution trends and research hotspots of artificial intelligence in colorectal cancer: A bibliometric analysis",
            "abstract": {
                "Background": "In recent years, the rapid development of artificial intelligence (AI) technology has created a new diagnostic and therapeutic opportunity for colorectal cancer (CRC). Numerous academic and clinical studies have demonstrated that high-level auxiliary diagnosis and treatment systems based on AI technology can significantly improve the readability of medical data, objectively provide a reliable and comprehensive reference for physicians, reduce the experience gap between physicians, and aid physicians in making more accurate diagnosis decisions. In this study, we used bibliometric techniques to visually analyze the literature about AI in the CRC field and summarize the current situation and research hotspots in this field.",
                "Methods": "The relevant literature on AI in the field of CRC research was obtained from the Web of Science Core Collection (WoSCC) database. The software CiteSpace was utilized to analyze the number of papers, countries, institutions, authors, journals, cited literature, and keywords of the included literature and generate a visual knowledge map. The present study aims to evaluate the origin, current hotspots, and research trends of AI in CRC using bibliometric analysis.",
                "Results": "As of March 2022, 64 nations/regions, 230 institutions, 245 journals, and 300 authors had published 562 AI-related articles in the field of CRC. Since 2016, each year has seen an exponential increase. China and the United States were the largest contributors, with the largest number of beneficial research institutions and the closest collaboration relationship. The World Journal of Gastroenterology is this field\u2019s most widely published journal. Diagnosis and treatment research, gene and immunology research, intestinal polyp research, tumor grading research, gastrointestinal endoscopy research, and prognosis research comprised the six topics derived from high-frequency keyword cluster analysis.",
                "Conclusion": "In recent years, field research has been a popular topic of discussion. The results of our bibliometric analysis allow us to comprehend better the current situation and trend of this research field, and the quantitative data indicators can serve as a guide for the research and application of global scholars."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.3389/fonc.2022.925924",
                "pmid": null,
                "pmc": "PMC9742812",
                "pub_year": "2022",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC9742812/"
            }
        },
        {
            "title": "The Impact of Popular Science Articles by Physicians on Their Performance on Online Medical Platforms",
            "abstract": {
                "title_content_0": "The public demand for popular science knowledge regarding health is increasing, and physicians\u2019 popular science practices on online medical platforms are becoming frequent. Few studies have been conducted to address the relationship between specific characteristics of popular science articles by physicians and their performance. This study explored the impact of the characteristics of popular science articles on physicians\u2019 performance based on the elaboration likelihood model (ELM) from the central path (topic focus and readability) and the peripheral path (form diversity). Data on four diseases, namely, lung cancer, brain hemorrhage, hypertension, and depression, were collected from an online medical platform, resulting in relevant personal data from 1295 doctors and their published popular science articles. Subsequently, the independent variables were quantified using thematic analysis and formula calculation, and the research model and hypotheses proposed in this paper were verified through empirical analysis. The results revealed that the topic focus, readability, and form diversity of popular science articles by physicians had a significant positive effect on physicians\u2019 performance. This study enriches the research perspective on the factors influencing physicians\u2019 performance, which has guiding implications for both physicians and platforms, thereby providing a basis for patients to choose physicians and enabling patients to receive popular science knowledge regarding health in an effective manner."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.3390/healthcare10122432",
                "pmid": null,
                "pmc": "PMC9777991",
                "pub_year": "2022",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC9777991/"
            }
        },
        {
            "title": "Availability of Readable Online Spanish Rhinosinusitis Outcome Measures",
            "abstract": {
                "title_content_0": "Background: Patient-reported outcome measures (PROMs) are useful instruments that give providers insight into patients\u2019 experiences with disease by quantifying the symptoms that matter most to patients. Results of these questionnaires can help guide management in chronic rhinosinusitis. However, these tools are often developed for native English speakers, which disadvantages others, who already have a language barrier to care. The aim of this study is to evaluate accessibility and readability of Spanish PROMs used to evaluate rhinosinusitis. Methods: Three Spanish readability measures, Gilliam, Pe\u00f1a & Mountain; SOL; and Fernandez-Huerta were used to evaluate PROMs utilized for rhinosinusitis. PROMs with sixth-grade readability level or easier were considered to meet health literacy recommendations. Results: Four Spanish PROMs utilized in assessment of rhinosinusitis were identified and evaluated. Cuestionario Espa\u00f1ol de Calidad de Vida en Rinitis (ESPRINT-15) was the most readable PROM and met readability recommendations in two of three measures. Nasal Obstruction Symptom Evaluation met suggested levels in one measure. The remainder of readability scores were more difficult than recommended. Conclusion: PROMs are powerful clinical tools that help patients communicate their symptoms and self-advocate. For providers to gain accurate and useful information, these measures should be written at appropriate readability levels. Most Spanish PROMs used for assessment of rhinosinusitis were above recommended readability. Development of future PROMs should ensure appropriate readability levels to provide good patient-centered care for our primarily Spanish speaking patients."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.3390/jcm11247364",
                "pmid": "36555979",
                "pmc": "PMC9782990",
                "pub_year": "2022",
                "url": "https://pubmed.ncbi.nlm.nih.gov/36555979/"
            }
        },
        {
            "title": "Co-design and Development of EndoSMS, a Supportive Text Message Intervention for Individuals Living With Endometriosis: Mixed Methods Study",
            "abstract": {
                "Background": "Endometriosis, which affects 1 in 10 people assigned female at birth, is a chronic systemic inflammatory disease with a high symptom burden and adverse socioemotional impacts. There is a need for an accessible, cost-effective, and low-burden intervention to support individuals in managing their endometriosis condition.",
                "Objective": "This study aimed to co-design and evaluate the acceptability, readability, and quality of a bank of supportive SMS text messages (EndoSMS) for individuals with endometriosis.",
                "Methods": "In phase 1 of this mixed method design, 17 consumer representatives (individuals with endometriosis) participated across three 3-hour web-based (Zoom, Zoom Video Communications, Inc) focus groups. The transcripts were encoded and analyzed thematically. In phase 2, consumer representatives (n=14) and health care professionals (n=9) quantitatively rated the acceptability, readability, and appropriateness of the developed text messages in a web-based survey. All the participants initially completed a background survey assessing sociodemographic and medical factors.",
                "Results": "Consumer representatives demonstrated diverse sociodemographic characteristics (Mage=33.29), varying in location (metropolitan vs rural or regional), employment, and relationship and educational statuses. Participants reached a consensus regarding the delivery of 4 SMS text messages per week, delivered randomly throughout the week and in one direction (ie, no reply), with customization for the time of day and use of personal names. Seven main areas of unmet need for which participants required assistance were identified, which subsequently became the topic areas for the developed SMS text messages: emotional health, social support, looking after and caring for your body, patient empowerment, interpersonal issues, general endometriosis information, and physical health. Through a web-based survey, 371 co-designed SMS text messages were highly rated by consumers and health care professionals as clear, useful, and appropriate for individuals with endometriosis. Readability indices (Flesch-Kincaid scale) indicated that the SMS text messages were accessible to individuals with a minimum of 7th grade high school education.",
                "Conclusions": "On the basis of the needs and preferences of a diverse consumer representative group, we co-designed EndoSMS, a supportive SMS text message program for individuals with endometriosis. The initial evaluation of the SMS text messages by consumer representatives and health professionals suggested the high acceptability and suitability of the developed SMS text messages. Future studies should further evaluate the acceptability and effectiveness of EndoSMS in a broader population of individuals with endometriosis."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.2196/40837",
                "pmid": "36485029",
                "pmc": "PMC9789499",
                "pub_year": "2022",
                "url": "https://pubmed.ncbi.nlm.nih.gov/36485029/"
            }
        },
        {
            "title": "Improving the quality of medical records",
            "abstract": {
                "title": "Abstract",
                "Background": "Medical record is an essential tool both in patients\u2019 diagnostic and therapeutic pathways and communication between different care providers. It also has an economic-administrative, medical-legal and epidemiological function. From an economic-administrative point of view, a medical record allows an evaluation and review of services to better manage the corporate health budget. In addition, it allows traceability and complete transparency of the health activities carried out. The study evaluates the formal quality of medical records compiled in an Italian private clinic before and after a training intervention.",
                "Methods": "In June 2019, a retrospective study was carried out to assess a private clinic\u2019s quality of medical records. One month later, healthcare providers were trained on the appropriate compilation of medical records, whose pre-printed format was structurally improved. In March 2020, we verified the quality of medical records produced after that training intervention. Statistical analysis (Wilcoxon test) was carried out through Stata.",
                "Results": "A total of 149 medical records were reviewed. Statistically significant improvements (p < 0,05), between before and after training intervention, were for overall readability (33.3% vs 74.7%), completeness of admission and discharge dates (33.3% vs 74.40%), for completeness of anamnesis (13.6% vs 63.9%), for completeness of therapeutic card (53% vs 85.5%), in the reduction of non-compliance corrections (22.7% vs 4.8%), signature presence of physical examination (34.9% vs 71.1%) and for signature presence in the hospital discharge card (86.4% vs 96.4%).",
                "Conclusions": "The results show that simple measures, such as staff training and restructuring of the format, are effective in improving the quality of medical records.",
                "Key messages": "\u2022\u2002Healthcare providers should perceive the proper completion of medical records as a common goal.\u2022\u2002Well-completed medical records contribute to better health care."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1093/eurpub/ckac131.372",
                "pmid": null,
                "pmc": "PMC9831581",
                "pub_year": "2022",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC9831581/"
            }
        },
        {
            "title": "Development of Educational Print Materials for Physical Activity in Cancer: Evaluation of Readability and Suitability",
            "abstract": {
                "title_content_0": "Educational health materials may be important tools to increase physical activity in cancer patients. Nevertheless, most of the available resources regarding physical activity for cancer patients were found not suitable, had a low grade of readability, and thus, represent a significant barrier to behavior change. To date, little data about development criteria and evaluation of physical activity resources for cancer before their spread exist. The purposes of this study were (i) to describe the development of a physical activity guidebook designed for cancer patients and (ii) to test its readability and suitability. The guidebook was developed through multi-step passages, including group discussions, a literature review, identification of a motivational theory, and using previous research on exercise preferences, barriers, and facilitators to target the information. Two validated formulae were used to assess the readability, whereas thirty-four judges completed the Suitability of Assessment Materials questionnaire to evaluate the suitability of the guidebook. The guidebook was found readable for patients having at least a primary education, and the judges scored it as \u201csuperior\u201d material. Our guidebook, following a rigorous method in the development phase, was considered to be suitable and readable. Further evaluations through clinical trials could investigate its effectiveness for behavior change and its impact on cancer patients."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1007/s13187-021-02076-1",
                "pmid": "34523076",
                "pmc": "PMC9852108",
                "pub_year": "2021",
                "url": "https://pubmed.ncbi.nlm.nih.gov/34523076/"
            }
        },
        {
            "title": "Growing Taller without Hormones? Dr. Consult Google\u2014An Evaluation of Online Information Related to Limb Lengthening",
            "abstract": {
                "title_content_0": "Purpose: The aim of this study was to investigate the reliability, content and readability of the information available on the Internet related to limb lengthening surgeries, which have recently been progressively in fashion. Methods: The three most commonly used browsers on the Internet were determined and a search term for \u201cLimb Lengthening Surgery\u201d was typed for each browser. The websites were categorized by their type, and the content and the quality of them was evaluated using the DISCERN score, the Journal of American Medical Association (JAMA) benchmark and the Global Quality Score (GQS). The Flesch Kincaid Grade Level (FKGL) and the Flesch Reading Ease Score (FKRS) were used to evaluate the readability. Each website also assessed the presence (or absence) of the Health on Net (HON) code. Results: The academic category was found to be significantly higher than the medical and commercial categories. Mean FKGL and FCRS scores, DISCERN score values, JAMA, GQS and LLCS score values of Websites with HON code were significantly higher than those without. Conclusions: The quality of online information related to limb lengthening was of low quality. Although some websites, especially academic resources, were of higher quality, the readability of their content is just about 2.5 degrees higher than the sixth-grade reading level."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.3390/healthcare11020172",
                "pmid": null,
                "pmc": "PMC9858970",
                "pub_year": "2023",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC9858970/"
            }
        },
        {
            "title": "Contactless, autonomous robotic alignment of optical coherence tomography for in vivo evaluation of diseased retinas",
            "abstract": {
                "title_content_0": "During the COVID-19 pandemic, an emphasis was placed on contactless, physical distancing and improved telehealth; contrariwise, standard-of-care ophthalmic imaging of patients required present, trained personnel. Here, we introduce contactless, autonomous robotic alignment of optical coherence tomography (RAOCT) for in vivo imaging of retinal disease and compare measured retinal thickness and diagnostic readability to technician operated clinical OCT. In a powered study, we found no statistically significant difference in retinal thickness in both healthy and diseased retinas (p > 0.7) or across a variety of demographics (gender, race, and age) between RAOCT and clinical OCT. In a secondary study, a retina specialist labeled a given volume as normal/abnormal. Compared to the clinical diagnostic label, sensitivity/specificity for RAOCT were equal or improved over clinical OCT. Contactless, autonomous RAOCT, that improves upon current clinical OCT, could play a role in both ophthalmic care and non-ophthalmic settings that would benefit from improved eye care."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.21203/rs.3.rs-2371365/v1",
                "pmid": "36711930",
                "pmc": "PMC9882601",
                "pub_year": "2023",
                "url": "https://pubmed.ncbi.nlm.nih.gov/36711930/"
            }
        },
        {
            "title": "Notebook of experiences: a therapeutic resource in educational audiology",
            "abstract": {
                "title": "ABSTRACT",
                "Purpose": "To translate the My Experience Book toll into Portuguese, evaluate the translation content, readability, quality, and visual identity of the material, and make it available online.",
                "Methods": "Descriptive, cross-sectional, quantitative, and qualitative study. The procedures consisted of five stages: translation of the material; evaluation and response to questionnaire one regarding translation; determining the validity of the content; readability assessment; availability of material online and evaluation of content aimed at quality and visual identity (questionnaire two). Twenty-five professionals (audiologists and physicians) participated in the study.",
                "Results": "The translation of the notebook showed validity agreement rates greater than 90%. Readability rated the material easy to read. Among the 184 invited professionals, only 25 agreed to participate, demonstrating low adherence to the study. Most respondents agreed positively about the content and consistency of the translated material, videos, illustrative images, and captions. The material was also considered necessary by the majority of the evaluators. All audiologists reported using the instrument in clinical practice.",
                "Conclusion": "The translated toll is of great relevance. It gathers practical information to create a notebook of experiences and guidance on using the material as an additional resource to stimulate the auditory skills of children with hearing impairment."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1590/2317-1782/20212020422",
                "pmid": "35019064",
                "pmc": "PMC9886119",
                "pub_year": "2022",
                "url": "https://pubmed.ncbi.nlm.nih.gov/35019064/"
            }
        },
        {
            "title": "Readability and topics of the German Health Web: Exploratory study and text analysis",
            "abstract": {
                "Background": "The internet has become an increasingly important resource for health information, especially for lay people. However, the information found does not necessarily comply with the user\u2019s health literacy level. Therefore, it is vital to (1) identify prominent information providers, (2) quantify the readability of written health information, and (3) to analyze how different types of information sources are suited for people with differing health literacy levels.",
                "Objective": "In previous work, we showed the use of a focused crawler to \u201ccapture\u201d and describe a large sample of the \u201cGerman Health Web\u201d, which we call the \u201cSampled German Health Web\u201d (sGHW). It includes health-related web content of the three mostly German speaking countries Germany, Austria, and Switzerland, i.e. country-code top-level domains (ccTLDs) \u201c.de\u201d, \u201c.at\u201d and \u201c.ch\u201d. Based on the crawled data, we now provide a fully automated readability and vocabulary analysis of a subsample of the sGHW, an analysis of the sGHW\u2019s graph structure covering its size, its content providers and a ratio of public to private stakeholders. In addition, we apply Latent Dirichlet Allocation (LDA) to identify topics and themes within the sGHW.",
                "Methods": "Important web sites were identified by applying PageRank on the sGHW\u2019s graph representation. LDA was used to discover topics within the top-ranked web sites. Next, a computer-based readability and vocabulary analysis was performed on each health-related web page. Flesch Reading Ease (FRE) and the 4th Vienna formula (WSTF) were used to assess the readability. Vocabulary was assessed by a specifically trained Support Vector Machine classifier.",
                "Results": "In total, n = 14,193,743 health-related web pages were collected during the study period of 370 days. The resulting host-aggregated web graph comprises 231,733 nodes connected via 429,530 edges (network diameter = 25; average path length = 6.804; average degree = 1.854; modularity = 0.723). Among 3000 top-ranked pages (1000 per ccTLD according to PageRank), 18.50%(555/3000) belong to web sites from governmental or public institutions, 18.03% (541/3000) from nonprofit organizations, 54.03% (1621/3000) from private organizations, 4.07% (122/3000) from news agencies, 3.87% (116/3000) from pharmaceutical companies, 0.90% (27/3000) from private bloggers, and 0.60% (18/3000) are from others. LDA identified 50 topics, which we grouped into 11 themes: \u201cResearch & Science\u201d, \u201cIllness & Injury\u201d, \u201cThe State\u201d, \u201cHealthcare structures\u201d, \u201cDiet & Food\u201d, \u201cMedical Specialities\u201d, \u201cEconomy\u201d, \u201cFood production\u201d, \u201cHealth communication\u201d, \u201cFamily\u201d and \u201cOther\u201d. The most prevalent themes were \u201cResearch & Science\u201d and \u201cIllness & Injury\u201d accounting for 21.04% and 17.92% of all topics across all ccTLDs and provider types, respectively. Our readability analysis reveals that the majority of the collected web sites is structurally difficult or very difficult to read: 84.63% (2539/3000) scored a WSTF \u2265 12, 89.70% (2691/3000) scored a FRE \u2264 49. Moreover, our vocabulary analysis shows that 44.00% (1320/3000) web sites use vocabulary that is well suited for a lay audience.",
                "Conclusions": "We were able to identify major information hubs as well as topics and themes within the sGHW. Results indicate that the readability within the sGHW is low. As a consequence, patients may face barriers, even though the vocabulary used seems appropriate from a medical perspective. In future work, the authors intend to extend their analyses to identify trustworthy health information web sites."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1371/journal.pone.0281582",
                "pmid": "36763573",
                "pmc": "PMC9916670",
                "pub_year": "2023",
                "url": "https://pubmed.ncbi.nlm.nih.gov/36763573/"
            }
        },
        {
            "title": "Metal Artifact Reduction in Dental CBCT Images Using Direct Sinogram Correction Combined with Metal Path-Length Weighting",
            "abstract": {
                "title_content_0": "Metal artifacts in dental computed tomography (CT) images, caused by highly X-ray absorbing objects, such as dental implants or crowns, often more severely compromise image readability than in medical CT images. Since lower tube voltages are used for dental CTs in spite of the more frequent presence of metallic objects in the patient, metal artifacts appear more severely in dental CT images, and the artifacts often persist even after metal artifact correction. The direct sinogram correction (DSC) method, which directly corrects the sinogram using the mapping function derived by minimizing the sinogram inconsistency, works well in the case of mild metal artifacts, but it often fails to correct severe metal artifacts. We propose a modified DSC method to reduce severe metal artifacts, and we have tested it on human dental images. We first segment the metallic objects in the CT image, and then we forward-project the segmented metal mask to identify the metal traces in the projection data with computing the metal path length for the rays penetrating the metal mask. In the sinogram correction with the DSC mapping function, we apply the weighting proportional to the metal path length. We have applied the proposed method to the phantom and patient images taken at the X-ray tube voltage of 90 kVp. We observed that the proposed method outperforms the original DSC method when metal artifacts were severe. However, we need further extensive studies to verify the proposed method for various CT scan conditions with many more patient images."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.3390/s23031288",
                "pmid": "36772330",
                "pmc": "PMC9919069",
                "pub_year": "2023",
                "url": "https://pubmed.ncbi.nlm.nih.gov/36772330/"
            }
        },
        {
            "title": "Websites about, not for, adolescents? A systematic analysis of online fertility preservation information for adolescent and young adult cancer patients",
            "abstract": {
                "Purpose": "Fertility preservation is an increasingly important topic in adolescent and young adult cancer survivorship, yet treatments remain under-utilized, possibly due to lack of awareness and understanding. The internet is widely used by adolescents and young adults and has been proposed to fill knowledge gaps and advance high-quality, more equitable care. As a first step, this study analyzed the quality of current fertility preservation resources online and identified opportunities for improvement.",
                "Methods": "We conducted a systematic analysis of 500 websites to assess the quality, readability, and desirability of website features, and the inclusion of clinically relevant topics.",
                "Results": "The majority of the 68 eligible websites were low quality, written at college reading levels, and included few features that younger patients find desirable. Websites mentioned more common fertility preservation treatments than promising experimental treatments, and could be improved with cost information, socioemotional impacts, and other equity-related fertility topics.",
                "Conclusions": "Currently, the majority of fertility preservation websites are about, but not for, adolescent and young adult patients. High-quality educational websites are needed that address outcomes that matter to teens and young adults, with a priority on solutions that prioritize equity.",
                "Implications for Cancer Survivors:": "Adolescent and young adult survivors have limited access to high-quality fertility preservation websites that are designed for their needs. There is a need for the development of fertility preservation websites that are clinically comprehensive, written at appropriate reading levels, inclusive, and desirable. We include specific recommendations that future researchers can use to develop websites that could better address AYA populations and improve the fertility preservation decision making process."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.21203/rs.3.rs-2587513/v1",
                "pmid": "36824765",
                "pmc": "PMC9949230",
                "pub_year": "2023",
                "url": "https://pubmed.ncbi.nlm.nih.gov/36824765/"
            }
        },
        {
            "title": "The Complexity of Online Patient Education Materials About Interventional Neuroradiology Procedures Published by Major Academic Institutions",
            "abstract": {
                "title_content_0": "Introduction",
                "title_content_1": "Health literacy is an independent predictor of population health status and is directly related to the readability of available patient education material. The National Institutes of Health (NIH) and the American Medical Association have recommended that patient education materials (PEMs) be written between a fourth- and a sixth-grade education level. The authors assessed the readability of online PEMs about neurointerventional procedures that have been published by several academic institutions across the US.",
                "title_content_2": "Methods",
                "title_content_3": "Online PEMs regarding five common neurointerventional procedures, including mechanical thrombectomy for large vessel occlusion, cerebral diagnostic angiography, carotid artery stenting, endovascular aneurysm embolization, and epidural steroid injection collected from the websites of 20 top institutions in Neurology and Neurosurgery. The materials were assessed via five readability scales and then were statistically analyzed and compared to non-institutional education websites (Wikipedia.com and WebMD.com).",
                "title_content_4": "Results",
                "title_content_5": "None of the PEMs were written at or below the NIH's recommended 6th-grade reading level. The average educational level required to comprehend the texts across all institutions, as assessed by the readability scales, was 10-11th grade level. Some materials required a college-level education or higher. Material from non-institutional websites had significantly lower readability scores compared to the 20 institutions.",
                "title_content_6": "Conclusions",
                "title_content_7": "Current PEMs related to neurointerventional procedures are not written at or below the NIH's recommended fourth- to sixth-grade education level. Given the complexity of those procedures, significant attention should be pointed toward an improvement in the available online materials."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.7759/cureus.34233",
                "pmid": null,
                "pmc": "PMC9962712",
                "pub_year": "2023",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC9962712/"
            }
        },
        {
            "title": "Online Health Information for Penile Prosthesis Implants Lacks Quality and Is Unreadable to the Average US Patient",
            "abstract": {
                "title_content_0": "Background: Online health information (OHI) has become widely accessible and affects patient decisions regarding their healthcare. The purpose of this study was to assess the readability, quality, and accuracy of information available to patients online about penile prosthesis implants (PPIs).",
                "title_content_1": "Methods: We performed a Google search using the keywords \u201cpenile implant\u201d and \u201cpenile prosthesis.\u201d The first 30 search results for both terms were analyzed, and advertisements, news articles, duplicates, and videos were excluded. Websites were categorized as institutional, commercial, and personal/patient support. Readability of each website was determined using the Flesch-Kincaid grade level (FKGL) readability formula within the readable tool. Quality was measured by Health On the Net (HON) certification status and the DISCERN scoring method. For website accuracy, a score of 1-4 (1=0-25%, 2=25-50%, 3=50-75%, and 4=75-100%) was assigned.",
                "title_content_2": "Results: Forty-four websites met the criteria (23 institutional, 12 commercial, and 9 personal/patient support). The mean total FKGL score was 9.55. No statistical difference was detected between mean FKGL for each website category (p=0.69). Only eight websites (18%) scored \u22648th-grade reading level (average US adult level), while 36 (82%) were >8th-grade\u00a0level. Mean total DISCERN sum score was 39.74/75, with no statistical difference in mean DISCERN score between website types (p=0.08). Over half (55%) of the websites were defined as \u201cvery poor\u201d or \u201cpoor\u201d quality by DISCERN scoring. Mean total overall quality rating was 2.67/5. HON certification was verified for only nine websites (20%). Twenty-five percent of websites were classified as 0-25% accurate, 23% were 25-50% accurate, 30% were 50-75% accurate, and 23% were 75-100% accurate.",
                "title_content_3": "Conclusion: Most information on the Internet about PPIs is reasonably accurate; however, the majority of websites are deficient in quality and unreadable to the average patient, irrespective of website type."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.7759/cureus.34240",
                "pmid": null,
                "pmc": "PMC9964715",
                "pub_year": "2023",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC9964715/"
            }
        },
        {
            "title": "Analysis of the Readability and Accountability of Online Patient Education Materials Related to Glaucoma Diagnosis and Treatment",
            "abstract": {
                "Purpose": "To assess the readability and accountability of online patient education materials related to glaucoma diagnosis and treatment.",
                "Methods": "We conducted a Google search for 10 search terms related to glaucoma diagnosis and 10 search terms related to glaucoma treatment. For each search term, the first 10 patient education websites populated after Google search were assessed for readability and accountability. Readability was assessed using five validated measures: Flesch Reading Ease (FRE), Gunning Fog Index (GFI), Flesch-Kincaid Grade Level (FKGL), Simple Measure of Gobbledygook (SMOG), and New Dale-Chall (NDC). Accountability was assessed using the Journal of the American Medical Association (JAMA) benchmarks. The source of information for each article analyzed was recorded.",
                "Results": "Of the 200 total websites analyzed, only 11% were written at or below the recommended 6th grade reading level. The average FRE and grade level for 100 glaucoma diagnosis-related articles were 42.02 \u00b1 1.08 and 10.53 \u00b1 1.30, respectively. The average FRE and grade level for 100 glaucoma treatment-related articles were 43.86 \u00b1 1.01 and 11.29 \u00b1 1.54, respectively. Crowdsourced articles were written at the highest average grade level (12.32 \u00b1 0.78), followed by articles written by private practice/independent users (11.22 \u00b1 1.74), national organizations (10.92 \u00b1 1.24), and educational institutions (10.33 \u00b1 1.35). Websites averaged 1.12 \u00b1 1.15 of 4 JAMA accountability metrics.",
                "Conclusion": "Despite wide variation in the readability and accountability of online patient education materials related to glaucoma diagnosis and treatment, patient education materials are consistently written at levels above the recommended reading level and often lack accountability. Articles from educational institutions and national organizations were often written at lower reading levels but are less frequently encountered after Google search. There is a need for accurate and understandable online information that glaucoma patients can use to inform decisions about their eye health."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.2147/OPTH.S401492",
                "pmid": null,
                "pmc": "PMC10008728",
                "pub_year": "2023",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC10008728/"
            }
        },
        {
            "title": "A critical readability and quality analysis of internet\u2010based patient information on neck dissections",
            "abstract": {
                "title": "Abstract",
                "Objective": "Patients are increasingly turning to the Internet as a source of healthcare information. Given that neck dissection is a common procedure within the field of Otolaryngology \u2010 Head and Neck Surgery, the aim of this study was to evaluate the quality and readability of online patient education materials on neck dissection.",
                "Methods": "A Google search was performed using the term \u201cneck dissection.\u201d The first 10 pages of a Google search using the term \u201cneck dissection\u201d were analyzed. The DISCERN instrument was used to assess quality of information. Readability was calculated using the Flesch\u2010Reading Ease, Flesch\u2010Kincaid Grade Level, Gunning\u2010Fog Index, Coleman\u2010Liau Index, and Simple Measure of Gobbledygook Index.",
                "Results": "Thirty\u2010one online patient education materials were included. Fifty\u2010five percent (n\u2009=\u200917) of results originated from academic institutions or hospitals. The mean Flesch\u2010Reading Ease score was 61.2\u2009\u00b1\u200911.9. Fifty\u2010two percent (n\u2009=\u200916) of patient education materials had Flesch\u2010Reading Ease scores above the recommended score of 65. The average reading grade level was 10.5\u2009\u00b1\u20092.1. The average total DISCERN score was 43.6\u2009\u00b1\u200910.1. Only 26% of patient education materials (PEMs) had DISCERN scores corresponding to a \u201cgood quality\u201d rating. There was a significant positive correlation between DISCERN scores and both Flesch\u2010Reading Ease scores and average reading grade level.",
                "Conclusions": "The majority of patient education materials were written above the recommended sixth\u2010grade reading level and the quality of online information pertaining to neck dissections was found to be suboptimal. This research highlights the need for patient education materials regarding neck dissection that are high quality and easily understandable by patients."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1016/j.wjorl.2021.07.001",
                "pmid": null,
                "pmc": "PMC10050963",
                "pub_year": "2022",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC10050963/"
            }
        },
        {
            "title": "Improving efficiency, confidence, and readability in ECG interpretation; the role of signal processing",
            "abstract": {
                "title": "Abstract",
                "Funding Acknowledgements": "Type of funding sources: Private company. Main funding source(s): Project privately funded by B-Secur Ltd",
                "Background": "Electrocardiogram (ECG) interpretation is a crucial component in the cardiac clinical pathway that enables the triaging, diagnosis, and treatment of numerous medical conditions. However, noise contamination during signal acquisition can hinder clinical interpretation by hiding or distorting the pathology of the underlying ECG. By removing noise to uncover the true beat morphology, signal processing algorithms maximize the diagnostic value of the acquired ECG and reduce clinical burden by eliminating the need for costly and time-consuming repeat investigations.",
                "Purpose": "To evaluate the impact of robust signal processing software on the clinical interpretation and diagnosis of lead I ECGs. Additionally, to investigate whether signal processing could streamline the diagnostic process by revealing pathologies not previously seen in the ECG, improving the speed of diagnosis, and enhancing the readability of the ECG.",
                "Methods": "ECG strips (42 x 10 s) representative of a variety of beat morphologies, arrhythmias, and noise types were taken from a proprietary database and provided to healthcare professionals (N = 35) pre- and post-signal processing. Participants possessed varying degrees of experience (12 x Junior (<5 years), 10 x Experienced (5-10 years), 13 x Senior (>10 years)) and specialization in cardiology. Participants were asked to provide a rhythm annotation and corresponding confidence score for each strip when provided in a blinded, randomized order. Participants then provided an additional rhythm annotation when the raw and processed signals were presented side by side, in addition to answering three subjective questions on the impact of signal processing on the ECG interpretation process.",
                "Results": "Across all participants, 80.2% of processed ECG strips were able to be given a confident diagnosis. This increased to 86% when pre- and post-processed signals were provided alongside one another \u2013 an improvement of 10.8% over the raw signals (75.2%). In response to the questions, participants agreed that the processed signal had a positive or very positive impact on; i) helping to reveal a pathology not previously seen in the raw ECG (57% of strips), ii) speeding up the diagnosis (71% of strips), and iii) improving the readability of the ECG (80% of strips).",
                "Conclusion": "By effectively removing noise whilst retaining true beat morphology, signal processing increased the amount of clinically diagnostic ECG data and allowed clinicians to make a more efficient, confident diagnosis. With the positive impacts observed across all participant experience levels, the results suggest that robust signal processing has the potential to streamline the clinical interpretation of ECGs."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1093/europace/euad122.570",
                "pmid": null,
                "pmc": "PMC10206719",
                "pub_year": "2023",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC10206719/"
            }
        },
        {
            "title": "Simulated anesthesia consent discussions demonstrate high level of comprehension and education requirements for patients: A pilot study",
            "abstract": {
                "Objective": "Patient comprehension of informed consent and demonstration of procedural understanding is often lacking in anesthesiology. The purpose of this study was to determine if patient communication in anesthesiology is being conducted effectively, and in a manner that ensures adequate communication between anesthesia professionals and their patients regarding procedures with associated risks and benefits.",
                "Methods": "Anesthesia professionals were recorded in a simulated setting explaining anesthesia procedures of increasing complexity with one control scenario. Score means were calculated, and statistical comparisons made between discussion of anesthesia procedures and the control scenario.",
                "Results": "Calculation of means for 6 readability tests demonstrated the grade level required to understand the medical practitioners' verbal communication was high and increased with complexity of the anesthesia procedure described. The control scenario required a statistically significant lower level of comprehension for the recipient of the information.",
                "Conclusion": "In simulated settings, anesthesia professionals regularly communicate procedural details in a manner that is difficult for the general public to understand. Subjects could communicate in simple terms when discussing a control.",
                "Innovation": "This pilot study demonstrated effective methodology, using artificial intelligence technology for transcription, to assess patient comprehension of verbal communication."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1016/j.pecinn.2023.100153",
                "pmid": null,
                "pmc": "PMC10194181",
                "pub_year": "2023",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC10194181/"
            }
        },
        {
            "title": "Development and psychometric appraisal of Head Nurse Research Leadership Scale",
            "abstract": {
                "title": "Abstract",
                "Aim": "To develop a Head Nurse Research Leadership Scale and evaluate its reliability and validity.",
                "Design": "A psychometric instrument validation study was conducted in two phases.",
                "Methods": "The item tool was generated based on a literature review, semi\u2010structured interview and brainstorming. Twenty experts validated the content of the initial version for two rounds. Thirty\u2010nine clinical nurses conducted the HNRLS\u2010v3 to test the readability of the items in pilot study I. Items were screened based on the critical ratio, correlation coefficient analysis, Cronbach's \u03b1 coefficient and factor analysis using the data collected from 265 nurses in pilot study II. A cross\u2010sectional survey was conducted in six hospitals to evaluate the reliability and validity between 4 January 2022 and 15 January 2022. Three hundred and sixteen nurses participated in this survey, and 60 completed the questionnaire to validate the test\u2013retest reliability between 1 February and 6 February.",
                "Results": "A 15\u2010item Head Nurse Research Leadership Scale based on 5 dimensions was developed, and the content validity was satisfied. The 15 items accounted for 77.9% of the variance. Confirmatory factor analysis showed acceptable convergent validity and discriminant validity. The Cronbach's \u03b1 coefficient, split\u2010half reliability and test\u2013retest reliability of the scale were 0.966, 0.9633 and 0.927, respectively."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1002/nop2.1592",
                "pmid": "36622948",
                "pmc": "PMC10077399",
                "pub_year": "2023",
                "url": "https://pubmed.ncbi.nlm.nih.gov/36622948/"
            }
        },
        {
            "title": "Writing strategies for improving the access of medical literature",
            "abstract": {
                "title_content_0": "When conducting a literature review, medical authors typically search for relevant keywords in bibliographic databases or on search engines like Google. After selecting the most pertinent article based on the title\u2019s relevance and the abstract\u2019s content, they download or purchase the article and cite it in their manuscript. Three major elements influence whether an article will be cited in future manuscripts: the keywords, the title, and the abstract. This indicates that these elements are the \u201ckey dissemination tools\u201d for research papers. If these three elements are not determined judiciously by authors, it may adversely affect the manuscript\u2019s retrievability, readability, and citation index, which can negatively impact both the author and the journal. In this article, we share our informed perspective on writing strategies to enhance the searchability and citation of medical articles. These strategies are adopted from the principles of search engine optimization, but they do not aim to cheat or manipulate the search engine. Instead, they adopt a reader-centric content writing methodology that targets well-researched keywords to the readers who are searching for them. Reputable journals, such as Nature and the British Medical Journal, emphasize \u201conline searchability\u201d in their author guidelines. We hope that this article will encourage medical authors to approach manuscript drafting from the perspective of \u201clooking inside-out.\u201d In other words, they should not only draft manuscripts around what they want to convey to fellow researchers but also integrate what the readers want to discover. It is a call-to-action to better understand and engage search engine algorithms, so they yield information in a desired and self-learning manner because the \u201cCloud\u201d is the new stakeholder."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.5493/wjem.v13.i3.50",
                "pmid": null,
                "pmc": "PMC10308323",
                "pub_year": "2023",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC10308323/"
            }
        },
        {
            "title": "Perspectives From a Career in Breastfeeding Research, Mentorship, and Advocacy: An Interview With Karen Wambach",
            "abstract": {
                "title_content_0": "Karen Wambach recently retired from a distinguished career in nursing education and breastfeeding research in the United States, practicing her craft during the formative years of the field of lactation consulting. Her research focused on the description of biopsychosocial influences on breastfeeding initiation and duration, as well as interventions for promoting and supporting breastfeeding among vulnerable childbearing populations, for example, adolescent mothers. Her research career trajectory mirrors the development of breastfeeding research more broadly. She began with descriptive studies and theory testing, which included the development of the Breastfeeding Experience Scale quantifying early breastfeeding problems. She then moved on to randomized clinical trials of breastfeeding education/support for adolescent mothers, and finished her funded research using a multi-behavioral, technology-based education and support intervention to promote breastfeeding, healthy lifestyle, and depression prevention in adolescent mothers. As researcher and educator in a clinical science area, she has supported evidence-based practice and translational science through her work as lead editor of many editions of the textbook Breastfeeding and Human Lactation. She is a consummate teacher, having mentored many upcoming researchers during her teaching career, and directed the undergraduate nursing honors program and PhD program at the University of Kansas School of Nursing in the United States. She also believes in serving her profession and has been an active member of American Academy of Nursing, the Midwest Nursing Research Society, the Association of Women\u2019s Health, Obstetric, and the Neonatal Nursing and the International Lactation Consultant Association, including serving on JHL\u2019s Editorial Review Board for many years. (This conversation was recorded on October 14, 2022 then transcribed and edited for readability. EC\u2009=\u2009Ellen Chetwynd; KW\u2009=\u2009Karen Wambach)"
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1177/08903344231156599",
                "pmid": "36891645",
                "pmc": "PMC10115926",
                "pub_year": "2023",
                "url": "https://pubmed.ncbi.nlm.nih.gov/36891645/"
            }
        },
        {
            "title": "Readability assessment of patient educational materials for pediatric spinal conditions from top academic orthopedic institutions",
            "abstract": {
                "Background:": "The Internet has become a popular source of health information for patients and their families. Healthcare experts recommend that the readability of online education materials be at or below a sixth grade reading level. This translates to a standardized Flesch Reading Ease Score between 81 and 90, which is equivalent to conversational English. However, previous studies have demonstrated that the readability of online education materials of various orthopedic topics is too advanced for the average patient. To date, the readability of online education materials for pediatric spinal conditions has not been analyzed. The objective of this study was to assess the readability of online educational materials of top pediatric orthopedic hospital websites for pediatric spinal conditions.",
                "Methods:": "Online patient education materials from the top 25 pediatric orthopedic institutions, as ranked by the U.S. News and World Report hospitals for pediatric orthopedics, were assessed utilizing multiple readability assessment metrics including Flesch\u2013Kincaid, Flesch Reading Ease, Gunning Fog Index, and others. Correlations between academic institutional ranking, geographic location, and the use of concomitant multimedia modalities with Flesch\u2013Kincaid scores were evaluated using a Spearman regression.",
                "Results:": "Only 32% (8 of 25) of top pediatric orthopedic hospitals provided online health information at or below a sixth grade reading level. The mean Flesch\u2013Kincaid score was 9.3\u2009\u00b1\u20092.5, Flesch Reading Ease 48.3\u2009\u00b1\u200916.2, Gunning Fog Score 10.7\u2009\u00b1\u20093.0, Coleman\u2013Liau Index 12.1\u2009\u00b1\u20092.8, Simple Measure of the Gobbledygook Index 11.7\u2009\u00b1\u20092.1, Automated Readability Index 9.0\u2009\u00b1\u20092.7, FORCAST 11.3\u2009\u00b1\u20091.2, and Dale\u2013Chall Readability Index 6.7\u2009\u00b1\u20091.4. There was no significant correlation between institutional ranking, geographic location, or use of video material with Flesch\u2013Kincaid scores (p\u2009=\u20090.1042, p\u2009=\u20090.7776, p\u2009=\u20090.3275, respectively).",
                "Conclusion:": "Online educational material for pediatric spinal conditions from top pediatric orthopedic institutional websites is associated with excessively complex language which may limit comprehension for the majority of the US population.",
                "Type of study/Level of evidence:": "Economic and Decision Analysis/level III."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1177/18632521231156435",
                "pmid": null,
                "pmc": "PMC10242376",
                "pub_year": "2023",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC10242376/"
            }
        },
        {
            "title": "Evaluation of Reading Level of Result Letters Sent to Patients from an Academic Primary Care Practice",
            "abstract": {
                "Background": "In primary care, low health literacy, particularly reading ability, is associated with worse health outcomes. Most physicians do not receive feedback on the reading levels of written communication that they may provide to patients, including result letters.",
                "Objective": "Our study compares the readability of result letters, written by resident versus attending physicians, to patients with positive or negative screens for reading ability, as determined by the single-item literacy screener (SILS).",
                "Methods": "Result letters to 50 patients at high risk and 50 patients at low risk of low reading ability were randomly selected starting from January 1st, 2020 at Albany Medical Center. Flesch\u2013Kincaid Grade Level (FKGL), Gunning Fog Index (GFI), Coleman\u2013Liau Index (CLI), Simple Measure of Gobbledygook (SMOG), and Flesch Reading Ease (FRE) were used to compare the readability of resident versus attending result letters.",
                "Results": "For all SILS levels, attending physicians wrote result letters at a lower grade level than resident physicians based on the FKGL, GFI, and SMOG indices. The FKGL, GFI, and SMOG readability scores of result letters written to patients with SILS 3\u20135 were also lower when written by attending physicians compared to resident physicians.",
                "Conclusions": "Result letters written by attending physicians may be easier to read than result letters written by resident physicians, especially for patients with low reading ability. Future electronic health record (EHR) software should give physicians and providers feedback on the reading level of their written communication."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1177/23333928231172142",
                "pmid": null,
                "pmc": "PMC10134153",
                "pub_year": "2023",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC10134153/"
            }
        },
        {
            "title": "Evaluating the readability, quality and reliability of online patient education materials on transcutaneuous electrical nerve stimulation (TENS)",
            "abstract": {
                "title_content_0": "Increasing digitization also raises concerns regarding the reliability and comprehensibility of online health information. In this study, we aimed to examine the readability, reliability, and quality of internet-based patient education materials on \u201ctranscutaneous electrical nerve stimulation.\u201d On September 15, 2022, we used Google search engine to search the keyword \u201cTranscutaneous Electrical Nerve Stimulation\u201d and obtained information from 200 websites. The readability of the websites was evaluated using the Flesch Reading Ease Score (FRES), Flesch\u2013Kincaid Grade Level, Simple Measure of Gobbledygook, and Gunning Fog. The Journal of American Medical Association score and Health on the Net Foundation code of conduct were used to determine the reliability of the websites, whereas the DISCERN score and Global Quality Score were used to evaluate the quality of the websites. In the readability analysis of 102 websites that met the inclusion criteria of this study, we found that the Flesch Reading Ease Score was 47.91\u2009\u00b1\u200913.79 (difficult), average Flesch\u2013Kincaid Grade Level and Simple Measure of Gobbledygook were 11.20\u2009\u00b1\u20092.85 and 10.53\u2009\u00b1\u20092.11 years, respectively, and average Gunning Fog score was 14.04\u2009\u00b1\u20092.74 (very difficult). Commercial websites constituted the highest proportion of websites (n = 36, 35.5%). Overall, 16.7% of the websites were found to be of high quality according to the Global Quality Score, 16 (15.7%) websites had Health on the Net Foundation code of conduct certification, and 8.8% of the websites were found to be highly reliable according to the Journal of American Medical Association scores. There was a statistically significant difference between website typologies and quality and reliability scores (P < .001). Compared with the sixth-grade level recommended by the American Medical Association and the National Institute of Health, the readability of transcutaneous electrical nerve stimulation-related internet-based patient education materials was considerably high, but they showed low reliability and moderate-to-poor quality. Thus, the quality, reliability, and readability of websites developed by health professionals play a major role in conveying accurate and easily understandable information."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1097/MD.0000000000033529",
                "pmid": null,
                "pmc": "PMC10118348",
                "pub_year": "2023",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC10118348/"
            }
        },
        {
            "title": "Impact of clinical expertise and choice of wearable device on accuracy to detect atrial fibrillation via single-lead ECGs",
            "abstract": {
                "title": "Abstract",
                "Funding Acknowledgements": "Type of funding sources: None.",
                "Background": "Manual interpretation of single-lead ECGs (SL-ECG) is often required to confirm a diagnosis of atrial fibrillation (AF). However, accuracy to detect AF via SL-ECGs may vary according to clinical expertise and choice of the wearable device.",
                "Aims": "To compare the accuracy to detect AF via single-lead ECG from five different wearable devices (Apple Watch, Fitbit Sense, KardiaMobile, Samsung Galaxy Watch, Withings ScanWatch) between cardiologists, internal medicine (IM) residents, and medical students.",
                "Methods": "In this prospective study, invitations to an online survey were distributed via digital invitations among physicians from major Swiss hospitals and medical students from Swiss universities. Participants needed to classify 50 SL-ECGs (from 10 patients and 5 different devices) into three categories: sinus rhythm, AF, or inconclusive. This classification was compared to the diagnosis from an almost simultaneously recorded 12-lead ECG interpreted by two independent cardiologists. In addition, participants were asked to choose the best/worst quality/readability of each manufacturer\u2019s SL-ECG. (Figure A)",
                "Results": "Overall 450 participants rated 10\u2019865 SL-ECGs. Sensitivity and specificity for the detection of AF via SL-ECG was 75% and 92% for cardiologists, 70% and 86% for IM residents, 55% and 65% for master medical students (year 4-6) and 45% and 58% for bachelor medical students (year 1-3), p<0.001, (Figure A). Participants which stated prior experience in interpreting SL-ECGs demonstrated a sensitivity and specificity of 65% and 81% compared to a sensitivity and specificity of 56% and 67% for participants with no prior experience in interpreting SL-ECGs, p<0.001. Of all participants, 107 rated all 50 ECGs. Diagnostic accuracy of the first five interpreted SL-ECGs was 60% (IQR 40-80%) and diagnostic accuracy of the last five interpreted SL- ECGs was 80% (IQR 60-90%), p<0.001. No significant difference in the accuracy of AF detection was seen between the 5 different wearable devices, p=0.58. 203 participants (45%) and 226 participants (50 %) ranked SL- ECGs from the Apple Watch as the best quality and readability, respectively.",
                "Conclusion": "SL-ECG can be difficult to interpret. Accuracy to correctly identify AF depends on clinical expertise, while the choice of the wearable device seems to have no impact.Figure A"
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1093/europace/euad122.476",
                "pmid": null,
                "pmc": "PMC10207389",
                "pub_year": "2023",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC10207389/"
            }
        },
        {
            "title": "TANGO: Development of Consumer Information Leaflets to Support TAperiNG of Opioids in Older Adults with Low Back Pain and Hip and Knee Osteoarthritis",
            "abstract": {
                "Introduction": "Globally, the rate of opioid prescription is high for chronic musculoskeletal conditions despite guidelines recommending against their use as their adverse effects outweigh their modest benefit. Deprescribing opioids is a complex process that can be hindered by multiple prescriber- and patient-related barriers. These include fear of the process of, or outcomes from, weaning medications, or a lack of ongoing support. Thus, involving patients, their carers, and healthcare professionals (HCPs) in the development of consumer materials that can educate and provide support for patients and HCPs over the deprescribing process is critical to ensure that the resources have high readability, usability, and acceptability to the population of interest.",
                "Objective": "This study aimed to (1) develop two educational consumer leaflets to support opioid tapering in older people with low back pain (LBP) and hip or knee osteoarthritis (HoKOA), and (2) evaluate the perceived usability, acceptability, and credibility of the consumer leaflets from the perspective of consumers and HCPs.",
                "Design": "This was an observational survey involving a consumer review panel and an HCP review panel.",
                "Participants": "30 consumers (and/or their carers) and 20 HCPs were included in the study. Consumers were people older than 65 years of age who were currently experiencing LBP or HoKOA, and with no HCP background. Carers were people who provided unpaid care, support, or assistance to an individual meeting the inclusion criteria for consumers. HCPs included physiotherapists (n\u00a0=\u00a09), pharmacists (n\u00a0=\u00a07), an orthopaedic surgeon (n\u00a0=\u00a01), a rheumatologist (n\u00a0=\u00a01), nurse practitioner (n\u00a0=\u00a01) and a general practitioner (n\u00a0=\u00a01), all with at least three\u00a0years of clinical experience and who reported working closely with this target patient population within the last 12 months.",
                "Methods": "Prototypes of two educational consumer leaflets (a brochure and a personal plan) were developed by a team of LBP, OA, and geriatric pharmacotherapy researchers and clinicians. The leaflet prototypes were evaluated by two separate chronological review panels involving (1) consumers and/or their carers, and (2) HCPs. Data collection for both panels occurred via an online survey. Outcomes were the perceived usability, acceptability, and credibility of the consumer leaflets. Feedback received from the consumer panel was used to refine the leaflets, before circulating the leaflets for further review by the HCP panel. Additional feedback from the HCP review panel was then used to refine the final versions of the consumer leaflets.",
                "Results": "Both consumers and HCPs perceived the leaflets and personal plan to be usable, acceptable, and credible. Consumers rated the brochure against several categories, which scored between 53 and 97% positive responses. Similarly, the overall feedback provided by HCPs was 85\u2013100% positive. The modified System Usability Scale scores obtained from HCPs was 55\u201395% positive, indicating excellent usability. Feedback for the personal plan from both HCPs and consumers was largely positive, with consumers providing the highest positive ratings (80\u201393%). While feedback for HCPs was also high, we did identify that prescribers were hesitant to provide the plan to patients frequently (no positive responses).",
                "Conclusions": "This study led to the development of a leaflet and personal plan to support the reduction of opioid use in older people with LBP or HoKOA. The development of the consumer leaflets incorporated feedback provided by HCPs and consumers to maximise clinical effectiveness and future intervention implementation.",
                "Supplementary Information": "The online version contains supplementary material available at 10.1007/s40266-023-01011-x."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1007/s40266-023-01011-x",
                "pmid": "36972011",
                "pmc": "PMC10040925",
                "pub_year": "2023",
                "url": "https://pubmed.ncbi.nlm.nih.gov/36972011/"
            }
        },
        {
            "title": "Poster 307: A Standardized Method that Improves Readability of Patient Education Materials in Sports Medicine Knee Injuries",
            "abstract": {
                "Objectives:": "Sports-related knee injuries such as anterior cruciate ligament (ACL) or meniscus tears are very common, and there are ample resources for knee injury-related patient education materials (PEMs). However, the anatomy, pathology, and treatment plans of knee injuries are often described in terms that are obscure to patients. While the internet has become a vast resource for medical information, online PEMs in orthopaedics have consistently been shown to be written above the NIH- recommended sixth-grade reading level to the detriment of patient health literacy. The purpose of this study is to develop a standardized method to improve readability of orthopaedic PEMs without diluting their critical content by reducing the use of complex words (>3 syllables) and shortening sentence length to <15 words.",
                "Methods:": "OrthoInfo, a patient education website developed by the Academy of American Orthopedic Surgeons, was queried for PEMs relevant to the care of athletic injuries of the knee. Inclusion criteria were PEMs that were unique, pertained to topics of knee pathology in sports medicine, and written in a prose-format. Exclusion criteria were information presented in video or slideshow format, or topics not pertaining to knee pathology in sports medicine. Readability of PEMs was evaluated using seven unique readability formulas before and after applying a standardized method to improve readability while preserving critical content (reducing the use of >3 syllable words and ensuring sentence length is <15 words). Paired samples t-tests were conducted to assess the relationship between reading levels of the original PEMs and reading level of edited PEMs.",
                "Results:": "Reading levels differed significantly between the 23 original PEMs and edited PEMs across all seven readability formulas (p<0.01). Mean Flesch Kincaid Grade Level of original PEMs (9.8\u00b11.4) was significantly increased compared to that of edited PEMs (6.4\u00b11.1) (p=1.9x10-13). 4.0% of original PEMs met NIH recommendations of a sixth-grade reading level compared to 48.0% of modified PEMs.",
                "Conclusions:": "A standardized method that reduces the use of >3 syllable words and ensures sentence length is <15 words significantly reduces the reading-grade level of PEMs for sports-related knee injuries. Orthopaedic organizations and institutions should apply this simple standardized method when creating PEMs to enhance health literacy."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1177/2325967123S00280",
                "pmid": null,
                "pmc": "PMC10392504",
                "pub_year": "2023",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC10392504/"
            }
        },
        {
            "title": "Online resources for information on shoulder arthroplasty: an assessment of quality and readability",
            "abstract": {
                "Background": "Many patients use online resources to educate themselves on surgical procedures and make well-informed healthcare decisions. The aim of our study was to evaluate the quality and readability of online resources exploring shoulder arthroplasty.",
                "Methods": "An internet search pertaining to shoulder arthroplasty (partial, anatomic, and reverse) was conducted using the three most popular online search engines. The top 25 results generated from each term in each search engine were included. Webpages were excluded if they were duplicates, advertised by search engines, subpages of other pages, required payments or subscription, or were irrelevant to our scope. Webpages were classified into different source categories. Quality of information was assessed by HONcode certification, Journal of the American Medical Association (JAMA) criteria, and DISCERN benchmark criteria. Webpage readability was assessed using the Flesch reading ease score (FRES).",
                "Results": "Our final dataset included 125 web pages. Academic sources were the most common with 45 web pages (36.0%) followed by physician/private practice with 39 web pages (31.2%). The mean JAMA and DISCERN scores for all web pages were 1.96\u00b11.31 and 51.4\u00b110.7, respectively. The total mean FRES score was 44.0\u00b111.0. Only nine web pages (7.2%) were HONcode certified. Websites specified for healthcare professionals had the highest JAMA and DISCERN scores with means of 2.92\u00b10.90 and 57.96\u00b18.91, respectively (P<0.001). HONcode-certified webpages had higher quality and readability scores than other web pages.",
                "Conclusions": "Web-based patient resources for shoulder arthroplasty information did not show high-quality scores and easy readability. When presenting medical information, sources should maintain a balance between readability and quality and should seek HONcode certification as it helps establish the reliability and accessibility of the presented information.",
                "Level of evidence": "IV."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.5397/cise.2023.00290",
                "pmid": "37607858",
                "pmc": "PMC10497924",
                "pub_year": "2023",
                "url": "https://pubmed.ncbi.nlm.nih.gov/37607858/"
            }
        },
        {
            "title": "FRI401 Co-creating Patient-facing Materials For Primary Ovarian Insufficiency",
            "abstract": {
                "title": "Abstract",
                "title_content_0": "Disclosure: I.R. McDonald: None. E.S. Blocker: None. E.A. Weyman: None. C.K. Welt: None. A.A. Dwyer: None.",
                "title_content_1": "Evidence suggests people with POI have unmet health and information needs that contribute to impaired health-related quality of life (HR-QoL). Co-creation can produce high-quality patient-facing educational materials that respond to patient-identified needs and overcome healthcare disparities. Our goal was to co-create patient-facing materials that respond to unmet health and informational needs, mitigate health disparities, and improve HR-QoL for people with POI. The project had three sequential steps. First, we synthesized the state of the science on HR-QoL, care, and management of POI. Next, we conducted a systematic scoping review to identify \u201cbest practices\u201d for co-creating patient-facing materials. Last, we employed co-creation best practices, patient engagement, and an iterative \u201cdesign thinking\u201d process to co-create and evaluate patient-facing POI materials. The HR-QoL scoping review identified three inter-related themes related to impaired HR-QoL in POI (diagnostic odyssey, isolation & stigma, and ego integrity) along with sub-themes of decreased sexual function, altered body image, psychological vulnerability and catastrophizing. The co-creation scoping review identified 6 best practices for co-creation: (1) begin with a review of the literature, (2) utilize a framework to inform the process, (3) involve clinical and patient experts from the beginning, (4) engage diverse perspectives, (5) ensure patients have the final decision, and (6) employ validated evaluation tools. Informed by the synthesis of care and management of POI, we partnered with patients to create concise patient-facing materials responding to unmet health and informational needs of people with POI. Patients engaged in co-creation using the \u201cdesign thinking\u201d process: empathize, define the question, ideate/brainstorm, iterate prototypes (n=4), and testing using the \u201cgold standard\u201d Patient Education Materials Assessment Tool (PEMAT). The final 2-page patient-facing materials were evaluated using 7 readability algorithms revealing a consensus reading level of 7th grade (i.e., fairly easy to read for an 11-13 year-old). The patient partners rated the materials as highly acceptable and actionable. The online PEMAT evaluation is underway in collaboration with POI patient organizations. We envision such co-creation will produce understandable and actionable materials (i.e., all PEMAT domain scores >80%). This project may serve as a roadmap for healthcare organizations and patients to collaborate and surmount health disparities and improve care for other health conditions.",
                "title_content_2": "Presentation: Friday, June 16, 2023"
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1210/jendso/bvad114.1594",
                "pmid": null,
                "pmc": "PMC10555954",
                "pub_year": "2023",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC10555954/"
            }
        },
        {
            "title": "Readability and reliability of online patient education materials about statins",
            "abstract": {
                "Objective": "Statins are the cornerstone for the prevention and treatment of cardiovascular disease. Patients often consult online patient education materials (OPEMs) to inform medical decision-making. We therefore aimed to assess the readability and reliability of OPEMs related to statins.",
                "Methods": "A total of 17 statin-related terms were queried using an online search engine to identify the top 20 search results for each statin-related term. Each OPEM was then grouped into the following categories based on 2 independent reviewers: government OPEMs (national, state, or local government agencies); healthcare/nonprofit OPEMs (major health systems and nonprofit organizations with a specific cardiovascular health focus); industry/commercial OPEMs (pharmaceutical manufacturers and online pharmacies); lay press OPEMs (healthcare-oriented news organizations); and dictionary/encyclopedia OPEMs. Grade-level readability for each OPEM was calculated using 5 standard readability metrics and compared with AMA-recommended readability recommendations. Reliability of each OPEM was evaluated using the JAMA benchmark criteria for online health information and certification from Health on the Net (HONCode).",
                "Results": "A total of 340 websites were identified across the 17 statin search terms. There were 211 statin OPEMs after excluding non-OPEM results; 172 OPEMs had unique content. Statin OPEM readability exceeded the recommended 6th grade AMA reading level (average reading grade level of 10.9). The average JAMA benchmark criteria score was 2.13 (on a scale of 0\u20134, with higher scores indicating higher reliability), and only 60% of statin OPEMs were HONCode-certified. There was an inverse association between readability and reliability. The most readable results were from industry and commercial sources, while the most reliable sites were from lay press sources.",
                "Conclusions": "Statin OPEMs are written at an overall averaging reading grade level of 10.9. There was an inverse association between readability and reliability. Lack of accessible, high-quality online health information may contribute to statin nonadherence."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1016/j.ajpc.2023.100594",
                "pmid": null,
                "pmc": "PMC10562660",
                "pub_year": "2023",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC10562660/"
            }
        },
        {
            "title": "Initial Steps in Creating a Patient-Centric Addendum to Clinical Trial Informed Consent Forms",
            "abstract": {
                "Introduction": "The purpose of the informed consent form (ICF) is to outline the risks and benefits of an interventional clinical trial to potential participants. The aim of this study was to explore the feasibility of a short addendum to the ICF, summarizing key points most relevant to potential participants.",
                "Methods": "A sample of 20 ICFs was reviewed against the requirements of the U.S. federal regulation documents and assessed for readability. Alongside the ICF review, we conducted focus groups and one-on-one interviews with people with lung cancer (n\u00a0= 9) to learn what information was most important when considering participation in a clinical trial using a hypothetical phase 3 ICF.",
                "Results": "The 20 ICFs reviewed were from phases 1 to 3, expanded-access, and single-patient trials covering predominantly NSCLC; 60% were global. The mean length of the ICFs was 21 (range: 15\u201334) pages. The average reading level was tenth grade whereas the average U.S. reading level was eighth grade. Readability varied by section, the \u201cpurpose of the study\u201d section had the highest reading level. In the qualitative research component, participants were \u201coverwhelmed\u201d by the hypothetical ICF. Participants were also asked to list information for the addendum; their suggestions broadly map to federal regulations. An addendum with reference to sections in the ICF for additional details was well received.",
                "Conclusions": "The variations in ICF architecture and readability make it difficult for patients to make an informed decision to participate in a clinical trial. Implications extend beyond lung cancer, highlighting key areas for ICF improvements and providing a roadmap for developing a patient-centric addendum."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1016/j.jtocrr.2023.100575",
                "pmid": null,
                "pmc": "PMC10568273",
                "pub_year": "2023",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC10568273/"
            }
        },
        {
            "title": "How readable and quality are online patient education materials about Helicobacter pylori?: Assessment of the readability, quality and reliability",
            "abstract": {
                "title_content_0": "This study aimed to examine the readability, reliability, quality, and content of patient education materials (PEM) on the Internet about \u201cHelicobacter pylori (H pylori).\u201d A search was conducted on March 14, 2023, using the keyword \u201cH pylori\u201d in the Google search engine. The readability of PEMs was assessed using the Flesch reading ease score, FKGL, simple measure of gobbledygook, and gunning fog readability formulas. The reliability and quality of the websites were determined using the Journal of American Medical Association score, health on the net foundation code of conduct, global quality score, and DISCERN score. A total of 93 patient education websites were included in the study. In the readability analysis of PEMs, we determined that the Flesch reading ease score was 49,73 (47,46\u201352,00) (difficult), the mean Flesch\u2013Kincaid grade level and simple measure of gobbledygook were 9,69 (9,26\u201310,12) and 9,28 (8,96\u20139,61) years, respectively, and the mean gunning fog score was 12,47 (12,03\u201312,91) (very difficult). Most of the evaluated patient educational materials were commercial websites (n\u2005=\u200550, 53.8%). It was found that 16.1% of the websites were of high quality according to global quality score, 30.1% were HON code certified, and 23.7% of the websites were highly reliable according to Journal of American Medical Association scores. There was no statistically significant difference between website typologies and readability (P\u2005>\u2005.05). However, there was a statistically significant difference between website typologies and quality and reliability scores (P\u2005<\u2005.005). Compared to the sixth grade level recommended by the American Medical Association and National Institutes of Health, the readability of H pylori-related internet-based PEMs is quite high. On the other hand, the reliability and quality of the PEMs were determined as moderate to poor. PEMs for issues threatening public health should be prepared with attention to recommendations on readability."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1097/MD.0000000000035543",
                "pmid": null,
                "pmc": "PMC10615431",
                "pub_year": "2023",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC10615431/"
            }
        },
        {
            "title": "Health literacy in rotator cuff repair: a quantitative assessment of the understandability of online patient education material",
            "abstract": {
                "Background": "The American Medical Association and National Institutes of Health recommend online health information be written at a 6th grade or lower reading level for clear understanding. While syntax reading grade level has previously been utilized, those analyses do not determine whether readers are processing key information (understandability) or identifying available actions to take (actionability). The Patient Education Materials Assessment Tool (PEMAT-P) is a method to measure the understandability and actionability of online patient education materials. The purpose of this study was to evaluate online resources regarding rotator cuff repair utilizing measures of readability, understandability, and actionability.",
                "Methods": "The search term \u201crotator cuff surgery\u201d was used in two independent online searches to obtain the top 50 search results. The readability of included resources was quantified using valid objective algorithms: Flesch-Kincaid Grade-Level, Simple Measure of Gobbledygook grade, Coleman-Liau Index, and Gunning Fog Index. The PEMAT-P form was used to assess actionability and understandability.",
                "Results": "A total of 49 unique websites were identified to meet our inclusion criteria and were included in our analysis. The mean Flesch-Kincaid Grade Level graded materials at a 10.6 (approximately a 10th grade reading level), with only two websites offering materials at a 6th grade reading level or below. The remaining readability studies graded the mean reading level at high school or greater, with the Gunning Fog Index scoring at a collegiate reading level. Mean understandability and actionability scores were 64.6% and 29.5%, respectively, falling below the 70% PEMAT score threshold for both scales. Fourteen (28.6%) websites were above the threshold for understandability, while no website (0%) scored above the 70% threshold for actionability. When comparing source categories, commercial health publishers provided websites that scored higher in understandability (P\u00a0<\u00a0.05), while private practice materials scored higher in actionability (P\u00a0<\u00a0.05). Resources published by academic institutions or organizations scored lower in both understandability and actionability than private practice and commercial health publishers (P\u00a0<\u00a0.05). No readability, understandability, or actionability score was significantly associated with search result rank.",
                "Conclusion": "Overall, online patient education materials related to rotator cuff surgery scored poorly with respect to readability, understandability, and actionability. Only two (4.1%) of the patient education websites scored at the American Medical Association and National Institutes of Health recommended reading level. Fourteen (28.6%) scored above the 70% PEMAT score for understandability; however, no website met the threshold for actionability."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1016/j.jseint.2023.06.016",
                "pmid": null,
                "pmc": "PMC10638567",
                "pub_year": "2023",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC10638567/"
            }
        },
        {
            "title": "Evaluation of readability levels of online patient education materials for female pelvic floor disorders",
            "abstract": {
                "title_content_0": "Most women hesitate to seek help from healthcare providers as they find it difficult to share complaints of involuntary leakage or vaginal prolapse. Hence, they often refer to the websites of national and/or international bodies\u2019 patient education materials (PEMs), which are considered the most reliable sources. The crucial factor that determines their usefulness is their readability level, which makes them \u201ceasy\u201d or \u201cdifficult\u201d to read, and is recommended, not to exceed the sixth grade level. In this study, we aimed to assess the readability levels of Turkish translated PEMs from the websites of the International Urogynecological Association and the European Association of Urology and the PEMs originally written in Turkish from the website of the Society of Urological Surgery in Turkey. All the PEMs (n\u2005=\u200552) were analyzed by online calculators using the Atesman formula, Flesch-Kincaid grade level, and Gunning Fog index. The readability parameters, number of sentences, words, letters, syllables, and readability intervals of these methods were compared among the groups using the Kruskal-Wallis test, or ANOVA test, with post hoc comparisons where appropriate. The readability level of all PEMs is at least at an \u201caveragely difficult\u201d interval, according to both assessment methods. No significant differences were found among the PEM groups in terms of readability parameters and assessment methods (P\u2005>\u2005.05). Whether original or translated, international or national societies\u2019 PEMs\u2019 readability scores were above the recommended level of sixth grade. Thus, the development of PEMs needs to be revised accordingly by relevant authorities."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1097/MD.0000000000036636",
                "pmid": null,
                "pmc": "PMC10754616",
                "pub_year": "2023",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC10754616/"
            }
        },
        {
            "title": "Development of text messages for primary prevention of cardiovascular disease in persons with HIV",
            "abstract": {
                "Objective": "Persons with HIV (PWH) have increased risk for atherosclerotic cardiovascular disease (CVD). Despite this increased risk, perceived cardiovascular risk among PWH is low, and interventions that are known to be beneficial in the general population, such as statins, have low uptake in this population. We sought to develop a bank of text messages about (1) the association between HIV and CVD and (2) advice on reducing cardiovascular risk.",
                "Methods": "We developed an initial bank of 162 messages. We solicited feedback from 29 PWH recruited from outpatient clinics providing HIV care at a large urban tertiary medical center and a public hospital in San Francisco, California. Participants reviewed 7\u201310 messages each and rated message usefulness, readability, and potential impact on behavior on a scale from 1 (least) to 5 (most). We also collected open-ended feedback on the messages and data on preferences about message timing.",
                "Results": "The average score for the messages was 4.4/5 for usefulness, 4.4/5 for readability, and 4.0/5 for potential impact on behavior. The text messages were iteratively revised based on participant feedback, and lowest-rated messages were removed from the message bank. The final message bank included 116 messages on diet (30.2%), physical activity (24.8%), tobacco (11.2%), the association between HIV and cardiovascular disease (9.5%), general heart health (6.9%), cholesterol (5.2%), blood pressure (4.3%), blood sugar (2.6%), sleep (2.6%), and weight (2.6%).",
                "Conclusion": "We describe an approach for developing educational text messages on primary prevention of cardiovascular disease among PWH."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1016/j.cvdhj.2023.11.002",
                "pmid": null,
                "pmc": "PMC10787147",
                "pub_year": "2023",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC10787147/"
            }
        },
        {
            "title": "Assessing the readability of online information about jones fracture",
            "abstract": {
                "BACKGROUND": "Hand in hand with technological advancements, treatment modalities continue to grow. With the turn of the century, the internet has become the number one source of information for almost every topic. Thus, many patients look toward the internet as their primary source of information to learn about their respective medical conditions. The American Medical Association and National Institute of Health strongly recommend that online medical information be written at the 6th to 8th-grade level to aid comprehension by patients of all literacy backgrounds.",
                "AIM": "To assess the readability of online information regarding Jones fracture. Our hypothesis is that the reading level of medical information published on websites far exceeds the recommended reading level of 6th-8th grade as proposed by the American Medical Associate and National Institute of Health. The result of this study can help us formulate improved recommendations for publishing more comprehensible material and, thus, eventually improve patient compliance and clinical outcomes.",
                "METHODS": "The exact phrase \u201cJones fracture\u201d was queried on the three most common search engines, Google, Yahoo!, and Bing, on December 28, 2022. As of December 2022, Google held 84%, Bing held 9%, and Yahoo! held 2% of the worldwide search engine market share. Web pages uniform resource locator from the first three pages of search results were recorded from each search engine. These web pages were classified according to academic, physician-sponsored, governmental and non-government organizations (NGO), commercial, and unspecified as per formally defined categories. Websites associated with an educational institution or medical organization were classified as academic. Websites with products for sale, corporate sponsorship, or advertisements were classified as commercial. Governmental websites or NGOs comprised those that received government subsidies or grants. Webpages that were independently owned by physicians or physician groups were respectively classed as physician sponsored. The remainder of websites that did not fall under the above categories were classified as unspecified.",
                "RESULTS": "A total of 93 websites were analyzed for reading assessment. A whopping 44% of websites were commercial, followed by 22% of physician-sponsored websites. Third place belonged to non-government organization websites holding a 15% share. The academic website held a meager 9% portion, while unspecified sites were 3%. The table illustrates mean readability scores, along with average cumulative grade level. The average grade level was 10.95 \u00b1 2.28 for all websites, with a range of 6.18 to 18.90. Since P values were more than 0.05, there was not a significant statistical difference between the first page results and the results of all pages. Thus, we can rationalize that readability scores are consistent throughout all pages of a website.",
                "CONCLUSION": "Hand in hand with technological advancements, treatment modalities continue to grow. With the turn of the century, the internet has become the number one source of information for almost every topic. Thus, many patients look towards the internet as the primary source of information to learn about their respective medical conditions. Our study demonstrates that current online medical information regarding Jones fracture is written at an extraordinarily high-grade level, with an average grade level of all websites at 10.95, nearly an 10th-grade educational level. The American Medical Association and National Institute of Health strongly recommend that online medical information should be written at the 6th to 8th-grade level to aid comprehension by patients of all literacy backgrounds. On the contrary, most of the medical information evaluated was at an 10th-grade level, which far exceeds recommendations by AMA and NIH. This is particularly relevant because readability scores are directly proportional to the level of comprehension attained by readers, thus directly impacting patient outcomes. In conclusion, we suggest and encourage that all online reading materials should be re-written at the 6th to 8th-grade level in a public service effort to increase compliance with treatment goals and raise awareness of preventive measures."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.5662/wjm.v13.i5.439",
                "pmid": null,
                "pmc": "PMC10789098",
                "pub_year": "2023",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC10789098/"
            }
        },
        {
            "title": "Spanish Translation and Cultural Adaptations of Physical Therapy Parent Educational Materials for Use in Neonatal Intensive Care",
            "abstract": {
                "Background": "A paucity of Spanish language, culturally relevant parent education materials in the healthcare setting results in suboptimal care for Latinx families and further perpetuates health disparities. The purpose of this article is to describe the process for Spanish translation and cultural adaptations to parent education materials of a parent-centered physical therapy program designed to support maternal mental health and infant development during Neonatal Intensive Care (NICU).",
                "Methods": "Two bilingual physical therapy (PT) students translated educational materials from English to Spanish and were proofread by a professional translator. Next, we conducted a materials review with 5 members of the Latine Community Review Board (CRB), a \u201cstanding\u201d advisory group of natively Spanish-speaking, Latine North Carolinians who contract with research teams under the coordination of the Inclusive Science Program (ISP) of the North Carolina Translational and Clinical Sciences Institute (NC TraCS). Review session recruitment, facilitation, and data analysis were conducted by bilingual NC TraCS project managers and the primary investigator for the main feasibility study. Readability analyses were performed at the final stage of translation and adaptation.",
                "Results": "Themes from CRB review sessions for improvement included to 1) use parent-friendly language, 2) use the plural masculine form of gendered language for caregivers to include all gender identities in this neonatal context, 3) address challenges with direct translation, and 4) use written education materials to supplement in-person, hands-on training with parents and their infants.\u00a0All translated materials received a grade level of 5 on the Crawford grade-level index.",
                "Conclusion": "Based on CRB feedback and readability analysis, the translation and cultural-adaptation process resulted in comprehensible written parent education materials for Spanish-speaking families. Review meetings with the CRB reinforced the need for Spanish materials in the healthcare setting. Further assessment of these materials with Spanish-speaking families in the NICU setting is needed."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.2147/PPA.S432635",
                "pmid": "38229765",
                "pmc": "PMC10790584",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/38229765/"
            }
        },
        {
            "title": "ChatGPT: Is This Patient Education Tool for Urological Malignancies Readable for the General Population?",
            "abstract": {
                "Background": "With widespread adoption of technological advancements in everyday life, patients are now increasingly able and willing to obtain information about their health conditions, treatment options, and indeed expected outcomes via the convenience of any device than can access the worldwide web. This introduces another aspect of patient care in the provision of healthcare for the modern doctor. ChatGPT is the first of an increasing number of self learning programs that have been released recently which may revolutionize and impact healthcare delivery.",
                "Methods": "The aim of this study is to obtain an objective measure of the readability of information provided on ChatGPT when compared with current validated patient information sheets provided by government health institutions in Western Australia. The same structured questions were input into the program for three major urological malignancies (urothelial, renal, and prostate), with the response generated evaluated with a validated readability scoring system \u2013 Flesch-Kincaid reading ease score. The same scoring system was then applied to current patient information sheets in circulation from Cancer Council Australia and UpToDate.",
                "Results": "Findings in this study looking at ease of readability of information provided on ChatGPT as compared to other government bodies and healthcare institutions confirm that they are non-inferior and may be a useful tool or adjunct to the traditional clinic based consultations. Ease of use of the information generated from ChatGPT was increased further when the question was modified to target an audience of 16 years of age, the average level of education attained by an Australian.",
                "Discussion": "Future research can be done to look into incorporating the use of similar technologies to increase efficiency in the healthcare system and reduce healthcare costs."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.2147/RRU.S440633",
                "pmid": "38259300",
                "pmc": "PMC10800281",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/38259300/"
            }
        },
        {
            "title": "Health Literacy Analytics of Accessible Patient Resources in Cardiovascular Medicine: What are Patients Wanting to Know?",
            "abstract": {
                "Introduction": "There remains an increasing utilization of internet-based resources as a first line of medical knowledge. Among patients with cardiovascular disease, these resources often are relied upon for numerous diagnostic and therapeutic modalities. However, the reliability of this information is not fully understood. The aim of this study was to provide a descriptive profile on the literacy quality, readability, and transparency of publicly available educational resources in cardiology.",
                "Methods": "The frequently asked questions and associated online educational articles on common cardiovascular diagnostic and therapeutic interventions were investigated using publicly available data from the Google RankBrain machine learning algorithm after applying inclusion and exclusion criteria. Independent raters evaluated questions for Rothwell\u2019s Classification and readability calculations.",
                "Results": "Collectively, 520 questions and articles were evaluated across 13 cardiac interventions, resulting in 3,120 readability scores. The sources of articles were most frequently from academic institutions followed by commercial sources. Most questions were classified as \u201cFact\u201d at 76.0% (n = 395), and questions regarding \u201cTechnical Details\u201d of each intervention were the most common subclassification at 56.3% (n = 293).",
                "Conclusions": "Our data show that patients most often are using online search query programs to seek information regarding specific knowledge of each cardiovascular intervention rather than form an evaluation of the intervention. Additionally, these online patient educational resources continue to not meet grade-level reading recommendations."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.17161/kjm.vol16.20554",
                "pmid": "38298385",
                "pmc": "PMC10829858",
                "pub_year": "2023",
                "url": "https://pubmed.ncbi.nlm.nih.gov/38298385/"
            }
        },
        {
            "title": "Artificial Intelligence as a Triage Tool during the Perioperative Period: Pilot Study of Accuracy and Accessibility for Clinical Application",
            "abstract": {
                "Background:": "Given the dialogistic properties of ChatGPT, we hypothesized that this artificial intelligence (AI) function can be used as a self-service tool where clinical questions can be directly answered by AI. Our objective was to assess the content, accuracy, and accessibility of AI-generated content regarding common perioperative questions for reduction mammaplasty.",
                "Methods:": "ChatGPT (OpenAI, February Version, San Francisco, Calif.) was used to query 20 common patient concerns that arise in the perioperative period of a reduction mammaplasty. Searches were performed in duplicate for both a general term and a specific clinical question. Query outputs were analyzed both objectively and subjectively. Descriptive statistics, t tests, and chi-square tests were performed where appropriate with a predetermined level of significance of P less than 0.05.",
                "Results:": "From a total of 40 AI-generated outputs, mean word length was 191.8 words. Readability was at the thirteenth grade level. Regarding content, of all query outputs, 97.5% were on the appropriate topic. Medical advice was deemed to be reasonable in 100% of cases. General queries more frequently reported overarching background information, whereas specific queries more frequently reported prescriptive information (P < 0.0001). AI outputs specifically recommended following surgeon provided postoperative instructions in 82.5% of instances.",
                "Conclusions:": "Currently available AI tools, in their nascent form, can provide recommendations for common perioperative questions and concerns for reduction mammaplasty. With further calibration, AI interfaces may serve as a tool for fielding patient queries in the future; however, patients must always retain the ability to bypass technology and be able to contact their surgeon."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1097/GOX.0000000000005580",
                "pmid": null,
                "pmc": "PMC10836902",
                "pub_year": "2024",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC10836902/"
            }
        },
        {
            "title": "Validation of ChatGPT 3.5 as a Tool to Optimize Readability of Patient-facing Craniofacial Education Materials",
            "abstract": {
                "Background:": "To address patient health literacy, the American Medical Association recommends that readability of patient education materials should not exceed a sixth grade reading level; the National Institutes of Health recommend no greater than an eigth-grade reading level. However, patient-facing materials in plastic surgery often remain at an above-recommended average reading level. The purpose of this study was to evaluate ChatGPT 3.5 as a tool for optimizing patient-facing craniofacial education materials.",
                "Methods:": "Eighteen patient-facing craniofacial education materials were evaluated for readability by a traditional calculator and ChatGPT 3.5. The resulting scores were compared. The original excerpts were then inputted to ChatGPT 3.5 and simplified by the artificial intelligence tool. The simplified excerpts were scored by the calculators.",
                "Results:": "The difference in scores for the original excerpts between the online calculator and ChatGPT 3.5 were not significant (P = 0.441). Additionally, the simplified excerpts\u2019 scores were significantly lower than the originals (P\u2005<\u20050.001), and the mean of the simplified excerpts was 7.78, less than the maximum recommended 8.",
                "Conclusions:": "The use of ChatGPT 3.5 for simplification and readability analysis of patient-facing craniofacial materials is efficient and may help facilitate the conveyance of important health information. ChatGPT 3.5 rendered readability scores comparable to traditional readability calculators, in addition to excerpt-specific feedback. It was also able to simplify materials to the recommended grade levels. With human oversight, we validate this tool for readability analysis and simplification."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1097/GOX.0000000000005575",
                "pmid": null,
                "pmc": "PMC10836906",
                "pub_year": "2024",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC10836906/"
            }
        },
        {
            "title": "Evaluation of information from artificial intelligence on rotator cuff repair surgery",
            "abstract": {
                "Purpose": "The purpose of this study was to analyze the quality and readability of information regarding rotator cuff repair surgery available using an online AI software.",
                "Methods": "An open AI model (ChatGPT) was used to answer 24 commonly asked questions from patients on rotator cuff repair. Questions were stratified into one of three categories based on the Rothwell classification system: fact, policy, or value. The answers for each category were evaluated for reliability, quality and readability using The Journal of the American Medical Association Benchmark criteria, DISCERN score, Flesch-Kincaid Reading Ease Score and Grade Level.",
                "Results": "The Journal of the American Medical Association Benchmark criteria score for all three categories was 0, which is the lowest score indicating no reliable resources cited. The DISCERN score was 51 for fact, 53 for policy, and 55 for value questions, all of which are considered good scores. Across question categories, the reliability portion of the DISCERN score was low, due to a lack of resources. The Flesch-Kincaid Reading Ease Score (and Flesch-Kincaid Grade Level) was 48.3 (10.3) for the fact class, 42.0 (10.9) for the policy class, and 38.4 (11.6) for the value class.",
                "Conclusion": "The quality of information provided by the open AI chat system was generally high across all question types but had significant shortcomings in reliability due to the absence of source material citations. The DISCERN scores of the AI generated responses matched or exceeded previously published results of studies evaluating the quality of online information about rotator cuff repairs. The responses were U.S. 10th grade or higher reading level which is above the AMA and NIH recommendation of 6th grade reading level for patient materials. The AI software commonly referred the user to seek advice from orthopedic surgeons to improve their chances of a successful outcome."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1016/j.jseint.2023.09.009",
                "pmid": null,
                "pmc": "PMC10837709",
                "pub_year": "2023",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC10837709/"
            }
        },
        {
            "title": "Elderly users\u2019 perceptions of signage systems from tertiary hospitals in Guangzhou",
            "abstract": {
                "title_content_0": "Wayfinding in hospitals today is a significant challenge for urban residents, especially for the elderly. This study investigated the perceptions and attitudes of the elderly toward existing hospital signage systems to identify the wayfinding needs in the healthcare environment. This study collected 762 elderly participants' perceptions and personal preferences regarding 12 features of the existing signage systems in three hospitals in the Yuexiu, Haizhu, and Liwan districts of Guangzhou using a questionnaire methodology. The study further explored the differences in perceptions and preferences for signage based on the gender, age, and educational level of the elderly participants. The findings indicate that most of the elderly participants experienced becoming lost in the hospital; they typically chose to ask others for directions first, followed by using the signage system. Most of the elderly participants had positive attitudes toward the current hospital signage system. Furthermore, they emphasized the importance of the signage system's graphics, texts, colors, and updates, which directly affects the readability and comprehensibility of signs. We found gender differences in perceptions and attitudes toward signage; male participants had more positive attitudes toward the hospital signage systems than female participants. Additionally, consistent with previous findings, the older the age of participants, the less comprehension they had regarding signage graphic symbols. We also found that the more educated elderly participants were, the more understanding of signage they had. At the same time, however, they were less satisfied, which is possibly because the more educated they were, the more aware they were of signage issues."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1016/j.heliyon.2024.e25003",
                "pmid": "38317991",
                "pmc": "PMC10840000",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/38317991/"
            }
        },
        {
            "title": "Arabic websites assessment of irritable bowel syndrome: How trustworthy are they? A cross\u2010sectional study",
            "abstract": {
                "title": "Abstract",
                "Background and Aims": "Irritable bowel syndrome (IBS) is a chronic GI disorder that affects people all over the world. Patients frequently look for information about their ailments online. Despite being widely and easily accessible, online information's quality and readability are under doubt. In this investigation, we assessed the effectiveness and usability of IBS Arabic websites found on significant search engines.",
                "Methods": "IBS\u2010related search terms in Arabic were entered into two search engines (Google and YouTube), and the first 30 websites per word from Google, and the first 20 websites per word from YouTube were assessed for eligibility. Eligible Google websites were assessed for quality and readability, while YouTube websites were assessed for quality. Quality was assessed using the DISCERN score and the JAMA Benchmark. Readability was assessed using the automatic Arabic readability index (AARI). Associations between quality, readability, types of websites (medical/nonmedical), and video length were analyzed.",
                "Results": "For Google: A total of 48 websites were evaluated, mean DISCERN score was 40.4 (SD\u2009=\u200910.28) indicating fair quality. The mean JAMA score was 1.6 (SD\u2009=\u20090.69), with the readability being worse the higher the quality. Medical websites had higher quality scores than nonmedical ones. For YouTube: A total of 34 YouTube videos were evaluated, mean DISCERN score was 34.7 (SD\u2009=\u20097.35), indicating poor quality. The mean JAMA score was 1.4 (SD\u2009=\u20090.72). Medical websites had higher quality scores than nonmedical ones. There was no association between the quality of the videos and their length.",
                "Conclusion": "The majority of websites were of low to fair quality and required a high degree of readability. As a result, we advise (1) healthcare practitioners to offer helpful websites to their patients, and (2) the development of IBS\u2010related websites under the guidance of experts, with the involvement of patients."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1002/hsr2.1819",
                "pmid": "38323123",
                "pmc": "PMC10845817",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/38323123/"
            }
        },
        {
            "title": "Automated Deep Learning Segmentation of Cardiac Inflammatory FDG PET",
            "abstract": {
                "Background:": "Fluorodeoxyglucose positron emission tomography (FDG PET) with glycolytic metabolism suppression plays a pivotal role in diagnosing cardiac sarcoidosis. Reorientation of images to match perfusion datasets is critical and myocardial segmentation enables consistent image scaling and quantification. However, both are challenging and labor intensive. We developed a 3D U-Net deep learning (DL) algorithm for automated myocardial segmentation in cardiac sarcoidosis FDG PET.",
                "Methods:": "The DL model was trained on 316 patients\u2019 FDG PET scans, and left ventricular contours derived from perfusion datasets. Qualitative analysis of clinical readability was performed to compare DL segmentation with the current automated method on a 50-patient test subset. Additionally, left ventricle displacement and angulation, as well as SUVmax sampling were compared to inter-user reproducibility results.",
                "Results:": "DL segmentation enhanced readability scores in over 90% of cases compared to the standard segmentation currently used in the software. DL segmentation performed similarly to a trained technologist, surpassing standard segmentation for left ventricle displacement and angulation, as well as correlation of SUVmax.",
                "Conclusion:": "The DL-based automated segmentation tool presents a marked improvement in the processing of cardiac sarcoidosis FDG PET, promising enhanced clinical workflow. This tool holds significant potential for accelerating clinical practice and improving consistency and quality. Further research with varied datasets is warranted to broaden its applicability."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1101/2024.01.31.24302113",
                "pmid": "38352354",
                "pmc": "PMC10863008",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/38352354/"
            }
        },
        {
            "title": "Clear as Mud: Readability Scores in Cloacal Exstrophy Literature and Its Treatment",
            "abstract": {
                "Purpose": "This study examines the readability of online medical information regarding cloacal exstrophy (CE). We hypothesize that inappropriate levels of comprehension are required in these resources, leading to poor understanding and confusion amongst caregivers.",
                "Methods": "The Google and Bing search engines were used to search the terms \u201ccloacal exstrophy\u201d and \u201ccloacal exstrophy treatment\u201d. The first 100 results for each were collected. Each webpage was analyzed for readability using four independent validated scoring systems: the Gunning-Fog index (GFI), SMOG grade (Simple Measure of Gobbledygook), Dale-Chall index (DCI), and the Flesch-Kincaid grade (FKG).",
                "Results": "Forty-seven unique webpages fit the inclusion criteria. Mean readability scores across all websites were GFI, 14.6; SMOG score, 10.8; DCI, 9.3; and FKG, 11.8, correlating to adjusted grade levels of college sophomore, 11th grade, college, and 11th grade, respectively. There were significant differences across all readability formulas. Non-profit websites were significantly less readable than institutional and commercial webpages (GFI p = 0.012, SMOG p = 0.018, DCI p = 0.021, FKG p = 0.0093).",
                "Conclusion": "Caregiver-directed health information regarding CE and its treatment available online is written at the 11th grade reading level or above. Online resources pertaining to CE must be simplified to be effective."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.2147/RRU.S430744",
                "pmid": null,
                "pmc": "PMC10871133",
                "pub_year": "2024",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC10871133/"
            }
        },
        {
            "title": "Assessment of Knowledge and Practice of Healthcare Providers in Saudi Arabia Regarding Clostridioides difficile Infection Diagnosis and Management: A Cross-Sectional Questionnaire-Based Study",
            "abstract": {
                "Introduction": "Diagnosis of Clostridioides difficile infection (CDI) depends on clinical presentation and laboratory testing. Stool diagnostic tests are essential for effective detection of toxigenic C. difficile strains. No study to date has evaluated the readability of microbiology labs in Saudi Arabia to test for CDI and evaluated the knowledge and practice of healthcare providers regarding CDI management. Therefore, this study aimed to assess the knowledge and practice of healthcare providers in Saudi Arabia regarding CDI diagnosis and treatment.",
                "Methods": "A cross-sectional, descriptive, questionnaire-based study was conducted on healthcare providers in Saudi Arabia, primarily physicians and clinical pharmacists. The questionnaire was developed based on a literature review and input from infectious diseases experts. The questionnaire was administered online. Data were analyzed using descriptive and inferential statistics.",
                "Results": "Of 183 respondents, 27.9% had adequate knowledge on CDI diagnosis and management. The majority were internal medicine specialists (37.7%) working in governmental or semi-governmental hospitals (80.9%) in central (46.6%) or southern (30.1%) regions of Saudi Arabia. Most participants assessed laxative use (86.3%) and reported positive C. difficile specimens to infection control (67.2%). However, knowledge varied, with 57.4% supporting unnecessary retesting and 53% assuming positive PCR test indicates moderate CDI probability. Factors such as specialization, hospital accreditation status, and bed capacity influenced knowledge levels (p<0.01 for all factors).",
                "Conclusion": "The study revealed a significant knowledge gap among Saudi healthcare providers regarding CDI diagnosis, management, and severity classification, highlighting the need for improved education and adherence to guidelines to improve patient outcomes and reduce recurrence risks."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.2147/IDR.S450281",
                "pmid": "38375099",
                "pmc": "PMC10875178",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/38375099/"
            }
        },
        {
            "title": "Application of generative language models to orthopaedic practice",
            "abstract": {
                "Objective": "To explore whether large language models (LLMs) Generated Pre-trained Transformer (GPT)-3 and ChatGPT can write clinical letters and predict management plans for common orthopaedic scenarios.",
                "Design": "Fifteen scenarios were generated and ChatGPT and GPT-3 prompted to write clinical letters and separately generate management plans for identical scenarios with plans removed.",
                "Main outcome measures": "Letters were assessed for readability using the Readable Tool. Accuracy of letters and management plans were assessed by three independent orthopaedic surgery clinicians.",
                "Results": "Both models generated complete letters for all scenarios after single prompting. Readability was compared using Flesch-Kincade Grade Level (ChatGPT: 8.77 (SD 0.918); GPT-3: 8.47 (SD 0.982)), Flesch Readability Ease (ChatGPT: 58.2 (SD 4.00); GPT-3: 59.3 (SD 6.98)), Simple Measure of Gobbledygook (SMOG) Index (ChatGPT: 11.6 (SD 0.755); GPT-3: 11.4 (SD 1.01)), and reach (ChatGPT: 81.2%; GPT-3: 80.3%). ChatGPT produced more accurate letters (8.7/10 (SD 0.60) vs 7.3/10 (SD 1.41), p=0.024) and management plans (7.9/10 (SD 0.63) vs 6.8/10 (SD 1.06), p<0.001) than GPT-3. However, both LLMs sometimes omitted key information or added additional guidance which was at worst inaccurate.",
                "Conclusions": "This study shows that LLMs are effective for generation of clinical letters. With little prompting, they are readable and mostly accurate. However, they are not consistent, and include inappropriate omissions or insertions. Furthermore, management plans produced by LLMs are generic but often accurate. In the future, a healthcare specific language model trained on accurate and secure data could provide an excellent tool for increasing the efficiency of clinicians through summarisation of large volumes of data into a single clinical letter."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1136/bmjopen-2023-076484",
                "pmid": "38485486",
                "pmc": "PMC10941106",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/38485486/"
            }
        },
        {
            "title": "Patient-Directed Vasectomy Information: How Readable Is It?",
            "abstract": {
                "Purpose": "To assess the quality and readability of online health information on vasectomy using validated readability and quality assessment tools.",
                "Materials and Methods": "The top 50 search results for \"vasectomy\" on Google, Bing, and Yahoo were selected. Duplicate links, advertisements, blog posts, paid webpages, and information intended for healthcare providers were excluded. Flesch Reading Ease score, Flesch\u2013Kincaid Grade level, Gunning Fog Index, and Simple Measure of Gobbledygook (SMOG) index were used to assess readability, with optimal readability level for online health information established as being at sixth grade reading level. DISCERN Instrument and JAMA Benchmark were used to assess the quality of selected webpages. Inter-assessment score correlation and results by webpage type were analyzed.",
                "Results": "We analyzed 44 webpages, including 16 academic, 5 hospital-affiliated, 6 commercial, 13 non-profit health advocacy, and 4 uncategorized sources. The average readability of the evaluated webpages was at a 10th grade reading level as measured by the Flesch Kincaid Assessment tool, and an undergraduate reading level per the SMOG and Gunning Fog indices. Non-profit health advocacy webpages had the best reading level but still was not at the recommended level of grade 6 to 7. The overall DISCERN quality of the webpages was \u201cfair\u201d, with non-profit health advocacy pages performing best.",
                "Conclusions": "The assessed webpages offer education on vasectomy in a language that is too complex for the general population to understand. Furthermore, several sources for online health information, such as non-profits, outperformed webpages by academic institutions. Increased healthcare collaboration and dedication to producing quality online patient resources is necessary to address these shortcomings and build trust among patients to increase utilization of vasectomy and decrease decisional regret."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.5534/wjmh.230033",
                "pmid": "37853530",
                "pmc": "PMC10949024",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/37853530/"
            }
        },
        {
            "title": "Readability of Patient Electronic Materials for Atopic Dermatitis in 23 Languages: Analysis and Implications for Dermatologists",
            "abstract": {
                "Introduction": "Patients search on the Internet for information about various medical procedures and conditions. The main aim of this study was to evaluate the readability of online health information related to atopic dermatitis (AD). Online resources are becoming a standard in facilitating shared decision-making processes. With a pipeline of new therapeutic options like immunomodulators, understanding of the complexity of AD by the patients is crucial.",
                "Methods": "The term \u201catopic dermatitis\u201d translated into 23 official European Union languages was searched using the Google search engine. The first 50 records in each language were evaluated for suitability. Included materials were barrier-free, focused on patient education, and were not categorized as advertisements. Article sources were classified into four categories: non-profit, online shops, pharmaceutical companies, and dermatology clinic. Readability was assessed with Lix score.",
                "Results": "A total of 615 articles in Swedish, Spanish, Slovenian, Slovak, Romanian, Portuguese, Polish, Lithuanian, Latvian, Irish, Italian, Hungarian, Greek, German, French, Finnish, Estonian, English, Dutch, Danish, Czech, Croatian, and Bulgarian were evaluated. The overall mean Lix score was 56\u2009\u00b1\u20098, which classified articles as very hard to comprehend. Significant differences in mean Lix scores were observed across all included languages (all P\u2009<\u20090.001). Articles released by non-profit organizations and pharmaceutical companies had the highest readability (P\u2009<\u20090.001). Low readability level was correlated with high article prevalence (R2\u2009=\u20090.189, P\u2009=\u20090.031).",
                "Conclusions": "Although there was an abundance of online articles related to AD, the readability of the available information was low. As online health information has become essential in making shared decisions between patients and physicians, an improvement in AD-related materials is needed."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1007/s13555-024-01115-1",
                "pmid": "38402338",
                "pmc": "PMC10965833",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/38402338/"
            }
        },
        {
            "title": "Adaptation and Validation of a Questionnaire to Measure Satisfaction With Telephone Care Among Individuals Living With Inflammatory Bowel Disease",
            "abstract": {
                "title": "Abstract",
                "Background": "Individuals with inflammatory bowel disease (IBD) require routine medical follow-up. The usage of telephone care (TC) appointments increased because of the coronavirus disease 2019 (COVID-19) pandemic. We aimed to adapt a questionnaire to evaluate satisfaction with TC use and validate it among IBD individuals.",
                "Methods": "A committee of experts adapted the Telehealth Usability Questionnaire to the TC context and validated its use in individuals with IBD. This committee included three IBD gastroenterology care providers (GCPs), two IBD-patient partners, and two healthcare researchers. The committee evaluated the content validity of the adapted items to measure TC satisfaction. A pilot study assessed the readability and usability of the questionnaire. Individuals with IBD in Saskatchewan completed an online survey with the adapted questionnaire between December 2021 and April 2022. Data were analyzed using descriptive and correlational techniques. Psychometric analyses were conducted to examine the reliability and validity of the questionnaire.",
                "Results": "The committee of experts developed the Telephone Care Satisfaction Questionnaire (TCSQ patient), with 16 items and one overall item for TC satisfaction. After the pilot, 87 IBD individuals participated in the online survey. A strong correlation was observed between the 16-item standardized level of TC satisfaction and the overall item, r = 0.85 (95%CI 0.78\u20130.90, p < 0.001). The TCSQ patient had optimal internal reliability (\u03b1 = 0.96). Two dimensions were identified in the exploratory factor analysis (i.e., TC usefulness and convenience).",
                "Conclusion": "The TCSQ patient is a valid and reliable measure of TC satisfaction among individuals with IBD. This questionnaire demonstrated excellent psychometric properties and we recommend its use."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1093/jcag/gwad053",
                "pmid": "38596801",
                "pmc": "PMC10999769",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/38596801/"
            }
        },
        {
            "title": "Hidradenitis Suppurativa Online Documents Readability: An Analysis Including 23 European Languages",
            "abstract": {
                "Purpose": "Hidradenitis suppurativa (HS) is a complex disease with the vast burden to patients. The aim of the study was to evaluate readability of online electronic materials dedicated to HS.",
                "Patients and Methods": "The terms \u201chidradenitis suppurativa\u201d and \u201cacne inversa\u201d translated into 23 official European Union languages were searched with Google. For each language, first 50 results were assessed for suitability. Included materials were focused on patient\u2019s education, had no barriers and were not advertisements. If both terms generated the same results, duplicated materials were excluded from the analysis. Origin of the article was categorized into non-profit, online-shop, dermatology clinic or pharmaceutical company class. Readability was evaluated with Lix score.",
                "Results": "A total of 458 articles in 22 languages were evaluated. The overall mean Lix score was 57 \u00b1 9. This classified included articles as very hard to comprehend. Across all included languages significant differences in Lix score were revealed (P < 0.001). No significant differences across all origin categories and Lix scores were observed (all P > 0.05).",
                "Conclusion": "Despite the coverage of HS on the Internet, its complexity made it hard to comprehend. Dermatologist should ensure readable, barrier-free online educational materials. With adequate Google promotion, these would be beneficial for both physicians and patients."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.2147/CCID.S463861",
                "pmid": null,
                "pmc": "PMC11032155",
                "pub_year": "2024",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11032155/"
            }
        },
        {
            "title": "Organ Donation in Canadian PICUs: A Cross-Sectional Survey, 2021\u20132022*",
            "abstract": {
                "OBJECTIVES:": "To understand contemporary pediatric organ donation programs in Canadian PICUs, including: policies and practices, data collection and reporting, and system and process barriers.",
                "DESIGN:": "A cross-sectional survey carried out 2021\u20132022.",
                "SETTING:": "Canadian PICUs affiliated with a donor physician network.",
                "SUBJECTS:": "Pediatric intensivists identified as the donation program lead, or most knowledgeable about donation for their institution.",
                "MEASUREMENTS AND MAIN RESULTS:": "A 19-item survey was developed through collaboration with stakeholders from the organ donation and transplantation community within Canada. Domains and items were generated and reduced iteratively during an in-person workshop. Pretesting and pilot testing were completed to ensure readability, flow, clinical sensibility, and construct validity. Fifteen of 16 (94%) invited Canadian PICUs from seven provinces completed the survey representing 88% (15/18) of all noncardiac Canadian PICUs. Surveys were completed between June 2021 and September 2022. All units support donation after death by neurologic criteria (DNC); 14 of 15 indicated donation policies were in place and 1 of 15 indicated no policy but the ability to facilitate donation. Thirteen of 15 units (87%) support donation after death by circulatory criteria (DCC) with policies in place, with 11 of 13 of these indicating routine support of donation opportunities. The majority (13/15) of units identified a donation champion. Of the 16 identified champions across these centers, 13 were physicians and were registered nurses or nurse practitioners. Eight of 13 units (62%) with donation champions had positions supported financially, of which 5 units came from the Organ Donation Organization and the other 3 came from the provincial health authority. Finally, only 3 of 15 PICU donation programs have a pediatric donation committee with family involvement. Variability exists in identification (including determination of death practices), referral, and approach for donation between units.",
                "CONCLUSIONS:": "Although all Canadian PICUs support donation after DNC donation, and most support donation after DCC, variability exists in the identification, referral, and approach of potential donors. There is a notable lack of family involvement in pediatric donation programs. There are many opportunities for standardization of PICU donation programs which may result in improved rates of pediatric organ donation in Canada."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1097/PCC.0000000000003404",
                "pmid": "37966310",
                "pmc": "PMC11060061",
                "pub_year": "2023",
                "url": "https://pubmed.ncbi.nlm.nih.gov/37966310/"
            }
        },
        {
            "title": "Use and Utility of Patient After\u2010Visit Instructions at a University Rheumatology Outpatient Clinic: Status and Randomized Prospective Pilot Intervention Study",
            "abstract": {
                "Objective": "The aim of this study was to evaluate the use of after\u2010visit instructions (AVIs) in an academic rheumatology clinic and assess the impact of standardized AVIs (sAVIs) and teach\u2010back (TB) on comprehension of health information.",
                "Methods": "A retrospective review of adult patients seen between October 1 and 8, 2021, at the rheumatology clinic collected data on patient demographics, clinical features, and the presence, content, and readability of AVIs. During a subsequent prospective proof\u2010of\u2010concept study, routinely scheduled patients seen at the rheumatology clinic were randomized into three groups: control (received standard of care), received sAVIs only, and received sAVIs plus TB. Patients completed a health literacy questionnaire, satisfaction survey, and a one\u2010 to two\u2010week postvisit telephone survey to assess AVI comprehension.",
                "Results": "Out of 316 retrospective patient visits, 82 (25.9%) received AVIs. Among 210 of 316 patients (66.5%) with management changes, 76 (36.1%) received AVI, with 74.2% of the instructions considered concordant with the provider's note. Use of AVIs was higher with management changes, new patient visits, and medical trainee/teaching clinics. AVIs were written at a median 6.8 grade level. A total of 75 patients completed the prospective study: 31 (41.3%) were in the control group, 19 (25.3%) were in the group that received sAVIs only, and 25 (33.3%) were in the group that received AVIs with TB. There were no differences in overall postvisit survey comprehension/retention scores among the three patient groups evaluated.",
                "Conclusion": "Although a lack of AVI use was identified, implementation of sAVIs did not appear to impact patient retention or comprehension of discharge health information."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1002/acr2.11659",
                "pmid": "38387613",
                "pmc": "PMC11089441",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/38387613/"
            }
        },
        {
            "title": "Performance of an online chat-based artificial intelligence interface for patient education on atrial fibrillation ablation",
            "abstract": {
                "title": "Abstract",
                "Background": "Chat-based artificial intelligence (AI) web interfaces that aim to mimic human conversation have increasing utilization in healthcare to help with simple tasks such as scheduling appointments, and even more complex tasks such as providing patient educational responses to COVID-19 questions as done by the World Health Organization.1 Chat-based AI has also been shown to provide accurate responses to cardiovascular disease prevention questions.2 Its ability to provide patient education for more complex treatments like atrial fibrillation (AF) ablation has not been explored.",
                "Purpose": "To evaluate the quality of a popular chat-based AI program\u2019s answers to patient questions about AF ablation.",
                "Methods": "Twenty commonly asked questions (\"prompts\") regarding AF ablation were entered into ChatGPT (Chat Generative Pre-trained Transformer), a large language model-based AI program (Fig. 1). Prompts were written in plain language; technical terms were avoided except for \"radiofrequency\", \"cryoablation\" and \"pulsed field ablation\" (PFA). SMOG readability calculator was used to assess responses for difficulty and grade-level, as healthcare organizations recommend \u2264 8th-grade level complexity for patient information. Response content was graded by 3 experienced cardiac electrophysiologists as \"reasonable\", \"missing important elements/some inaccuracies\" or \"misleading/inappropriate\". Responses are presented in mean +/- standard deviation and percentages.",
                "Results": "Responses averaged 118\u00b167 words (Fig. 1). Of 20 responses, 17 (85%) were deemed reasonable, 3 (15%) missing important elements/some inaccuracies and none inappropriate or misleading; 16 (80%) emphasized discussion of issues with the healthcare team (Fig. 2). Responses missing important elements/some inaccuracies were those about risks/complications of ablation [missing phrenic nerve palsy, atrioesophageal fistula (AEF), potential need for emergent cardiac surgery or pacemaker, death], concerning symptoms post-procedure (missing symptoms of hematoma, AEF, stroke), and that PFA is not yet approved for use in all regions. Average reading grade level of responses was 13.8 (college level or \"professional\"): 17 (85%) responses were 12th grade level, 11 (55%) were college-level or higher, and 6 (30%) were college-graduate level (\"extremely difficult\"). None were \u2264 8th grade level (Fig. 2).",
                "Conclusions": "A majority of ChatGPT responses to common patient questions about AF ablation had reasonable content quality that frequently emphasized the importance of discussion with the healthcare team. However, responses to more difficult questions regarding risks, symptoms of potential complications, or newer technology missed important details; more than half of responses required college-level reading skills. While use of Chat-AI for patient education on EP topics appears promising, patients should be advised to use caution. Further AI training to improve content and readability should be explored."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1093/europace/euae102.565",
                "pmid": null,
                "pmc": "PMC11118149",
                "pub_year": "2024",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11118149/"
            }
        },
        {
            "title": "Assessing the readability, reliability, and quality of artificial intelligence chatbot responses to the 100 most searched queries about cardiopulmonary resuscitation: An observational study",
            "abstract": {
                "title_content_0": "This study aimed to evaluate the readability, reliability, and quality of responses by 4 selected artificial intelligence (AI)-based large language model (LLM) chatbots to questions related to cardiopulmonary resuscitation (CPR). This was a cross-sectional study. Responses to the 100 most frequently asked questions about CPR by 4 selected chatbots (ChatGPT-3.5 [Open AI], Google Bard [Google AI], Google Gemini [Google AI], and Perplexity [Perplexity AI]) were analyzed for readability, reliability, and quality. The chatbots were asked the following question: \u201cWhat are the 100 most frequently asked questions about cardio pulmonary resuscitation?\u201d in English. Each of the 100 queries derived from the responses was individually posed to the 4 chatbots. The 400 responses or patient education materials (PEM) from the chatbots were assessed for quality and reliability using the modified DISCERN Questionnaire, Journal of the American Medical Association and Global Quality Score. Readability assessment utilized 2 different calculators, which computed readability scores independently using metrics such as Flesch Reading Ease Score, Flesch-Kincaid Grade Level, Simple Measure of Gobbledygook, Gunning Fog Readability and Automated Readability Index. Analyzed 100 responses from each of the 4 chatbots. When the readability values of the median results obtained from Calculators 1 and 2 were compared with the 6th-grade reading level, there was a highly significant difference between the groups (P\u2005<\u2005.001). Compared to all formulas, the readability level of the responses was above 6th grade. It can be seen that the order of readability from easy to difficult is Bard, Perplexity, Gemini, and ChatGPT-3.5. The readability of the text content provided by all 4 chatbots was found to be above the 6th-grade level. We believe that enhancing the quality, reliability, and readability of PEMs will lead to easier understanding by readers and more accurate performance of CPR. So, patients who receive bystander CPR may experience an increased likelihood of survival."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1097/MD.0000000000038352",
                "pmid": null,
                "pmc": "PMC11142831",
                "pub_year": "2024",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11142831/"
            }
        },
        {
            "title": "Quality of Popular Online Resources About Vestibular Migraine",
            "abstract": {
                "title": "Abstract",
                "Objective": "To evaluate the readability, understandability, actionability, and accuracy of online resources covering vestibular migraine (VM).",
                "Study Design": "Cross\u2010sectional descriptive study design.",
                "Setting": "Digital collection of websites appearing on Google search.",
                "Methods": "Google searches were conducted to identify common online resources for VM. We examined readability using the Flesch Reading Ease\u00a0(FRE) and Flesch\u2010Kincaid Grade Level scores, understandability and actionability using the Patient Education Materials Assessment Tool (PEMAT), and accuracy by comparing the website contents to the consensus definition of \u201cprobable vestibular migraine.\u201d",
                "Results": "Eleven of the most popular websites were analyzed. Flesch\u2010Kincaid Grade Level averaged at a 13th\u2010grade\u00a0level (range: 9th\u201018th). FRE scores averaged 35.5 (range: 9.1\u201054.4). No website had a readability grade level at the US\u00a0Agency for Healthcare Research and Quality recommended 5th\u2010grade level or an equivalent FRE score of 90 or greater. Understandability scores varied ranging from 49% to 88% (mean 70%). Actionability scores varied more, ranging from 12% to 87% (mean 44%). There was substantial inter\u2010rater agreement for both PEMAT understandability scoring (mean \u03ba\u2009=\u20090.76, SD\u2009=\u20090.08) and actionability scoring (mean \u03ba\u2009=\u20090.65, SD\u2009=\u20090.08). Three sites included all 3 \u201cprobable vestibular migraine\u201d diagnostic criteria as worded in the consensus statement.",
                "Conclusion": "The quality of online resources for VM is poor overall in terms of readability, actionability, and agreement with diagnostic criteria."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1002/oto2.137",
                "pmid": "39015736",
                "pmc": "PMC11250137",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/39015736/"
            }
        },
        {
            "title": "Evaluating Readability, Understandability, and Actionability of Online Printable Patient Education Materials for Cholesterol Management: A Systematic Review",
            "abstract": {
                "Background": "Dyslipidemia management is a cornerstone in cardiovascular disease prevention and relies heavily on patient adherence to lifestyle modifications and medications. Numerous cholesterol patient education materials are available online, but it remains unclear whether these resources are suitable for the majority of North American adults given the prevalence of low health literacy. This review aimed to (1) identify printable cholesterol patient education materials through an online search, and (2) evaluate the readability, understandability, and actionability of each resource to determine its utility in practice.",
                "Methods and Results": "We searched the MEDLINE database for peer\u2010reviewed educational materials and the websites of Canadian and American national health organizations for gray literature. Readability was measured using the Flesch\u2013Kincaid Grade Level, and scores between fifth\u2010 and sixth\u2010grade reading levels were considered adequate. Understandability and actionability were scored using the Patient Education Materials Assessment Tool and categorized as superior (>80%), adequate (50%\u201370%), or inadequate (<50%). Our search yielded 91 results that were screened for eligibility. Among the 22 educational materials included in the study, 15 were identified through MEDLINE, and 7 were from websites. The readability across all materials averaged an 11th\u2010grade reading level (Flesch\u2013Kincaid Grade Level=11.9\u00b12.59). The mean\u00b1SD understandability and actionability scores were 82.8\u00b16.58% and 40.9\u00b128.60%, respectively.",
                "Conclusions": "The readability of online cholesterol patient education materials consistently exceeds the health literacy level of the average North American adult. Many resources also inadequately describe action items for individuals to self\u2010manage their cholesterol, representing an implementation gap in cardiovascular disease prevention."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1161/JAHA.123.030140",
                "pmid": "38567668",
                "pmc": "PMC11262522",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/38567668/"
            }
        },
        {
            "title": "Is Arabic online patient-centered information about dental extraction trustworthy? An infodemiological study",
            "abstract": {
                "Background": "Assessment of the Arabic online patient-centered information is understudied. The study aims to assess the quality and readability of the Arabic web-based knowledge about dental extraction.",
                "Methods": "The first 100 Arabic websites focusing on dental extraction were gathered using popular terms from Google, Bing, and Yahoo searches. These sites were organized and their quality was assessed using three key standards: the Journal of the American Medical Association (JAMA) benchmark criteria, the DISCERN instrument, and the inclusion of the Health on the Net Foundation Code of Conduct (HON code) seal. Additionally, the ease of reading of these websites was evaluated through various online readability indexes.",
                "Results": "Out of 300 initially reviewed websites on dental extraction in Arabic, 80 met the eligibility criteria. Nonprofit organizations were most common (41.3%), followed by university/medical centers (36.3%), and commercial entities (21.3%). Government organizations were minimally represented (1.3%). All websites were medically oriented, with 60% offering Q&A sections. Quality assessment showed moderate scores on the DISCERN instrument, with no site reaching the highest score. JAMA benchmarks were poorly met, and none had the HON code seal. Readability was generally high, with most sites scoring favorably on readability scales.",
                "Conclusions": "The rapidly evolving online information about dental extraction lacks readability and quality and can spread misinformation. Creators should focus on clear, unbiased content using simple language for better public understanding."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1177/20552076241264390",
                "pmid": "39055782",
                "pmc": "PMC11271091",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/39055782/"
            }
        },
        {
            "title": "Best practices on patient education materials in hip surgery based on learnings from major hip centers and societies",
            "abstract": {
                "title": "ABSTRACT",
                "title_content_0": "Patient education is important as it gives patients a better understanding of the risks and benefits of medical and surgical interventions. Developing communication technologies have completely changed and enhanced patient access to medical information. The aim of this study was to evaluate available patient education materials (PEMs) regarding hip surgery on the websites of major hip societies and centers. The PEM from 11 selected leading hip centers and societies were evaluated with the following assessment tools: Flesch\u2013Kincaid (FK) readability test, Flesch Reading Ease formula, Literature-Intelligence-Data-Analysis (LIDA) instrument and Discernibility Interpretability Sources Comprehensive Evidence Relevance Noticeable (DISCERN) tool. Videos were assessed using Patient Educational Video Assessment Tool (PEVAT). A total of 69 educational items, including 52 text articles (75.4%) and 17 videos (24.6%) were retrieved and evaluated. The median Interquartile Range (IQR) FK level of 52 text articles was 10.8 (2.2). The median (IQR) LIDA score of text articles by center was 45. According to the LIDA score, 60% of all website articles demonstrated high accessibility (LIDA score\u2009>\u200944). The median DISCERN score of text articles by center was 69. Overall, 52 (100%) of the text articles were deemed to be at \u2018good\u2019 quality rating or higher, and 23.2% (16 out of 69) of the articles had excellent quality. The mean PEVAT score for the 17 videos was 25\u2009\u00b1\u20091.9. Analysis of text and video articles from the 11 leading orthopedic surgery centers and societies demonstrated that by selecting a reliable source of information from main scientific societies and major centers in hip surgery, patients can find more accurate information regarding their hip conditions."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1093/jhps/hnae011",
                "pmid": null,
                "pmc": "PMC11272639",
                "pub_year": "2024",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11272639/"
            }
        },
        {
            "title": "Assessment of readability, reliability, and quality of ChatGPT\u00ae, BARD\u00ae, Gemini\u00ae, Copilot\u00ae, Perplexity\u00ae responses on palliative care",
            "abstract": {
                "title_content_0": "There is no study that comprehensively evaluates data on the readability and quality of \u201cpalliative care\u201d information provided by artificial intelligence (AI) chatbots ChatGPT\u00ae, Bard\u00ae, Gemini\u00ae, Copilot\u00ae, Perplexity\u00ae. Our study is an observational and cross-sectional original research study. In our study, AI chatbots ChatGPT\u00ae, Bard\u00ae, Gemini\u00ae, Copilot\u00ae, and Perplexity\u00ae were asked to present the answers of the 100 questions most frequently asked by patients about palliative care. Responses from each 5 AI chatbots were analyzed separately. This study did not involve any human participants. Study results revealed significant differences between the readability assessments of responses from all 5 AI chatbots (P\u2005<\u2005.05). According to the results of our study, when different readability indexes were evaluated holistically, the readability of AI chatbot responses was evaluated as Bard\u00ae, Copilot\u00ae, Perplexity\u00ae, ChatGPT\u00ae, Gemini\u00ae, from easy to difficult (P\u2005<\u2005.05). In our study, the median readability indexes of each of the 5 AI chatbots Bard\u00ae, Copilot\u00ae, Perplexity\u00ae, ChatGPT\u00ae, Gemini\u00ae responses were compared to the \u201crecommended\u201d 6th grade reading level. According to the results of our study answers of all 5 AI chatbots were compared with the 6th grade reading level, statistically significant differences were observed in the all formulas (P\u2005<\u2005.001). The answers of all 5 artificial intelligence robots were determined to be at an educational level well above the 6th grade level. The modified DISCERN and Journal of American Medical Association scores was found to be the highest in Perplexity\u00ae (P\u2005<\u2005.001). Gemini\u00ae responses were found to have the highest Global Quality Scale score (P\u2005<\u2005.001). It is emphasized that patient education materials should have a readability level of 6th grade level. Of the 5 AI chatbots whose answers about palliative care were evaluated, Bard\u00ae, Copilot\u00ae, Perplexity\u00ae, ChatGPT\u00ae, Gemini\u00ae, their current answers were found to be well above the recommended levels in terms of readability of text content. Text content quality assessment scores are also low. Both the quality and readability of texts should be brought to appropriate recommended limits."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1097/MD.0000000000039305",
                "pmid": "39151545",
                "pmc": "PMC11332738",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/39151545/"
            }
        },
        {
            "title": "Disambiguation of acronyms in clinical narratives with large language models",
            "abstract": {
                "title": "Abstract",
                "Objective": "To assess the performance of large language models (LLMs) for zero-shot disambiguation of acronyms in clinical narratives.",
                "Materials and Methods": "Clinical narratives in English, German, and Portuguese were applied for testing the performance of four LLMs: GPT-3.5, GPT-4, Llama-2-7b-chat, and Llama-2-70b-chat. For English, the anonymized Clinical Abbreviation Sense Inventory (CASI, University of Minnesota) was used. For German and Portuguese, at least 500 text spans were processed. The output of LLM models, prompted with contextual information, was analyzed to compare their acronym disambiguation capability, grouped by document-level metadata, the source language, and the LLM.",
                "Results": "On CASI, GPT-3.5 achieved 0.91 in accuracy. GPT-4 outperformed GPT-3.5 across all datasets, reaching 0.98 in accuracy for CASI, 0.86 and 0.65 for two German datasets, and 0.88 for Portuguese. Llama models only reached 0.73 for CASI and failed severely for German and Portuguese. Across LLMs, performance decreased from English to German and Portuguese processing languages. There was no evidence that additional document-level metadata had a significant effect.",
                "Conclusion": "For English clinical narratives, acronym resolution by GPT-4 can be recommended to improve readability of clinical text by patients and professionals. For German and Portuguese, better models are needed. Llama models, which are particularly interesting for processing sensitive content on premise, cannot yet be recommended for acronym resolution."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1093/jamia/ocae157",
                "pmid": "38917444",
                "pmc": "PMC11339513",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/38917444/"
            }
        },
        {
            "title": "Performance of large language models (LLMs) in providing prostate cancer information",
            "abstract": {
                "Purpose": "The diagnosis and management of prostate cancer (PCa), the second most common cancer in men worldwide, are highly complex. Hence, patients often seek knowledge through additional resources, including AI chatbots such as ChatGPT and Google Bard. This study aimed to evaluate the performance of LLMs in providing education on PCa.",
                "Methods": "Common patient questions about PCa were collected from reliable educational websites and evaluated for accuracy, comprehensiveness, readability, and stability by two independent board-certified urologists, with a third resolving discrepancy. Accuracy was measured on a 3-point scale, comprehensiveness was measured on a 5-point Likert scale, and readability was measured using the Flesch Reading Ease (FRE) score and Flesch\u2013Kincaid FK Grade Level.",
                "Results": "A total of 52 questions on general knowledge, diagnosis, treatment, and prevention of PCa were provided to three LLMs. Although there was no significant difference in the overall accuracy of LLMs, ChatGPT-3.5 demonstrated superiority over the other LLMs in terms of general knowledge of PCa (p\u2009=\u20090.018). ChatGPT-4 achieved greater overall comprehensiveness than ChatGPT-3.5 and Bard (p\u2009=\u20090.028). For readability, Bard generated simpler sentences with the highest FRE score (54.7, p\u2009<\u20090.001) and lowest FK reading level (10.2, p\u2009<\u20090.001).",
                "Conclusion": "ChatGPT-3.5, ChatGPT-4 and Bard generate accurate, comprehensive, and easily readable PCa material. These AI models might not replace healthcare professionals but can assist in patient education and guidance.",
                "Supplementary Information": "The online version contains supplementary material available at 10.1186/s12894-024-01570-0."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1186/s12894-024-01570-0",
                "pmid": null,
                "pmc": "PMC11342655",
                "pub_year": "2024",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11342655/"
            }
        },
        {
            "title": "Practices and Barriers in Developing and Disseminating Plain-Language Resources Reporting Medical Research Information: A Scoping Review",
            "abstract": {
                "Background": "The intent of plain-language resources (PLRs) reporting medical research information is to advance health literacy among the general public and enable them to participate in shared decision-making (SDM). Regulatory mandates coupled with academic and industry initiatives have given rise to an increasing volume of PLRs summarizing medical research information. However, there is significant variability in the quality, format, readability, and dissemination channels for PLRs. In this scoping review, we identify current practices, guidance, and barriers in developing and disseminating PLRs reporting medical research information to the general public including patients and caregivers. We also report on the PLR preferences of these intended audiences.",
                "Methods": "A literature search of three bibliographic databases (PubMed, EMBASE, Web of Science) and three clinical trial registries (NIH, EMA, ISRCTN registry) was performed. Snowball searches within reference lists of primary articles were added. Articles with PLRs or reporting topics related to PLRs use and development available between January 2017 and June 2023 were identified. Evidence mapping and synthesis were used to make qualitative observations. Identified PLRs were quantitatively assessed, including temporal annual trends, availability by field of medicine, language, and publisher types.",
                "Results": "A total of 9116 PLRs were identified, 9041 from the databases and 75 from clinical trial registries. The final analysis included 6590 PLRs from databases and 72 from registries. Reported barriers to PLR development included ambiguity in guidance, lack of incentives, and concerns of researchers writing for the general public. Available guidance recommendations called for greater dissemination, increased readability, and varied content formats. Patients preferred visual PLRs formats (e.g., videos, comics), which were easy to access on the internet and used short jargon-free text. In some instances, older audiences and more educated readers preferred text-only PLRs. Preferences among the general public were mostly similar to those of patients. Psychology, followed by oncology, showed the highest number of PLRs, predominantly from academia-sponsored research. Text-only PLRs were most commonly available, while graphical, digital, or online formats were less available. Preferred dissemination channels included paywall-free journal websites, indexing on PubMed, third-party websites, via email to research participants, and social media.",
                "Conclusions": "This scoping review maps current practices, recommendations, and patients\u2019 and the general public\u2019s preferences for PLR development and dissemination. The results suggest that making PLRs available to a wider audience by improving nomenclature, accessibility, and providing translations may contribute to empowerment and SDM. Minimizing variability among available guidance for PLR development may play an important role in amplifying the value and impact of these resources.",
                "Supplementary Information": "The online version contains supplementary material available at 10.1007/s40271-024-00700-y."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1007/s40271-024-00700-y",
                "pmid": "38878237",
                "pmc": "PMC11343906",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/38878237/"
            }
        },
        {
            "title": "An assessment of ChatGPT\u2019s responses to frequently asked questions about cervical and breast cancer",
            "abstract": {
                "Background": "Cervical cancer (CC) and breast cancer (BC) threaten women\u2019s well-being, influenced by health-related stigma and a lack of reliable information, which can cause late diagnosis and early death. ChatGPT is likely to become a key source of health information, although quality concerns could also influence health-seeking behaviours.",
                "Methods": "This cross-sectional online survey compared ChatGPT\u2019s responses to five physicians specializing in mammography and five specializing in gynaecology. Twenty frequently asked questions about CC and BC were asked on 26th and 29th of April, 2023. A panel of seven experts assessed the accuracy, consistency, and relevance of ChatGPT\u2019s responses using a 7-point Likert scale. Responses were analyzed for readability, reliability, and efficiency. ChatGPT\u2019s responses were synthesized, and findings are presented as a radar chart.",
                "Results": "ChatGPT had an accuracy score of 7.0 (range: 6.6-7.0) for CC and BC questions, surpassing the highest-scoring physicians (P\u2009<\u20090.05). ChatGPT took an average of 13.6\u00a0s (range: 7.6-24.0) to answer each of the 20 questions presented. Readability was comparable to that of experts and physicians involved, but ChatGPT generated more extended responses compared to physicians. The consistency of repeated answers was 5.2 (range: 3.4-6.7). With different contexts combined, the overall ChatGPT relevance score was 6.5 (range: 4.8-7.0). Radar plot analysis indicated comparably good accuracy, efficiency, and to a certain extent, relevance. However, there were apparent inconsistencies, and the reliability and readability be considered inadequate.",
                "Conclusions": "ChatGPT shows promise as an initial source of information for CC and BC. ChatGPT is also highly functional and appears to be superior to physicians, and aligns with expert consensus, although there is room for improvement in readability, reliability, and consistency. Future efforts should focus on developing advanced ChatGPT models explicitly designed to improve medical practice and for those with concerns about symptoms.",
                "Supplementary Information": "The online version contains supplementary material available at 10.1186/s12905-024-03320-8."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1186/s12905-024-03320-8",
                "pmid": null,
                "pmc": "PMC11367894",
                "pub_year": "2024",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11367894/"
            }
        },
        {
            "title": "Evaluating the Accuracy, Quality, and Readability of Online Breast Cancer Information",
            "abstract": {
                "Objective:": "To assess the accuracy, quality, and readability of patient-focused breast cancer websites using expert evaluation and validated tools.",
                "Background:": "Ensuring access to accurate, high-quality, and readable online health information supports informed decision-making and health equity but has not been recently evaluated.",
                "Methods:": "A qualitative analysis on 50 websites was conducted; the first 10 eligible websites for the following search terms were included: \u201cbreast cancer,\u201d \u201cbreast surgery,\u201d \u201cbreast reconstructive surgery,\u201d \u201cbreast chemotherapy,\u201d and \u201cbreast radiation therapy.\u201d Websites were required to be in English and not intended for healthcare professionals. Accuracy was evaluated by 5 breast cancer specialists. Quality was evaluated through the DISCERN questionnaire. Readability was measured using 9 standardized tests. Mean readability was compared with the American Medical Association and National Institutes of Health 6th grade recommendation.",
                "Results:": "Nonprofit hospital websites had the highest accuracy (mean = 4.06, SD = 0.42); however, no statistical differences were observed in accuracy by website affiliation (P = 0.08). The overall mean quality score was 50.8 (\u201cfair\u201d/\u201cgood\u201d quality) with no significant differences among website affiliations (P = 0.10). Mean readability was at the 10th grade reading level, the lowest being for commercial websites with a mean 9th grade reading level (SD = 2.38). All websites exceeded the American Medical Association- and National Institutes of Health-recommended reading level by 4.4 levels (P < 0.001). Websites with higher accuracy tended to have lower readability levels, whereas those with lower accuracy had higher readability levels.",
                "Conclusion:": "As breast cancer treatment has become increasingly complex, improving online quality and readability while maintaining high accuracy is essential to promote health equity and empower patients to make informed decisions about their care."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1097/AS9.0000000000000465",
                "pmid": null,
                "pmc": "PMC11415127",
                "pub_year": "2024",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11415127/"
            }
        },
        {
            "title": "Assessing the reliability and readability of migraine treatment information on Turkish websites",
            "abstract": {
                "Background": "The quality and safety of information provided on online platforms for migraine treatment remains uncertain. We evaluated the top 10 trending websites accessed annually by Turkish patients seeking solutions for migraine treatment and assessed information quality, security, and readability in this cross-sectional study.",
                "Methods": "A comprehensive search strategy was conducted using Google starting in 2015, considering T\u00fcrkiye\u2019s internet usage trends. Websites were evaluated using the DISCERN measurement tool and Ate\u015fman Turkish readability index.",
                "Results": "Ninety websites were evaluated between 2015 and 2024. According to the DISCERN measurement tool, most websites exhibited low quality and security levels. Readability analysis showed that half of the websites were understandable by readers with 9th\u2009\u2212\u200910th grade educational levels. The author distribution varied, with neurologists being the most common. A significant proportion of the websites were for profit. Treatment of attacks and preventive measures were frequently mentioned, but some important treatments, such as greater occipital nerve blockade, were rarely discussed.",
                "Conclusion": "This study highlights the low quality and reliability of online information websites on migraine treatment in T\u00fcrkiye. These websites\u2019 readability level remains a concern, potentially hindering patients\u2019 access to accurate information. This can be a barrier to migraine care for both patients with migraine and the physician. Better supervision and cooperation with reputable medical associations are needed to ensure the dissemination of reliable information to the public.",
                "Supplementary Information": "The online version contains supplementary material available at 10.1186/s12913-024-11599-4."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1186/s12913-024-11599-4",
                "pmid": null,
                "pmc": "PMC11430538",
                "pub_year": "2024",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11430538/"
            }
        },
        {
            "title": "Evaluation of super resolution technology for digestive endoscopic images",
            "abstract": {
                "Object": "This study aims to evaluate the value of super resolution (SR) technology in augmenting the quality of digestive endoscopic images.",
                "Methods": "In the retrospective study, we employed two advanced SR models, i.e., SwimIR and ESRGAN. Two discrete datasets were utilized, with training conducted using the dataset of the First Affiliated Hospital of Soochow University (12,212 high-resolution images) and evaluation conducted using the HyperKvasir dataset (2,566 low-resolution images). Furthermore, an assessment of the impact of enhanced low-resolution images was conducted using a 5-point Likert scale from the perspectives of endoscopists. Finally, two endoscopic image classification tasks were employed to evaluate the effect of SR technology on computer vision (CV).",
                "Results": "SwinIR demonstrated superior performance, which achieved a PSNR of 32.60, an SSIM of 0.90, and a VIF of 0.47 in test set. 90\u00a0% of endoscopists supported that SR preprocessing moderately ameliorated the readability of endoscopic images. For CV, enhanced images bolstered the performance of convolutional neural networks, whether in the classification task of Barrett's esophagus (improved F1-score: 0.04) or Mayo Endoscopy Score (improved F1-score: 0.04).",
                "Conclusions": "SR technology demonstrates the capacity to produce high-resolution endoscopic images. The approach enhanced clinical readability and CV models\u2019 performance of low-resolution endoscopic images."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1016/j.heliyon.2024.e38920",
                "pmid": null,
                "pmc": "PMC11489312",
                "pub_year": "2024",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11489312/"
            }
        },
        {
            "title": "Evaluation of the Appropriateness and Readability of ChatGPT-4 Responses to Patient Queries on Uveitis",
            "abstract": {
                "Purpose": "To compare the utility of ChatGPT-4 as an online uveitis patient education resource with existing patient education websites.",
                "Design": "Evaluation of technology.",
                "Participants": "Not applicable.",
                "Methods": "The term \u201cuveitis\u201d was entered into the Google search engine, and the first 8 nonsponsored websites were selected to be enrolled in the study. Information regarding uveitis for patients was extracted from Healthline, Mayo Clinic, WebMD, National Eye Institute, Ocular Uveitis and Immunology Foundation, American Academy of Ophthalmology, Cleveland Clinic, and National Health Service websites. ChatGPT-4 was then prompted to generate responses about uveitis in both standard and simplified formats. To generate the simplified response, the following request was added to the prompt: 'Please provide a response suitable for the average American adult, at a sixth-grade comprehension level.\u2019 Three dual fellowship-trained specialists, all masked to the sources, graded the appropriateness of the contents (extracted from the existing websites) and responses (generated responses by ChatGPT-4) in terms of personal preference, comprehensiveness, and accuracy. Additionally, 5 readability indices, including Flesch Reading Ease, Flesch\u2013Kincaid Grade Level, Gunning Fog Index, Coleman\u2013Liau Index, and Simple Measure of Gobbledygook index were calculated using an online calculator, Readable.com, to assess the ease of comprehension of each answer.",
                "Main Outcome Measures": "Personal preference, accuracy, comprehensiveness, and readability of contents and responses about uveitis.",
                "Results": "A total of 497 contents and responses, including 71 contents from existing websites, 213 standard responses, and 213 simplified responses from ChatGPT-4 were recorded and graded. Standard ChatGPT-4 responses were preferred and perceived to be more comprehensive by dually trained (uveitis and retina) specialist ophthalmologists while maintaining similar accuracy level compared with existing websites. Moreover, simplified ChatGPT-4 responses matched almost all existing websites in terms of personal preference, accuracy, and comprehensiveness. Notably, almost all readability indices suggested that standard ChatGPT-4 responses demand a higher educational level for comprehension, whereas simplified responses required lower level of education compared with the existing websites.",
                "Conclusions": "This study shows that ChatGPT can provide patients with an avenue to access comprehensive and accurate information about uveitis, tailored to their educational level.",
                "Financial Disclosure(s)": "The author(s) have no proprietary or commercial interest in any materials discussed in this article."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1016/j.xops.2024.100594",
                "pmid": "39435137",
                "pmc": "PMC11492124",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/39435137/"
            }
        },
        {
            "title": "Can ChatGPT affect health literacy?",
            "abstract": {
                "title": "Abstract",
                "Background": "ChatGPT is a large language model based on artificial intelligence, which nowadays finds application in various fields, including medicine.",
                "Objectives": "To investigate the usefulness of ChatGPT related to health literacy and to highlight its potential limitations.",
                "Methods": "We searched PubMed/MEDLINE for published research and articles that reviewed ChatGPT in the context of health literacy, written in English over the past five years with free full text access. All original articles were screened.",
                "Results": "A total of 53 publications were found. Twenty-five publications met the inclusion criteria and were included in the review. Most of the works presented a positive opinion towards the use of ChatGPT. The authors of six articles believe that ChatGPT generates more readable papers, while four of them believe the opposite. In four articles authors found out possibility to simplify text. The negative sides relate to the possibly insufficient education of users, the accuracy of the information, potential bias, dependence on the outdated data, and the ethical problem of the real authors not being cited, and therefore not recognized for their work.",
                "Conclusions": "According to the conducted research, ChatGPT represents an opportunity to improve health literacy by generating accessible and comprehensible information about health. It has also been shown to improve the readability and accessibility of public education materials on selected health topics. Furthermore, while AI models like ChatGPT can serve as a reliable source of health information, they should be used as an addition to professional medical consultation, and not as a standalone advice service.",
                "Key messages": "\u2022\u2002ChatGPT represents an opportunity to improve health literacy.\u2022\u2002AI models like ChatGPT can serve as a reliable source of health information, but they should be used as an addition to professional medical consultation, and not as a standalone advice service."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1093/eurpub/ckae144.1709",
                "pmid": null,
                "pmc": "PMC11518226",
                "pub_year": "2024",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11518226/"
            }
        },
        {
            "title": "IVC filter \u2013 assessing the readability and quality of patient information on the Internet",
            "abstract": {
                "Objective": "The internet is an increasingly favorable source of information regarding health-related issues. The aim of this study is to apply appropriate evaluation tools to assess the evidence available online about inferior vena cava (IVC) filters with a focus on quality and readability.",
                "Methods": "A search was performed during December 2022 using three popular search engines, namely Google, Yahoo, and Bing. Websites were categorized into academic, physician, commercial, and unspecified websites according to their content. Information quality was determined using Journal of the American Medical Association (JAMA) criteria, the DISCERN scoring tool, and whether a Health On the Net Foundation certification (HONcode) seal was present. Readability was established using the Flesch Reading Ease Score (FRES) and Flesch-Kincaid Grade Level (FKGL). Statistical significance was accepted as P\u00a0< .05.",
                "Results": "In total, 110 websites were included in our study. The majority of websites were categorized as commercial (25%), followed by hospital (24%), academic (21%), unspecified (16%), and physician (14%). Average scores for all websites using JAMA and DISCERN were 1.93\u00a0\u00b1 1.19 (median, 1.5; range, 0-4) and 45.20\u00a0\u00b1 12.58 (median, 45.5; range, 21-75), respectively. The highest JAMA mean score of 3.07\u00a0\u00b1 1.16 was allocated to physician websites, and the highest DISCERN mean score of 52.85\u00a0\u00b1 12.66 was allocated to hospital websites. The HONcode seal appeared on two of the selected websites. Physician, hospital, and unspecified websites had a significantly higher mean JAMA score than academic and commercial websites (all with P\u00a0< .001). Hospital websites had a significantly higher mean DISCERN score than academic (P\u00a0= .007), commercial (P\u00a0< .001), and unspecified websites (P\u00a0= .017). Readability evaluation generated a mean FRES score of 51.57\u00a0\u00b112.04, which represented a 10th to 12th grade reading level and a mean FKGL score of 8.20\u00a0\u00b1 1.70, which represented an 8th to 10th grade reading level. Only 12 sources were found to meet the \u22646th grade target reading level. No significant correlation was found between overall DISCERN score and overall FRES score.",
                "Conclusions": "The study results demonstrate that the quality of online information about IVC filters is suboptimal, and academic and commercial websites, in particular, must enhance their content quality regarding the use of IVC filters. Considering the discontinuation of the HONcode as a standardized quality assessment marker, it is recommended that a similar certification tool be developed and implemented for the accreditation of patient information online."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1016/j.jvsv.2023.101695",
                "pmid": "37898304",
                "pmc": "PMC11523360",
                "pub_year": "2023",
                "url": "https://pubmed.ncbi.nlm.nih.gov/37898304/"
            }
        },
        {
            "title": "Comparison of Large Language Models in Diagnosis and Management of Challenging Clinical Cases",
            "abstract": {
                "Purpose": "Compare large language models (LLMs) in analyzing and responding to a difficult series of ophthalmic cases.",
                "Design": "A comparative case series involving LLMs that met inclusion criteria tested on twenty difficult case studies posed in open-text format.",
                "Methods": "Fifteen LLMs accessible to ophthalmologists were tested against twenty case studies published in JAMA Ophthalmology. Each case was presented in identical, open-ended text fashion to each LLM and open-ended responses regarding differential diagnosis, next diagnostic tests and recommended treatments were requested. Responses were recorded and assessed for accuracy against published correct answers. The main outcome was accuracy of LLMs against the correct answers. Secondary outcomes included comparative performance on the differential diagnosis, ancillary testing, and treatment subtests; and readability of responses.",
                "Results": "Scores were normally distributed and ranged from 0\u201335 (with a maximum score of 60) with a mean \u00b1 standard deviation of 19 \u00b1 9. Scores for three of the LLMs (ChatGPT 3.5, Claude Pro, and Copilot Pro) were statistically significantly higher than the mean. Two of the high-performing LLMs were paid subscription (Claude Pro and Copilot Pro) and one was free (ChatGPT 3.5). While there were no clinical or statistical differences between ChatGPT 3.5 and Claude Pro, a separation of +5 points, or 0.56 standard deviations, between Copilot Pro and the other highly ranked LLMs was present. Readability of all tested programs were above the AMA (American Medical Association) reading level recommendations to public consumers of eight grade.",
                "Conclusion": "Subscription LLMs were more prevalent among highly ranked LLMs suggesting that these perform better as ophthalmic assistants. While readability was poor for the average person, the content was understood by a board-certified ophthalmologist. The accuracy of LLMs is not high enough to recommend patient care in standalone mode, but aiding clinicians in patient care and prevent oversights is promising."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.2147/OPTH.S488232",
                "pmid": "39555212",
                "pmc": "PMC11568767",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/39555212/"
            }
        },
        {
            "title": "Exploring the opportunities of large language models for summarizing palliative care consultations: A pilot comparative study",
            "abstract": {
                "Introduction": "Recent developments in the field of large language models have showcased impressive achievements in their ability to perform natural language processing tasks, opening up possibilities for use in critical domains like telehealth. We conducted a pilot study on the opportunities of utilizing large language models, specifically GPT-3.5, GPT-4, and LLaMA 2, in the context of zero-shot summarization of doctor\u2013patient conversation during a palliative care teleconsultation.",
                "Methods": "We created a bespoke doctor\u2013patient conversation to evaluate the quality of medical conversation summarization, employing established automatic metrics such as BLEU, ROUGE-L, METEOR, and BERTScore for quality assessment, and using the Flesch-Kincaid grade Level for readability to understand the efficacy and suitability of these models in the medical domain.",
                "Results": "For automatic metrics, LLaMA2-7B scored the highest in BLEU, indicating strong n-gram precision, while GPT-4 excelled in both ROUGE-L and METEOR, demonstrating its capability to capture longer sequences and semantic accuracy. GPT-4 also led in BERTScore, suggesting better semantic similarity at the token level compared to others. For readability, LLaMA 7B and LLaMA 13B produced summaries with Flesch-Kincaid grade levels of 11.9 and 12.6, respectively, which are somewhat more complex than the reference value of 10.6. LLaMA 70B generated summaries closest to the reference in simplicity, with a score of 10.7. GPT-3.5\u2019s summaries were the most complex at a grade level of 15.2, while GPT-4\u2019s summaries had a grade level of 13.1, making them moderately accessible.",
                "Conclusion": "Our findings indicate that all the models have similar performance for the palliative care consultation, with GPT-4 being slightly better at balancing understanding content and maintaining structural similarity to the source, which makes it a potentially better choice for creating patient-friendly medical summaries. Threats and limitations of such approaches are also embedded in our analysis."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1177/20552076241293932",
                "pmid": "39569395",
                "pmc": "PMC11577459",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/39569395/"
            }
        },
        {
            "title": "Analysis of informed consent documents for compliance with ICMR guidelines for biomedical and health research",
            "abstract": {
                "Background:": "Ethical conduct of research depends on the voluntary expression of consent and adequate disclosure of information about the research in informed consent documents (ICDs).",
                "Objectives:": "The objective of this study was to analyze ICDs of academic studies for compliance with National Ethical Guidelines for Biomedical and Health Research laid down by the Indian Council of Medical Research (ICMR) and to determine the readability of ICDs using the Flesch\u2013Kincaid Grade Level scale and Flesch reading-ease (FRE) score.",
                "Methodology:": "ICDs of academic research projects submitted during 2020\u201322 were retrieved from the IEC office and analyzed for compliance with ICMR 2017 guidelines. The readability of the documents was assessed by the Flesch\u2013Kincaid Grade Level Scale and FRE score.",
                "Results:": "Among 177 protocols analyzed, the most common were epidemiological studies (36.72%), followed by diagnostic studies (28.81%). Vernacular translations of ICDs were present in significantly more studies in 2022 (\u03c72 = 7.18, P = 0.02) as compared to 2020 and 2021. FREs score was 45.75 \u00b1 10.76, and Flesch\u2013Kincaid Grade Level was 8.67 \u00b1 1.44. Content analysis of participant information sheet (PIS) revealed that significantly more PIS submitted in 2022 mentioned expected duration of participation (\u03c72 = 6.95, P < 0.001), benefit to patient/community (\u03c72 = 26.63, P < 0.001), disclosure of foreseeable risk or discomfort (\u03c72 = 21.72, P < 0.001), payment for participation (\u03c72 = 21.72, P < 0.001), and identity of research team and contact details (\u03c72 = 18.58, P < 0.001). Compliance score was significantly better in 2022 as compared to 2020 and 2021.",
                "Conclusion:": "Gradually, ICDs became more compliant with ICMR guidelines. Still, there is scope for improvement in ICDs regarding content and readability so that patients can comprehend facts easily to make informed decisions in a real sense."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.4103/picr.picr_257_23",
                "pmid": null,
                "pmc": "PMC11584158",
                "pub_year": "2024",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11584158/"
            }
        },
        {
            "title": "Digesting Digital Health: A Study of Appropriateness and Readability of ChatGPT-Generated Gastroenterological Information",
            "abstract": {
                "INTRODUCTION:": "The advent of artificial intelligence\u2013powered large language models capable of generating interactive responses to intricate queries marks a groundbreaking development in how patients access medical information. Our aim was to evaluate the appropriateness and readability of gastroenterological information generated by Chat Generative Pretrained Transformer (ChatGPT).",
                "METHODS:": "We analyzed responses generated by ChatGPT to 16 dialog-based queries assessing symptoms and treatments for gastrointestinal conditions and 13 definition-based queries on prevalent topics in gastroenterology. Three board-certified gastroenterologists evaluated output appropriateness with a 5-point Likert-scale proxy measurement of currency, relevance, accuracy, comprehensiveness, clarity, and urgency/next steps. Outputs with a score of 4 or 5 in all 6 categories were designated as \u201cappropriate.\u201d Output readability was assessed with Flesch Reading Ease score, Flesch-Kinkaid Reading Level, and Simple Measure of Gobbledygook scores.",
                "RESULTS:": "ChatGPT responses to 44% of the 16 dialog-based and 69% of the 13 definition-based questions were deemed appropriate, and the proportion of appropriate responses within the 2 groups of questions was not significantly different (P = 0.17). Notably, none of ChatGPT\u2019s responses to questions related to gastrointestinal emergencies were designated appropriate. The mean readability scores showed that outputs were written at a college-level reading proficiency.",
                "DISCUSSION:": "ChatGPT can produce generally fitting responses to gastroenterological medical queries, but responses were constrained in appropriateness and readability, which limits the current utility of this large language model. Substantial development is essential before these models can be unequivocally endorsed as reliable sources of medical information."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.14309/ctg.0000000000000765",
                "pmid": "39212302",
                "pmc": "PMC11596446",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/39212302/"
            }
        },
        {
            "title": "Impact of Demographic Modifiers on Readability of Myopia Education Materials Generated by Large Language Models",
            "abstract": {
                "Background": "The rise of large language models (LLM) promises to widely impact healthcare providers and patients alike. As these tools reflect the biases of currently available data on the internet, there is a risk that increasing LLM use will proliferate these biases and affect information quality. This study aims to characterize the effects of different race, ethnicity, and gender modifiers in question prompts presented to three large language models (LLM) on the length and readability of patient education materials about myopia.",
                "Methods": "ChatGPT, Gemini, and Copilot were provided a standardized prompt incorporating demographic modifiers to inquire about myopia. The races and ethnicities evaluated were Asian, Black, Hispanic, Native American, and White. Gender was limited to male or female. The prompt was inserted five times into new chat windows. Responses were analyzed for readability by word count, Simple Measure of Gobbledygook (SMOG) index, Flesch-Kincaid Grade Level, and Flesch Reading Ease score. Significant differences were analyzed using two-way ANOVA on SPSS.",
                "Results": "A total of 150 responses were analyzed. There were no differences in SMOG index, Flesch-Kincaid Grade Level, or Flesch Reading Ease scores between responses generated with prompts containing different gender, race, or ethnicity modifiers using ChatGPT or Copilot. Gemini-generated responses differed significantly in their SMOG Index, Flesch-Kincaid Grade Level, and Flesch Reading Ease based on the race mentioned in the prompt (p<0.05).",
                "Conclusion": "Patient demographic information impacts the reading level of educational material generated by Gemini but not by ChatGPT or Copilot. As patients use LLMs to understand ophthalmologic diagnoses like myopia, clinicians and users should be aware of demographic influences on readability. Patient gender, race, and ethnicity may be overlooked variables affecting the readability of LLM-generated education materials, which can impact patient care. Future research could focus on the accuracy of generated information to identify potential risks of misinformation."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.2147/OPTH.S483024",
                "pmid": null,
                "pmc": "PMC11625417",
                "pub_year": "2024",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11625417/"
            }
        },
        {
            "title": "It takes a village to raise a child- A multidisciplinary approach to promoting paediatric health literacy in cystic fibrosis",
            "abstract": {
                "Background": "In Northern Ireland, approximately 550 people with cystic fibrosis (PwCF) attend the regional paediatric and adult centres within the Belfast Health and Social Care Trust. This autosomal recessive chronic condition necessitates regular clinical monitoring and a high treatment burden, as well as time implications for the maintenance of respiratory devices. Development of health literacy skills at an early age and promoting children with CF (CwCF) to take an active role in their healthcare has many advantages relating to their long-term self-care in preparation for transition from paediatric to adult care, decision-making and partnership engagement with the CF-multidisciplinary team (CF-MDT).",
                "Methods": "This study comprised of four individual components, namely (i) an analysis of responses (n=24) to an anonymous questionnaire from the Northern Ireland CF community to determine where PwCF and their carers/families seek healthcare information; (ii) to co-produce paediatric-facing healthcare educational resources, namely colouring/ storybooks and animations, relating to the importance of microbiological sampling, nebuliser hygiene and pancreatic replacement therapy (PERT) in conjunction with the CF-MDT, CwCF, parents, students and animators and (iii) assess the readability of these new materials using Flesch Reading Ease (FRE), Flesch-Kincaid Grade Level (FKGL), SMOG Index and Gunning Fog (GF) Index and compare these with paediatric and adult-facing materials available from CF charities, pharmaceutical companies and the scientific literature. The final component (iv) examined parents\u2019 and children\u2019s knowledge of PERT pre- and post-viewing the bespoke animation.",
                "Results": "(i) The findings showed that the CF community relied upon the CF-MDT as their primary source of healthcare information, most frequently consulting the Doctor/CF Consultant (61.5%), the physiotherapist (61.6%), the nurse (57.7%), followed by the CF dietitian (34.6%), as well as the Cystic Fibrosis Trust (38.4%). Pharmaceutical websites were least consulted with 69.2% of respondents never consulting such resources.(ii) Reflective learning points from this co-production of resources are provided to assist other healthcare teams preparing engaging and effective healthcare information for the paediatric service user.(iii) The readability of the new paediatric-facing materials prepared by the CF-MDT was appropriate for primary school aged-children and was not statistically different from paediatric-facing information prepared by charities or pharmaceutical companies. A statistical difference was noted in relation to the prepared materials in comparison with adult-facing charity information (p=0.04; 0.02; 0.03; 0.04) and scientific abstracts (p<0.0001), which were more complex in terms of readability parameters, FRE, FKGL, SMOG and GF, respectively.(iv) Following viewing the PERT animation, both parents\u2019 and children\u2019s knowledge had improved with 50% of children\u2019s understanding determined as moderate/little understanding (pre-animation) and 50% very good/ 42 % good (post-animation).",
                "Conclusions": "Healthcare professionals are important custodians of healthcare information for their service user population. Paediatric healthcare teams have a responsibility to aid in the development of health literacy skills at an early age and promoting children to take an active role in their healthcare. The use of colouring/storybooks and animations are excellent media to initiate discussions and develop partnerships in paediatric healthcare in an engaging and informative manner. Whilst this study related to CwCF, the findings may be applicable to the health literacy of children of other disease states. For optimum impact, the healthcare team should (i) co-produce these media with the paediatric service user, their families and animation teams and (ii) ensure that the readability, legibility and formats are appropriate, informative and engaging for the target age-group."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": null,
                "pmid": null,
                "pmc": "PMC11633313",
                "pub_year": "2024",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11633313/"
            }
        },
        {
            "title": "Enhancing Multilingual Patient Education: ChatGPT's Accuracy and Readability for SSNHL Queries in English and Spanish",
            "abstract": {
                "title": "Abstract",
                "Objective": "This study investigates ChatGPT's accuracy, readability, understandability, and actionability in responding to patient queries on sudden sensorineural hearing loss (SSNHL) in English and Spanish, when compared to Google responses. The objective is to address concerns regarding its proficiency in addressing medical inquiries when presented in a language divergent from its primary programming.",
                "Study Design": "Observational.",
                "Setting": "Virtual environment.",
                "Methods": "Using ChatGPT 3.5 and Google, questions from the AAO\u2010HNSF guidelines were presented in English and Spanish. Responses were graded by 2 otolaryngologists proficient in both languages using a 4\u2010point Likert scale and the PEMAT\u2010P tool. To ensure uniform application of the Likert scale, a third independent evaluator reviewed the consistency in grading. Readability was evaluated using 3 different tools specific to each language. IBM SPSS Version 29 was used for statistical analysis using one\u2010way analysis of variance.",
                "Results": "Across both languages, the responses displayed a native\u2010level language proficiency. Accuracy was comparable between sources and languages. Google's Spanish responses had better readability (effect size 0.35, P\u2009<\u2009.001), while Google's English responses were more understandable (effect size 0.67, P\u2009=\u2009.018). ChatGPT's English responses demonstrated the highest level of actionability (60%), though not significantly different when compared to other sources (effect size 0.47, P\u2009=\u2009.14).",
                "Conclusion": "ChatGPT offers patients comprehensive and guideline\u2010conforming answers to SSNHL patient medical queries in the 2 most spoken languages in the United States. However, improvements in its readability and understandability are warranted for more accessible patient education."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1002/oto2.70048",
                "pmid": "39664064",
                "pmc": "PMC11633712",
                "pub_year": "2024",
                "url": "https://pubmed.ncbi.nlm.nih.gov/39664064/"
            }
        },
        {
            "title": "Evaluating Quality and Readability of AI-generated Information on Living Kidney Donation",
            "abstract": {
                "Background.": "The availability of high-quality and easy-to-read informative material is crucial to providing accurate information to prospective kidney donors. The quality of this information has been associated with the likelihood of proceeding with a living donation. Artificial intelligence\u2013based large language models (LLMs) have recently become common instruments for acquiring information online, including medical information. The aim of this study was to assess the quality and readability of artificial intelligence-generated information on kidney donation.",
                "Methods.": "A set of 35 common donor questions was developed by the authors and used to interrogate 3 LLMs (ChatGPT, Google Gemini, and MedGPT). Answers were collected and independently evaluated using the CLEAR tool for (1) completeness, (2) lack of false information, (3) evidence-based information, (4) appropriateness, and (5) relevance. Readability was evaluated using the Flesch-Kincaid Reading Ease Score and the Flesch-Kincaid Grade Level.",
                "Results.": "The interrater intraclass correlation was 0.784 (95% confidence interval, 0.716-0.814). Median CLEAR scores were ChatGPT 22 (interquartile range [IQR], 3.67), Google Gemini 24.33 (IQR, 2.33), and MedGPT 23.33 (IQR, 2.00). ChatGPT, Gemini, and MedGPT had mean Flesch-Kincaid Reading Ease Scores of 37.32 (SD = 10.00), 39.42 (SD = 13.49), and 29.66 (SD = 7.94), respectively. Using the Flesch-Kincaid Grade Level assessment, ChatGPT had an average score of 12.29, Gemini had 10.63, and MedGPT had 13.21 (P\u2005<\u20050.001), indicating that all LLMs had a readability at the college-level education.",
                "Conclusions.": "Current LLM provides fairly accurate responses to common prospective living kidney donor questions; however, the generated information is complex and requires an advanced level of education. As LLMs become more relevant in the field of medical information, transplant providers should familiarize themselves with the shortcomings of these technologies."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1097/TXD.0000000000001740",
                "pmid": null,
                "pmc": "PMC11634323",
                "pub_year": "2024",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11634323/"
            }
        },
        {
            "title": "P022 How well are we communicating the complications of adenotonsillectomy in children for sleep disordered breathing \u2013 a longitudinal analysis of medical information on the internet in Australia.",
            "abstract": {
                "title": "Abstract",
                "Introduction": "Sleep-disordered breathing is a prevalent sleep disorder among children, Adenotonsillectomy is the usual treatment. However, this procedure carries potential complications, including pain and various morbidities and even death. As the internet increasingly serves as the primary source of medical information, it is crucial to have quality information, that is consistent with current literature and at an appropriate readability available for parents and patients.",
                "Methods": "The study employed a three-point longitudinal study (2018, 2020 and 2023) mixed analysis, utilizing NVivo 12 software, to assess the quality, consistency, and readability of website text data. Various metrics were used, including the Flesch-Kincaid reading ease score, Flesch-Kincaid grade level, amongst others and the DISCERN Tool for evaluating quality. Website information was compared to known complications to determine the consistency which these potential issues are addressed.",
                "Results": "Quality: The 2020 average quality score of 2 of 4 indicated average quality, The 2018 quality was not measured, and the 2023 quality measure will be presented at the conference but is yet to be performed.",
                "Consistency": "Most sites mentioned common complications like bleeding and pain but mostly failed to discuss rarer complications across all 3 time points.",
                "Readability": "The readability for 2018 was 53.11 and 2023 was 53.36 for the Flesch-Kincaid reading ease score indicating content was \u201cfairly difficult to read\u201d across time points.",
                "Discussion": "This study demonstrates website information concerning the potential complications of adentonsillectomy could be improved with only average quality, low consistency and low readability over the preceding 6 years."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1093/sleepadvances/zpae070.104",
                "pmid": null,
                "pmc": "PMC11646008",
                "pub_year": "2024",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11646008/"
            }
        },
        {
            "title": "Pharmacists\u2019 perception of educational material to improve patient safety",
            "abstract": {
                "title": "Abstract",
                "title_content_0": "Educational material (EM) addresses particular safety information of medicinal products to healthcare professionals and patients. Since 2016, German national competent authorities label approved EM with a Blue Hand symbol. However, data is scarce regarding its usability as a safety communication tool in pharmacies to improve patient safety. The purpose of this study is to investigate for the first time pharmacists\u2019 awareness and perception of EM in the setting of community and hospital pharmacies in Germany.",
                "title_content_1": "The Drug Commission of German Pharmacists surveyed its nationwide network of 677 community and 51 hospital reference pharmacies, to investigate their awareness and perception of EM. The survey was conducted between January 16 and February 10, 2020 using SurveyMonkey. Data were analyzed using Microsoft Excel.",
                "title_content_2": "A total of 373 community and 32 hospital pharmacists participated; response rates were 55.1% and 62.8%, respectively. Overall, 320 (85.8%) community and all hospital pharmacists confirmed awareness of EM. Community and hospital pharmacists fully (n\u200a=\u200a172, 46.9% and n\u200a=\u200a9, 28.1%) or rather (n\u200a=\u200a109, 29.7% and n\u200a=\u200a10, 31.3%) agreed that EM for healthcare professionals is suitable to reduce risks of medicinal products. Moreover, 237 (64.7%) community and 17 (53.1%) hospital pharmacists confirmed to inform patients or care facilities about EM. Asking pharmacists on their personal perception of EM, the refinement of readability and accessibility was indicated.",
                "title_content_3": "Pharmacists confirm awareness of EM and its suitability as a safety communication tool. However, from a pharmacists\u2019 perspective, the applicability and readability of EM still needs further adjustment to improve patient safety."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1097/MD.0000000000025144",
                "pmid": "33725997",
                "pmc": "PMC7982216",
                "pub_year": "2021",
                "url": "https://pubmed.ncbi.nlm.nih.gov/33725997/"
            }
        },
        {
            "title": "Assessing the Readability of Medicine Information Materials: The Case of Tikur Anbessa Specialized Hospital \u2013 Mixed Approach",
            "abstract": {
                "Background": "Patients are frequently provided with medicine information materials (MIMs). Rendering medicine information through written material is a reliable method. Readability is an important attribute of written material that can affect the reader\u2019s ability to comprehend. Patient\u2019s perception can also affect the comprehensibility of written MIMs.",
                "Objective": "The objectives of the study were to assess the readability of medicine information in Tikur Anbessa Specialized Hospital (TASH); and assessing patients\u2019 perception and understanding of medicine information materials.",
                "Methods": "This was a cross-sectional study conducted from September 21, 2019 to November 24, 2020, at TASH. Quantitative and qualitative data collection approaches were used in this research. The readability value of each material was determined in accordance with the Flesch Reading ease scores (FRE) and Flesch-Kincaid Grade Level (FKGL). The tools compute readability based on an average number of syllables per word and an average number of words per sentence. FRE provides scores from 0 to 100; higher scores mean easily comprehensible while FKGL sets grade levels for written texts. A structured interview was administered with questions about how MIMs had been used, and was analyzed qualitatively.",
                "Results": "The results of this research showed low readability scores of MIMs found in TASH. Most patients do not get MIMs and are unaware of how to use them. They are interested to receive and read medicines information from pharmacists and physicians. Moreover, most of them preferred information through both verbal and written forms.",
                "Conclusion": "The readability levels of selected MIMs obtained from TASH are found to be not compliant with the patients\u2019 needs. This might be worsening their health outcomes and resulting in poorer use of healthcare services."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.2147/PPA.S302275",
                "pmid": "33790543",
                "pmc": "PMC7997408",
                "pub_year": "2021",
                "url": "https://pubmed.ncbi.nlm.nih.gov/33790543/"
            }
        },
        {
            "title": "Readability of Patient Education Materials From High-Impact Medical Journals: A 20-Year Analysis",
            "abstract": {
                "title_content_0": "Comprehensive patient education is necessary for shared decision-making. While patient\u2013provider conversations primarily drive patient education, patients also use published materials to enhance their understanding. In this investigation, we evaluated the readability of 2585 patient education materials published in high-impact medical journals from 1998 to 2018 and compared our findings to readability recommendations from national groups. For all materials, mean readability grade levels ranged from 11.2 to 13.8 by various metrics. Fifty-four (2.1%) materials met the American Medical Association recommendation of sixth grade reading level, and 215 (8.2%) met the National Institutes of Health recommendation of eighth grade level. When stratified by journal and material type, general medical education materials from Annals of Internal Medicine were the most readable (P < .001), with 79.8% meeting the eighth grade level. Readability did not differ significantly over time. Efforts to standardize publication practice with the incorporation of readability evaluation during the review process may improve patients\u2019 understanding of their disease processes and treatment options."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1177/2374373521998847",
                "pmid": "34179407",
                "pmc": "PMC8205335",
                "pub_year": "2021",
                "url": "https://pubmed.ncbi.nlm.nih.gov/34179407/"
            }
        },
        {
            "title": "Patient-reported outcome measures in ophthalmology: too difficult to read?",
            "abstract": {
                "Objective": "Patient-reported outcome measures (PROMs) are commonly used in clinical trials and research. Yet, in order to be effective, a PROM needs to be understandable to respondents. The aim of this cross-sectional analysis was to assess reading level of PROMs validated for use in common eye conditions.",
                "Methods and analysis": "Readability measures determine the level of education a person is expected to have attained to be able to read a passage of text; this was calculated using the Flesch-Kincaid Grade Level, FORCAST and Gunning-Fog tests within readability calculations software package Oleander Readability Studio 2012.1. Forty PROMs, previously validated for use in at least one of age-related macular degeneration, glaucoma and/or diabetic retinopathy, were identified for inclusion via a systematic literature search. The American Medical Association (AMA) and National Institutes of Health (NIH) recommend patient materials should not exceed a sixth-grade reading level. Number of PROMs exceeding this level was calculated.",
                "Results": "Median (IQR) readability scores were 7.9 (5.4\u201310.5), 9.9 (8.9\u201310.7) and 8.4 (6.9\u201311.1) for Flesch-Kincaid Grade Level, FORCAST and Gunning-Fog test, respectively. Depending on metric used, this meant 61% (95% CI 45% to 76%), 100% (95% CI 91% to 100%) and 80% (95% CI 65% to 91%) exceeded the recommended threshold.",
                "Conclusion": "Most PROMs commonly used in ophthalmology require a higher reading level than that recommended by the AMA and NIH and likely contain questions that are too difficult for many patients to read. Greater care is needed in designing PROMs appropriate for the literacy level of a population."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1136/bmjophth-2020-000693",
                "pmid": "34212114",
                "pmc": "PMC8208024",
                "pub_year": "2021",
                "url": "https://pubmed.ncbi.nlm.nih.gov/34212114/"
            }
        },
        {
            "title": "Availability and readability of online patient information on clubfoot: assessment of paediatric hospital clubfoot web pages",
            "abstract": {
                "title": "Abstract",
                "Purpose": "To determine the availability and readability of online patient information (OPI) provided by paediatric hospitals in the United States using clubfoot as a model condition",
                "Methods": "The websites of the top 95 paediatric hospitals identified using US News & World Report were included. The names of paediatric hospitals and the terms \u201cclubfoot\u201d, \u201cclubfeet\u201d and \u201ctalipes equinovarus\u201d were entered into the Google search engine. Readability was assessed using five validated metrics and the composite grade level (CGL). The number of unpaid monthly visits was calculated with the Ahrefs Organic Traffic Score (OTS) tool. Data for paediatric hospitals were compared with the same metrics for the top ten Google search results.",
                "Results": "Of 95 paediatric hospitals, 29 (30.5%) did not have at least one web page dedicated to clubfoot. The 128 web pages representing 66 paediatric hospitals had an average CGL of 9.4, representing a readability level requiring some high school education. The mean OTS for all paediatric hospitals was 116 estimated visits per month, which was significantly less than that for the top ten Google clubfoot search results (3035.1; p < 0.0001).",
                "Conclusion": "Paediatric hospital web pages on clubfoot were visited much less frequently than those from the top ten Google search results. Only two web pages (1.6%) from paediatric hospitals offered OPI on clubfoot that met the American Medical Association recommended reading level (sixth-grade level). Paediatric hospitals should create OPI on clubfoot with appropriate readability and accessibility for patient families.",
                "Level of Evidence": "N/A"
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1302/1863-2548.15.210013",
                "pmid": "34211606",
                "pmc": "PMC8223082",
                "pub_year": "2021",
                "url": "https://pubmed.ncbi.nlm.nih.gov/34211606/"
            }
        },
        {
            "title": "Evaluation of the Octreotide Acetate Pen Injector in a Formative Human Factors Study",
            "abstract": {
                "title": "Abstract",
                "title_content_0": "Introduction: Subcutaneous injection of octreotide acetate is indicated to treat adults with acromegaly and diarrhea associated with carcinoid tumors or vasoactive intestinal peptide tumors. In this formative human factors study, we evaluated the readability and comprehension of the instructions for use (IFU) and ease of use of the octreotide pen injector. Methods: The study enrolled patients and healthcare practitioners who would be using the pen injector. The IFU contained a stepwise process with illustrations to detail injection administration and safe storage of the octreotide pen injector. Participants read the IFU and familiarized themselves with the device. Participants administered 2 unaided injections into skin-like pads. Injection success was defined as an attempt that delivered the correct dose into the pad. Each injection was evaluated by objective performance and subjective measures. Objective performance measures included assessment of steps necessary to deliver the correct medication dose and ensure user safety. Subjective measures included soliciting participant feedback on perceived success and difficulties administering a dose with the octreotide pen injector, as well as suggestions for improvements. Additional goals included evaluation of the IFU and octreotide pen injector usability aspects. Results: A total of 8 patients and 3 healthcare practitioners enrolled in the study. All (n = 11) participants successfully administered both injections, leading to an overall injection success rate of 100% across twenty-two injections. Subtask errors included participants priming the pen injector with the incorrect dose (n = 1) and not holding the injection button for 10 seconds after the injection (n = 2), but neither error resulted in dosing failure. Participant suggestions for improving the IFU included changes to the illustration of the plunger, reordering statements to clarify the priming process, and detailing how long to let the pen injector come to room temperature. Conclusion: Overall, participants felt the octreotide pen injector was easy to use and the instructions were clearly written and illustrated. Participant feedback and observations by moderators of the study led to recommendations for improvements to the clarity of the IFU."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1210/jendso/bvab048.2083",
                "pmid": null,
                "pmc": "PMC8265737",
                "pub_year": "2021",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC8265737/"
            }
        },
        {
            "title": "The use of the Gunning Fog Index to evaluate the readability of Polish and English drug leaflets in the context of Health Literacy challenges in Medical Linguistics: An exploratory study",
            "abstract": null,
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.5603/CJ.a2020.0142",
                "pmid": "33140389",
                "pmc": "PMC8277010",
                "pub_year": "2021",
                "url": "https://pubmed.ncbi.nlm.nih.gov/33140389/"
            }
        },
        {
            "title": "Evaluation of quality and readability of online patient information on osteoporosis and osteoporosis drug treatment and recommendations for improvement",
            "abstract": {
                "Summary": "Patient information is important to help patients fully participate in their healthcare. Commonly accessed osteoporosis patient information resources were identified and assessed for readability, quality, accuracy and consistency. Resources contained inconsistencies and scored low when assessed for quality and readability. We recommend optimal language and identify information gaps to address.",
                "Introduction": "The purpose of this paper is to identify commonly accessed patient information resources about osteoporosis and osteoporosis drug treatment, appraise the quality and make recommendations for improvement.",
                "Methods": "Patient information resources were purposively sampled and text extracted. Data extracts underwent assessment of readability (Flesch Reading Ease and Flesch-Kincaid Grade Level) and quality (modified International Patient Decision Aid Standards (m-IPDAS)). A thematic analysis was conducted, and keywords and phrases were used to describe osteoporosis and its treatment identified. Findings were presented to a stakeholder group who identified inaccuracies and contradictions and discussed optimal language.",
                "Results": "Nine patient information resources were selected, including webpages, a video and booklets (available online), from government, charity and private healthcare providers. No resource met acceptable readability scores for both measures of osteoporosis information and drug information. Quality scores from the modified IPDAS ranged from 21 to 64% (7\u201321/33). Thematic analysis was informed by Leventhal\u2019s Common-Sense Model of Disease. Thirteen subthemes relating to the identity, causes, timeline, consequences and controllability of osteoporosis were identified. Phrases and words from 9 subthemes were presented to the stakeholder group who identified a predominance of medical technical language, misleading terms about osteoporotic bone and treatment benefits, and contradictions about symptoms. They recommended key descriptors for providers to use to describe osteoporosis and treatment benefits.",
                "Conclusions": "This study found that commonly accessed patient information resources about osteoporosis have highly variable quality, scored poorly on readability assessments and contained inconsistencies and inaccuracies. We produced practical recommendations for information providers to support improvements in understanding, relevance, balance and bias, and to address information gaps.",
                "Supplementary Information": "The online version contains supplementary material available at 10.1007/s00198-020-05800-7."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1007/s00198-020-05800-7",
                "pmid": "33501570",
                "pmc": "PMC8376728",
                "pub_year": "2021",
                "url": "https://pubmed.ncbi.nlm.nih.gov/33501570/"
            }
        },
        {
            "title": "Assessing the Accuracy, Quality, and Readability of Patient Accessible Online Resources Regarding Ocular Gene Therapy and Voretigene Neparvovec",
            "abstract": {
                "Purpose": "To evaluate the accuracy, quality, and readability of online information regarding the Food and Drug Administration (FDA) approved ocular gene therapy voretigene neparvovec (Luxturna, Spark Therapeutics, Philadelphia, PA, USA).",
                "Methods": "Ten online resources about voretigene neparvovec were assessed in this cross-sectional study. A novel 25-question assessment was created to evaluate the information most relevant to patients. Each article was assessed by independent graders using the assessment and the DISCERN instrument. An online readability tool, Readable, was used to assess readability. Accountability was evaluated using the Journal of the American Medical Association (JAMA) benchmarks.",
                "Results": "The average questionnaire score for all the articles was 33.93 (SD 11.21, CI 95% \u00b16.95) out of 100 possible points with significant variation in the content accuracy and quality between the articles (P=0.017). EyeWiki achieved the highest score and MedicineNet the lowest. The mean reading grade for all articles was 12.88 (SD 1.93, CI 95% \u00b11.19) with significant variation between articles (P=0.001). Wikipedia was the most readable, and the FDA website was the least. None of the articles achieved all four JAMA benchmarks, and only one of the ten articles, EyeWiki, achieved three of the four JAMA benchmarks.",
                "Conclusion": "The information available online regarding this FDA-approved ocular gene therapy is generally of low quality, above the average reading level of the general population, and varies significantly between sources. The articles provide incomplete information that is not entirely accurate or easy to read, and as a result, the material would not support patients adequately in their medical decisions and questions about this new therapeutic option."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.2147/OPTH.S324231",
                "pmid": "34556973",
                "pmc": "PMC8455297",
                "pub_year": "2021",
                "url": "https://pubmed.ncbi.nlm.nih.gov/34556973/"
            }
        },
        {
            "title": "Health Services: A Mixed Methods Assessment of Canadian Cancer Patient Education Materials Related to the 2019 Novel Coronavirus",
            "abstract": {
                "title_content_0": "The 2019 novel coronavirus (COVID-19) pandemic has prompted the reorganization in the scheduling and method of care for many patients, including patients diagnosed with cancer. Cancer patients, who have an immunocompromised status, may be at a higher risk of severe symptoms from infection with COVID-19. While information is rapidly evolving regarding COVID-19, Canada, both nationally and provincially, has been conveying new information to patients online. We assessed the content and readability of COVID-19-related online Canadian patient education material (PEM) for cancer patients to determine if the content of the material was written at a grade reading level that the majority of Canadians can understand. PEMs were extracted from provincial cancer agencies and the national Canadian Cancer Society, evaluated using 10 readability scales, qualitatively analyzed to identify their themes and difficult word content. Thirty-eight PEMs from both national and provincial cancers associations were, on average, written above the recommended 7th grade level. Each of the associations\u2019 average grade levels were: BC Cancer (11.00 95% confidence interval [CI] 8.27-13.38), CancerControl Alberta (10.46 95% CI 8.29-12.62), Saskatchewan Cancer Agency (11.08 95% CI 9.37-12.80), Cancer Care Manitoba (9.55 95% CI 6.02-13.01), Cancer Care Ontario (9.35 95% CI 6.80-11.90), Cancer Care Nova Scotia (10.95 95% CI 9.86-12.04), Cancer Care Eastern Health Newfoundland and Labrador (10.14 95% CI 6.87-13.41), and the Canadian Cancer Society (10.06 95% CI 8.07-12.05). Thematic analysis identified 4 themes: public health strategy, information about COVID-19, patient instructions during COVID-19, and resources. Fifty-three percent of the complex words identified were medical jargon. This represents an opportunity to improve PEM readability, to allow for greater comprehension amongst a wider target audience."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1177/1073274821989709",
                "pmid": "33563050",
                "pmc": "PMC8482715",
                "pub_year": "2021",
                "url": "https://pubmed.ncbi.nlm.nih.gov/33563050/"
            }
        },
        {
            "title": "Institutional improvements in readability of written informed consent forms sustained post-revised Common Rule",
            "abstract": {
                "title_content_0": "Obtaining informed consent is a fundamental and ethical practice within human subjects\u2019 research. Informed consent forms (ICFs) include a large amount of information, much of which may be unfamiliar to research subjects, and the revised Common Rule resulted in several required additions to that language. As limited health literacy impacts many potential subjects, efforts should be made to optimize subjects\u2019 ability to read and understand ICFs. In this brief report, we describe an assessment of ICFs at an academic medical center to evaluate longitudinal changes in readability with the introduction and update of a plain language ICF template."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1017/cts.2021.860",
                "pmid": "34849266",
                "pmc": "PMC8596070",
                "pub_year": "2021",
                "url": "https://pubmed.ncbi.nlm.nih.gov/34849266/"
            }
        },
        {
            "title": "BMJ HCI launches partnership programme for patients and carers as authors and peer reviewers",
            "abstract": {
                "title_content_0": "BMJ Health & Care Informatics (BMJHCI) is launching a partnership programme, where patients write articles and serve as peer reviewers on both patient-written and researcher-written articles. This article outlines the programme and describes the importance of public involvement in research and implementation in digital health. We think patients and carers should be funded to participate at this stage of research as well as other stages of research. The quality of peer review can be greatly improved by recruiting patients to peer review and improve readability and understanding of scientific literature and to ensure that research and other articles appropriately include what matters most to patients. Just as real-time communication is two-way communication, both healthcare providers and patients should have a voice in the literature, and involving patients in journals is an important step toward amplifying and supporting the balance of perspectives. Patients are the whole purpose of research and practice in health and care, so this rightly includes their role in the publication and review of health informatics literature as well as the publication of their own perspectives regarding access and delivery of healthcare. Patients and carers can provide valuable insights into research articles, and they can also serve as effective peer reviewers. The BMJHCI is excited to kick off the new partnership programme and encourages all interested patients and carers to apply to participate as authors and/or reviewers."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1136/bmjhci-2021-100471",
                "pmid": "34799412",
                "pmc": "PMC8609930",
                "pub_year": "2021",
                "url": "https://pubmed.ncbi.nlm.nih.gov/34799412/"
            }
        },
        {
            "title": "Are English-language online patient education materials related to breast cancer risk assessment understandable, readable, and actionable?",
            "abstract": {
                "Purpose": "To evaluate the readability, understandability, and actionability of online patient education materials (OPEM) related to breast cancer risk assessment.",
                "Material and methods": "We queried seven English-language search terms related to breast cancer risk assessment: breast cancer high-risk, breast cancer risk factors, breast cancer family history, BRCA, breast cancer risk assessment, Tyrer-Cuzick, and Gail model. Websites were categorized as: academic/hospital-based, commercial, government, non-profit or academic based on the organization hosting the site. Grade-level readability of qualifying websites and categories was determined using readability metrics and generalized estimating equations based on written content only. Readability scores were compared to the recommended parameters set by the American Medical Association (AMA). Understandability and actionability of OPEM related to breast cancer high-risk were evaluated using the Patient Education Materials Assessment Tool (PEMAT) and compared to criteria set at \u226570%. Descriptive statistics and inter-rater reliability analysis were utilized.",
                "Results": "343 websites were identified, of which 162 met study inclusion criteria. The average grade readability score was 12.1 across all websites (range 10.8\u201313.4). No website met the AMA recommendation. Commercial websites demonstrated the highest overall average readability of 13.1. Of the 26 websites related to the search term breast cancer high-risk, the average understandability and actionability scores were 62% and 34% respectively, both below criteria.",
                "Conclusions": "OPEM on breast cancer risk assessment available to the general public do not meet criteria for readability, understandability, or actionability. To ensure patient comprehension of medical information online, future information should be published in simpler, more appropriate terms."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1016/j.breast.2021.11.012",
                "pmid": "34894464",
                "pmc": "PMC8665407",
                "pub_year": "2021",
                "url": "https://pubmed.ncbi.nlm.nih.gov/34894464/"
            }
        },
        {
            "title": "Readability of Online Foot & Ankle Patient Resources",
            "abstract": {
                "Category:": "Other",
                "Introduction/Purpose:": "The internet is a popular source of health information for patients. Professional organizations, such as the American Orthopaedic Foot & Ankle Society (AOFAS) and the American Academy of Orthopaedic Surgeons (AAOS) have created patient-directed websites containing high quality information. However, previous studies have demonstrated that these sites are often written at a level that is too complex for the average reader. We aimed to investigate the current landscape of the readability of these websites.",
                "Methods:": "Online patient resources on foot and ankle topics published by the AOFAS and AAOS were reviewed. Each page was analyzed using the Flesch Kincaid Grade Level (FKGL) and Flesch Reading Ease (FRE) formulas. Sites were noted if they were written at or below an 8th grade reading level, which corresponds to the average US reading level. An independent two-sample t- test or chi-square test, where appropriate, were used to compare resource characteristics between the two professional societies.",
                "Results:": "162 total sites were reviewed, including 113 from the AOFAS and 49 from the AAOS. Overall, the mean FKGL was 9.4 (range, 6.7 - 15.3) and the mean FRE was 56.3 (range, 24.4 - 73.1). Websites written by the AAOS had significantly lower FKGL scores than sites written by the AOFAS (9.0 vs 9.5, p=0.01). 31.3% of all sites were written at or below an 8th grade reading level. Websites published by the AAOS had significantly more sites published at this level than sites published by the AOFAS (44.9% vs 25.7%, p=0.02).",
                "Conclusion:": "Online patient information on foot and ankle topics published by professional organizations are often written at a level that is too complex for the average reader. There was a statistical but not clinically significant difference between sites published by the AAOS and AOFAS. Professional organizations should continue to develop their online patient resources, with emphasis on increasing their readability."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1177/2473011421S00163",
                "pmid": "35097654",
                "pmc": "PMC8793614",
                "pub_year": "2022",
                "url": "https://pubmed.ncbi.nlm.nih.gov/35097654/"
            }
        },
        {
            "title": "Four reasons why too many informed consents to clinical research are invalid: a critical analysis of current practices",
            "abstract": {
                "Objective": "Informed consent (IC) is a central ethical and legal requirement for clinical research that aims to protect the autonomy of participants. To enable an autonomous decision and valid consent, adequate understanding must be ensured. However, a considerable proportion of participants do not understand the relevant aspects about participation in research, for example, approximately 45% could not name at least one risk. As such, the inadequate understanding of IC has been known for several decades, and it still constitutes a severe problem for the ethical conduct of research. Through delineating the most pressing deficits of current IC procedures that lead to insufficient understanding, we aim to encourage the discussion among stakeholders, for example, clinical researchers, and to provide the grounds for practical solutions.",
                "Main arguments": "(1) IC documents are too long to be read completely, thus, make it very difficult for potential participants to identify the material facts about the trial. (2) The low readability of the IC documents disadvantages persons with limited literacy. (3) The therapeutic misconception frequently prevents participants to realise that the primary purpose of clinical research is to benefit future patients. (4) Excessive risk disclosures, insufficient information about expected benefits and framing effects compromise a rational risk/benefit assessment.",
                "Conclusion": "Due to these deficits, practices of IC in clinical research too often preclude adequate understanding of prospective participants, thus, invalidating IC. The gap between the well-specified ethical norm to enable IC and its insufficient translation into practice can no longer be accepted, as participant rights and the public trust in responsible research are at stake. Hence, immediate action is needed to address the prevailing deficits."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1136/bmjopen-2021-050543",
                "pmid": "35246415",
                "pmc": "PMC8900041",
                "pub_year": "2022",
                "url": "https://pubmed.ncbi.nlm.nih.gov/35246415/"
            }
        },
        {
            "title": "Assessment of the readability and quality of online patient education materials for the medical treatment of open-angle glaucoma",
            "abstract": {
                "Objective": "Patient adherence to glaucoma medications is poor, and is linked to low literacy levels. Patients commonly use the internet to access health information, and it is recommended that patient information is written at an 11-year-old reading level. The aim of this study is to assess the readability and quality of online patient education materials for the medical management of open angle glaucoma.",
                "Methods and analysis": "The top 10 relevant Google searches for nine glaucoma medications (timolol, brimonidine, apraclonidine, dorzolamide, latanoprost, bimatoprost, travoprost, tafluprost and brinzolamide) and three generic searches were analysed for readability and accountability. Readability was assessed using Flesch Reading Ease Score (FRES), Flesch-Kincaid Grade Level (FKGL), Gunning Fog Index (GFI) and Simple Measure of Gobbledygook Index (SMOG). Webpages were classified by source and assessed using Journal of the American Medical Association (JAMA) benchmarks of accountability.",
                "Results": "111 articles were included in the analysis. Mean readability scores were: FRES 55.5 (95% CI 53.4 to 57.5); FKGL 9.7 (95% CI 9.3 to 10.0); GFI 12 (95% CI 11.6 to 12.4) and SMOG 9.3 (95% CI 8.9 to 9.6). One-way analysis of variance demonstrated no significant difference in readability score between source type. 9% of the webpages satisfied all 4 JAMA benchmarks. Pearson\u2019s correlation coefficient showed a correlation between the FRES and accountability score (r=0.19, p=0.045).",
                "Conclusion": "The majority of online patient education materials for the medical treatment of glaucoma are written at a level too difficult for the general population and fail to meet accountability standards."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1136/bmjophth-2021-000966",
                "pmid": null,
                "pmc": "PMC8961144",
                "pub_year": "2022",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC8961144/"
            }
        },
        {
            "title": "Older adult and family caregiver experiences with prescription medication labels and their suggestions for label improvement",
            "abstract": {
                "Background": "Prescription medication labels (PMLs) are an important source of written medication information for patients. However, real-world PMLs do not entirely conform with available labelling best practices and guidelines. Given this disconnect, older adults remain particularly at risk of misinterpreting their PMLs. Past studies have commonly assessed hypothetical PMLs, warranting studies that explore the lived experiences of older adults with real-world PMLs. Furthermore, the perspective of family caregivers of older adults is yet to be studied.",
                "Objective(s)": "This qualitative study documented the challenges faced by older adults and their family caregivers in using real-world PMLs, their strategies to cope with these challenges, and their suggestions to improve existing PMLs.",
                "Methods": "We conducted two focus group discussions (n\u00a0=\u00a017) and 30 in-depth interviews with older adults (n\u00a0=\u00a020; including those who can read in English and those with limited English proficiency) and caregivers (n\u00a0=\u00a010) in Singapore. The data were systematically assigned to codes that were continuously refined to accommodate emergent themes.",
                "Results": "Challenges, coping strategies and suggested improvements were related to the comprehensibility, availability, readability and consistency of medication information on PMLs.",
                "Conclusions": "Real-world PMLs continue to pose challenges for older adults and their caregivers, necessitating them to seek unique and personal coping strategies. The identified PML improvements, desired by older adults and their caregivers, urge healthcare systems to implement improved PMLs. Future research should explore system-level logistical, financial, and administrative barriers (or opportunities) that hinder (or facilitate) this implementation."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1016/j.rcsop.2021.100087",
                "pmid": null,
                "pmc": "PMC9029911",
                "pub_year": "2021",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC9029911/"
            }
        },
        {
            "title": "The Chinese version of the revised Diabetes Distress Scale for adults with type 2 diabetes: Translation and validation study",
            "abstract": {
                "Objectives": "This study aimed to translate the revised 17-item Diabetes Distress Scale (DDS17, 2017) into mandarin (simplified) Chinese and validate the Chinese version of DDS17 (C-DDS17, 2021) among adult patients with type 2 diabetes in China.",
                "Methods": "A scale translation and cross-sectional validation study was conducted. The DDS17 was translated into mandarin (simplified) Chinese through a five-step process: authorization, forward translation, synthesis, back translation, and amendment. During this session, 59 patients assessed the understandability and readability of the translated scale. From June 7 to September 4, 2021, a cross-sectional study that adhered to the COSMIN checklist was conducted with 400 individuals with type 2 diabetes from three Class A tertiary comprehensive hospitals in Beijing, China. The content, construct, convergent, discriminant validity, and reliability (Cronbach\u2019s \u03b1 coefficient and item-total correlation coefficients) of the C-DDS17 were evaluated. This study was a part of a project registered in the Chinese Clinical Trial Registry (no. ChiCTR2100047071).",
                "Results": "Among the participants, 33.3% (133/400) of them experienced moderate to high diabetes distress. The content validity indices of the C-DDS17 equaled 1.00. The scale yielded a four-factor structure. The average variances extracted were 0.42\u20130.57, which was lower than squared correlations. Cronbach\u2019s \u03b1 coefficient was 0.88 for the overall scale and ranged from 0.76 to 0.81 for sub-scales. Corrected item-total correlation coefficients ranged from 0.42 to 0.61. The eighth item (\u201cFeeling that I am often failing with my diabetes routine\u201d) was better fit to physician distress than regimen distress but had little influence on the validation results.",
                "Conclusions": "The C-DDS17 is a reliable and valid instrument for assessing diabetes distress in patients with type 2 diabetes. It is a promising instrument for early identification and management of diabetes distress in clinical practice and trials."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1016/j.ijnss.2022.03.002",
                "pmid": null,
                "pmc": "PMC9052264",
                "pub_year": "2022",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC9052264/"
            }
        },
        {
            "title": "A Health Communication Assessment of Web-based Obstructive Sleep Apnea Patient Education Materials",
            "abstract": {
                "Background": "The current care pathway for screening, diagnosis, and treatment for obstructive sleep apnea (OSA) is often fragmented and heavily reliant on patient action, leading to delays and gaps in care, which disproportionately affect race and ethnic minorities. There is a need for well-designed, accessible patient education materials (PEMs) to improve OSA awareness and empower those at risk for the condition with the necessary knowledge and skills to adhere to treatment.",
                "Objective": "Our study aimed to evaluate the understandability, accessibility, actionability, and readability of web-based PEMs designed for patients with OSA and their families and caregivers.",
                "Methods": "We engaged patients with OSA, clinicians, and patient advocates (n\u2009=\u200911) to identify a list of web-based OSA PEMs from the media, medical centers, medical device companies, and health professional and patient advocacy organizations. Two trained coders scored the PEMs using validated health communication assessments, including the Centers for Disease Control and Prevention Clear Communication Index (CCI; on a scale from 0 to 100%); the Patient Education Materials Assessment Tool (PEMAT), which features subscales for understandability and actionability, each measured from 0 to 100%; and readability measures, including the Simple Measure of Gobbledygook and Flesch-Kincaid, which correspond to grade levels.",
                "Results": "We identified 20 web-based PEMs, which included websites (n\u2009=\u200912, 60%), online flyers (n\u2009=\u20094, 20%), videos (n\u2009=\u20093, 15%), and one discussion board (n\u2009=\u20091, 5%). Scores on the CCI ranged from 21.4 to 85.7%. No PEMs met the CCI cutoff (90%). Scores on the PEMAT scales for understandability ranged from 37.5 to 100%. Scores on the PEMAT scales for actionability ranged from 0 to 100%. Fifteen percent of the PEMs met the PEMAT cutoff for understandability and actionability. Readability of the PEMs ranged from a 5th to a 15th-grade reading level, as scored by the Simple Measure of Gobbledygook and Flesch-Kincaid. Only one PEM (5%) met the recommended sixth-grade reading level.",
                "Conclusion": "Our study found that the majority of commonly used web-based PEMs for OSA did not meet recommended standards for clear communication and health literacy demands. OSA practitioners and future research should consider health communication best practices to design PEMs that reduce the gap between materials and average patient health literacy."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.34197/ats-scholar.2021-0055OC",
                "pmid": null,
                "pmc": "PMC9131885",
                "pub_year": "2022",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC9131885/"
            }
        },
        {
            "title": "Comparison of innovative communication approaches in nutrition to promote and improve health literacy",
            "abstract": {
                "title": "ABSTRACT",
                "title_content_0": "The translation of scientific evidence into guidelines and advice is a fundamental aspect of scientific communication within nutrition and dietetics. For communication to be effective for all patients, health literacy (HL) must be considered, i.e. an individual\u2019s capacity to obtain, comprehend and utilise information to empower decision-making and promote their own health. HL levels are varied and difficult to judge on an individual basis and have not been quantified, thus not giving a population mean HL competency indication. It has been evidenced that most of the working age population in England cannot comprehend healthcare materials due to complexity, thereby promoting a need for agreed readability thresholds for written healthcare information. A wide range of modalities within dietetics are used to communicate to a varied audience with the primary form written, e.g. journal articles, plain language summaries and leaflets. Audio/visual and digital communications are increasing in dietetic care and welcomed by patients; however, the effectiveness of such approaches has not been studied thoroughly and digital exclusion remains a concern. Communication considering a patient\u2019s HL level leads to empowerment which is key to effective management of chronic diseases with a high treatment burden. Therefore; this review will focus on the importance of modalities used to communicate science in nutrition to ensure they are appropriate in relation to Health Literacy."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": null,
                "pmid": null,
                "pmc": "PMC9200103",
                "pub_year": "2022",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC9200103/"
            }
        },
        {
            "title": "Assessing the Quality, Reliability, and Readability of Online Information on Dry Eye Disease",
            "abstract": {
                "title": "Purpose:",
                "title_content_0": "The purpose of this study was to assess the quality, reliability, readability, and technical quality of web sites relating to dry eye disease.",
                "Methods:": "A cross-sectional study was conducted that evaluated the first 75 web sites on a Google Search by using the keyword \u201cdry eyes.\u201d Each web site was evaluated by 2 independent reviewers using the DISCERN, HONcode, and JAMA criteria to assess quality and reliability. Interrater reliability was also analyzed. Readability was assessed using the Flesch\u2013Kincaid readability tests and the Gunning fog, Simple Measure of Gobbledygook, Coleman\u2013Liau, and automated readability indices. Technical quality was determined by the presence of 10 specific features. Web sites were further categorized into institutional (academic centers, medical associations, and government institutions) and private (private practices) categories.",
                "Results:": "There was no significant difference in scoring observed between the 2 reviewers. The overall mean DISCERN score \u00b1 standard error (SE) was 3.2 \u00b1 0.1, the mean HONcode score (\u00b1SE) was 9.3 \u00b1 0.3, and the mean JAMA score (\u00b1SE) was 1.9 \u00b1 0.1. Institutional web sites had a higher DISCERN score (3.4 \u00b1 0.1 vs. 3.1 \u00b1 0.1; P < 0.05) and HONcode score (10.3 \u00b1 0.5 vs. 8.8 \u00b1 0.4; P < 0.05) than private sites. Technical quality was higher in institutional web sites compared with private web sites (P < 0.05). Readability was poor among all web sites, with most web sites not achieving below a ninth grade reading level.",
                "Conclusions:": "Quality, reliability, and readability scores were low for most web sites. Although institutional web sites achieved higher scores than private web sites, revision is warranted to improve their overall quality of information and readability profile."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1097/ICO.0000000000003034",
                "pmid": "35344972",
                "pmc": "PMC9273298",
                "pub_year": "2022",
                "url": "https://pubmed.ncbi.nlm.nih.gov/35344972/"
            }
        },
        {
            "title": "Can Patients Read, Understand, and Act on Online Resources for\nAnterior Cruciate Ligament Surgery?",
            "abstract": {
                "Background:": "Patients undergoing elective procedures often utilize online educational\nmaterials to familiarize themselves with the surgical procedure and expected\npostoperative recovery. While the Internet is easily accessible and\nubiquitous today, the ability of patients to read, understand, and act on\nthese materials is unknown.",
                "Purpose:": "To evaluate online resources about anterior cruciate ligament (ACL) surgery\nutilizing measures of readability, understandability, and actionability.",
                "Study Design:": "Cross-sectional study; Level of evidence, 4.",
                "Methods:": "Using the term \u201cACL surgery,\u201d 2 independent searches were performed utilizing\na public search engine (Google.com). Patient education\nmaterials were identified from the top 50 results. Audiovisual materials,\nnews articles, materials intended for advertising or medical professionals,\nand materials unrelated to ACL surgery were excluded. Readability was\nquantified using the Flesch Reading Ease, Flesch-Kincaid Grade Level, Simple\nMeasure of Gobbledygook, Coleman-Liau Index, Automated Readability Index,\nand Gunning Fog Index. The Patient Education Materials Assessment Tool for\nPrintable Materials (PEMAT-P) was utilized to assess the actionability and\nunderstandability of materials. For each online source, the relationship\nbetween its Google search rank (from first to last) and its readability,\nunderstandability, and actionability was calculated utilizing the Spearman\nrank correlation coefficient (\u03c1S).",
                "Results:": "Overall, we identified 68 unique websites, of which 39 met inclusion\ncriteria. The mean Flesch-Kincaid Grade Level was 10.08 \u00b1 2.34, with no\nwebsite scoring at or below the 6th-grade level. Mean understandability and\nactionability scores were 59.18 \u00b1 10.86 (range, 33.64-79.17) and 34.41 \u00b1\n22.31 (range, 0.00-81.67), respectively. Only 5 (12.82%) and 1 (2.56%)\nresource scored above the 70% adequate PEMAT-P threshold mark for\nunderstandability and actionability, respectively. Readability (lowest\nP value = .103), understandability (\u03c1S =\n\u20130.13; P = .441), and actionability (\u03c1S = 0.28;\nP = .096) scores were not associated with Google\nrank.",
                "Conclusion:": "Patient education materials on ACL surgery scored poorly with respect to\nreadability, understandability, and actionability. No online resource scored\nat the recommended reading level of the American Medical Association or\nNational Institutes of Health. Only 5 resources scored above the proven\nthreshold for understandability, and only 1 resource scored above it for\nactionability."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1177/23259671221089977",
                "pmid": null,
                "pmc": "PMC9344126",
                "pub_year": "2022",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC9344126/"
            }
        },
        {
            "title": "Improving health literacy with mumps, measles and rubella (MMR) vaccination: comparison of the readability of MMR patient-facing literature and MMR scientific abstracts",
            "abstract": {
                "Background:": "Historically, there have been many factors that have influenced mumps, measles and rubella (MMR) vaccine uptake, including media bias, social/economic determinants, parental education level, deprivation and concerns over vaccine safety. Readability metrics through online tools are now emerging as a means for healthcare professionals to determine the readability of patient-facing vaccine information. The aim of this study was to examine the readability of patient-facing materials describing MMR vaccination, through employment of nine readability and text parameter metrics, and to compare these with MMR vaccination literature for healthcare professionals and scientific abstracts relating to MMR vaccination.",
                "Materials and methods:": "The subscription-based online Readable program (readable.com) was used to determine nine readability indices using various readability formulae: Established readability metrics (n\u2009=\u20095) (Flesch\u2013Kinkaid Grade Level, Gunning Fog Index, SMOG Index, Flesch Reading Ease and New Dale-Chall Score), as well as Text parameters (n\u2009=\u20094) (sentence count, word count, number of words per sentence, number of syllables per word) with 47 MMR vaccination texts [patient-facing literature (n\u2009=\u200922); healthcare professional\u2013focused literature (n\u2009=\u20098); scientific abstracts (n\u2009=\u200917)].",
                "Results:": "Patient-facing vaccination literature had a Flesch Reading Ease score of 58.4 and a Flesch\u2013Kincaid Grade Level of 8.1, in comparison with poorer readability scores for healthcare professional literature of 30.7 and 12.6, respectively. MMR scientific abstracts had the poorest readability (24.0 and 14.8, respectively). Sentence structure was also considered, where better readability metrics were correlated with significantly lower number of words per sentence and less syllables per word.",
                "Conclusion:": "Use of these readability tools enables the author to ensure their research is more readable to the lay audience. Patient co-production initiatives would help to ensure that not only can the target audience read the literature, but that they understand the content. Increased patient-centric focus groups would give better insights into reasons for MMR-associated vaccine hesitation and vaccine refusal."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1177/25151355221118812",
                "pmid": null,
                "pmc": "PMC9400405",
                "pub_year": "2022",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC9400405/"
            }
        },
        {
            "title": "Readability of information imprinted in patient information leaflets (PILs) in Saudi Arabia: The case of antihypertensive medications",
            "abstract": {
                "Background": "The Saudi Food and Drug Authority (SFDA) requires marketing authorization holders to submit a PIL in both Arabic and English language. However, the readability of imprinted and disseminated Patient information leaflets (PILs) was not assessed extensively in Saudi Arabia. This study aims to assess the readability of PIL of antihypertensive drugs in both Arabic and English languages.",
                "Method": "This study was a descriptive quantitative analysis conducted in Saudi Arabia in August 2021. PILs of all oral antihypertensive medications in Saudi Arabia were included in the study. The Arabic and English PILs were extracted from the Saudi Drugs Information System (SDI) and pharmaceutical companies' registration documents. The study used Flesch-Kincaid grade level to assess the readability of English and sentence length to assess the Arabic texts. Descriptive analyses were used to assess the readability scores and the mean differences.",
                "Results": "It was found that almost 88% of English PILs were above recommended readability level compared to 79% of Arabic PILs. About 89% of English PILs of generic and 86% of brand-name medications were above the readability cutoff point compared with 83% of Arabic PILs of generic and 68% of brand-name medications. The means of grade level for readability of PILs for the widely used antihypertensive medications including angiotensin II receptor blockers (ARBs), antiadrenergic, diuretics, Beta-blockers (BBs), calcium channel blockers (CCBs), and combination antihypertensive medications, and CCBs were higher than the recommended readability level (p < 0.05). The highest mean grade level for readability among English PILs was for combinations of antihypertensive agents (9.35 \u00b1 1.38, p 0.01) and among Arabic PILs was for ARBs (6.15 \u00b1 1.62, p < 0.01).",
                "Conclusions": "The majority of PILs of antihypertensive medications were above the recommended readability level that can be understood by the majority of the public, especially among generic medications and the most widely used antihypertensive medications. The study findings highlight the need of implementing guidelines to improve the readability of information imprinted in PILs and adopt new regulations requiring readability assessment for manufactures before submitting the PILs to the SFDA."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1016/j.rcsop.2022.100179",
                "pmid": null,
                "pmc": "PMC9513263",
                "pub_year": "2022",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC9513263/"
            }
        },
        {
            "title": "The quality and readability of online molar incisor hypomineralization patient education materials: a systematic analysis",
            "abstract": {
                "title": "Abstract",
                "Background": "Molar incisor hypomineralization (MIH) is estimated to affect 14% of children worldwide. It is crucial that patients and their families have access to easily comprehensible and reliable MIH\u2010relevant information. This study aims to determine the quality, reliability and readability of online patient education materials about MIH.",
                "Methods": "A systematic search strategy was adopted. Five validated tools were used to assess the content of the 21 websites that satisfied inclusion/exclusion criteria. Data analyses were applied via GraphPad Prism software version 9 (GraphPad Software, San Diego, CA, USA).",
                "Results": "Five (23.8%) websites only satisfied the criteria for understandability and two (9.5%) websites satisfied the criteria for actionability using the Patient Education Materials Assessment Tool (PEMAT). No website contained the Health on the Net (HON)Code Seal and the mean (SD) Journal of the\nAmerican Medical Association number of benchmarks per website was 1.33/4 (1.02). All websites failed to reach recommended minimum readability levels. Higher PEMAT scores were associated with \u2018easier\u2019 readability.",
                "Conclusions": "Online patient education materials related to MIH are lacking in quality and reliability, and are too difficult for most to read easily. The authors of MIH\u2010related online content should consider reference to quality of information tools when developing patient education materials."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1111/adj.12899",
                "pmid": "35075657",
                "pmc": "PMC9541321",
                "pub_year": "2022",
                "url": "https://pubmed.ncbi.nlm.nih.gov/35075657/"
            }
        },
        {
            "title": "Poor compliance documenting informed consent in trauma patients with distal radius fractures compared to elective total knee arthroplasty",
            "abstract": {
                "title": "Abstract",
                "Background": "The purpose of this study was (1) to evaluate the adequacy of informed consent documentation in the trauma setting for distal radius fracture surgery compared with the elective setting for total knee arthroplasty (TKA) at a large public hospital and (2) to explore the relevant guidelines in New Zealand relating to consent documentation.",
                "Methods": "Consecutive adult patients (\u226516\u2009years) undergoing operations for distal radius fractures and elective TKA over a 12\u2010month period in a single\u2010centre were retrospectively identified. All medical records were reviewed for the risks and complications recorded. The consent form was analysed using the Flesch Reading Ease Score (FRES) and the Simple Measure of Gobbledygook (SMOG) index readability scores.",
                "Results": "A total of 133 patients undergoing 134 operations for 135 distal radius fractures and 239 patients undergoing 247 TKA were included. Specific risks of surgery were recorded significantly less frequently for distal radius fractures than TKA (43.3% versus 78.5%, P\u2009<\u20090.001). Significantly fewer risks were recorded in the trauma setting compared to the elective (2.35\u2009\u00b1\u20092.98 versus 4.95\u2009\u00b1\u20093.33, P\u2009<\u20090.001). The readability of the consent form was 40.5 using the FRES and 10.9 using the SMOG index, indicating a university undergraduate level of reading.",
                "Conclusions": "This study has shown poor compliance in documenting risks of surgery during the informed consent process in an acute trauma setting compared to elective arthroplasty. Institutions must prioritize improving documentation of informed consent for orthopaedic trauma patients to ensure a patient\u2010centred approach to healthcare."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1111/ans.17781",
                "pmid": "35588267",
                "pmc": "PMC9543849",
                "pub_year": "2022",
                "url": "https://pubmed.ncbi.nlm.nih.gov/35588267/"
            }
        },
        {
            "title": "Readability and quality levels of websites that contain written information about anterior cruciate ligament injury: A survey of Turkish websites",
            "abstract": {
                "Objective:": "This study aimed to evaluate the contents and readability levels of informative texts about anterior cruciate ligament (ACL) on Turkish websites.",
                "Methods:": "In this cross-sectional study, online searches were performed using the Google, Yandex, and Yahoo search engines on 16, 17, and 18 November 2020, respectively. In these three search engines, \u2018anterior cruciate ligament\u2019, \u2018anterior cruciate ligament surgery\u2019, \u2018ACL\u2019, and \u2018ACL surgery\u2019 were entered in Turkish. The first 10 pages from each search on the websites were collected. The websites were divided into 3 groups according to their sources. Group 1 was classified as websites prepared by private hospitals or medical centers; group 2, as individual websites of orthopedics and traumatology physicians; and group 3, as non-profit websites providing general health information that does not fall into these two groups. The websites were analyzed based on both the website interface and a specific content scoring guide by two reviewers. The Flesch Kincaid (FK) grade level and the Flesch reading ease (FRE) score were used to determine the readability of information on the websites.",
                "Results:": "Eighty-five unique websites were evaluated. The mean quality score of all the websites was 10.4 \u00b1 4.5 with a maximum score of 25 (range\u2009=\u20093\u201321). No significant difference in quality score was found between the groups. The mean FK grade score of all the websites was 11.2 \u00b1 1.7 (range\u2009=\u20097.9\u201315.3). The mean FRE score of all the websites was 46.8 \u00b1 7.7 (range\u2009=\u200924.1\u201363.7). No statistically significant differences in FK grade and FRE score were found between the groups. Although 59 websites (69%) had a third-party seal indicating the certification of one of the organizations established to provide a standard of health information on the Internet, only 21 websites (25%) were updated in the year before the search.",
                "Conclusion:": "The readability level of the informative texts about the ACL on the Turkish websites was above the educational level in Turkey. In addition, the quality score of the Turkish websites related to ACL was low. The content of the informative texts should be organized while taking into account the patients\u2019 literacy level."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.5152/j.aott.2022.21142",
                "pmid": "35416158",
                "pmc": "PMC9612650",
                "pub_year": "2022",
                "url": "https://pubmed.ncbi.nlm.nih.gov/35416158/"
            }
        },
        {
            "title": "LBODP043 Analysis Of Plastic Surgery Websites And Their Recommendations For Diabetics",
            "abstract": {
                "title": "Abstract",
                "title_content_0": "Diabetic patients are predisposed to adverse complications after surgery, especially when their A1c is above 8. 0% (1). Despite the heightened risk for post-operative complications such as wound dehiscence and infection (2), no formal recommendations by the American Council of Academic Plastic Surgeons (ACAPS) exist regarding preoperative A1c thresholds. This study reviews the current online recommendations for diabetic patients undergoing plastic surgery and examines the readability and technical metrics presented on these webpages. We hypothesized that these webpages would enforce stricter preoperative A1c levels (< 8%) in comparison to the 8% threshold put forth by Diabetes Care in 2014. An anonymous, depersonalized Google search with the search term \"diabetes and plastic surgery\" was run. The initial 50 results were analyzed and 11 webpages meeting inclusion criteria extracted. 45.45% (5/11) of the webpages stated specific A1c recommendations, with 4 of the webpages recommending an A1c <7% and one webpage recommending an A1c <6%. A number of the websites recommended that patients consult their primary care physician (PCP) or endocrinologist, 63.6% (7/11). 100% (11/11) of webpages discussed poor wound healing and 45% (5/11) discussed the heightened risk of postoperative infection in diabetic surgical patients. Webpage average readability scores for seven readability measures greatly exceeded the 6th grade reading level recommended for medical information. This study determined that online resources for diabetic patients undergoing plastic surgery utilized stricter than standard preoperative criteria and exceeded recommended readability levels. Standardizing requirements for diabetic patients and improving readability may help patients better understand the preoperative expectation for better surgical outcomes. References: (1) Underwood, Patricia et al. \"Preoperative A1C and clinical outcomes in patients with diabetes undergoing major noncardiac surgical procedures.\" Diabetes care vol. 37,3 (2014): 611-6. doi: 10.2337/dc13-1929(2) Goltsman, David et al. \"Defining the Association between Diabetes and Plastic Surgery Outcomes: An Analysis of Nearly 40,000 Patients.\" Plastic and reconstructive surgery. Global open vol. 5,8 e1461. 17 Aug. 2017, doi: 10.1097/GOX. 0000000000001461",
                "title_content_1": "Presentation: No date and time listed"
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1210/jendso/bvac150.553",
                "pmid": null,
                "pmc": "PMC9624660",
                "pub_year": "2022",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC9624660/"
            }
        },
        {
            "title": "Assessing the Accuracy, Quality, and Readability of Online Patient\nResources on Achilles Tendonitis",
            "abstract": {
                "Category:": "Other; Hindfoot",
                "Introduction/Purpose:": "The internet is full of websites focused on different medical topics. With\nsuch a large accumulation of information, patients should be able to rely\nupon the internet as a reliable source for information on common medical\nconditions such as Achilles tendonitis. However, the question remains as to\nthe accuracy, quality and readability of such online patient resources.\nConsumers themselves commonly evaluate online medical sites by\ntrustworthiness, expertise, and objectivity. The purpose of this study was\nto investigate the top search-engine-generated online resources for Achilles\ntendonitis for their content and accuracy.",
                "Methods:": "The term 'Achilles tendonitis' was searched using the three most population\nsearch engines. The first 50 sites from each search were recorded. Duplicate\nsites were eliminated. Sites were categorized based on authorship type. Each\nwebsite was independently evaluated by two reviewers. Quality was evaluated\nutilizing the DISCERN score and the Journal of the American Medical\nAssociation (JAMA) benchmark criteria. Accuracy was evaluated using a novel\nAchilles tendinosis-specific content rubric developed with a foot and ankle\nfellowship-trained orthopaedic surgeon and the AAOS OrthoInfo webpage,\nsimilar to previous studies. Readability was evaluated using the Flesch\nKincaid grade level (FKGL) and Flesch Reading Ease (FRE) formulas. Higher\nFRE scores indicate better readability. FKGL was also compared against an\n8th grade level, which represents the average US reading level.",
                "Results:": "The search resulted in 77 unique websites with 60 sites included for\nanalysis. The average DISCERN score in our investigation is 42.1 which\nplaces it in the 'fair' category. Commercial websites were found to have\nhigher DISCERN scores as compared to websites created by physicians or\nmedical groups as well as academic websites. Our study found an average JAMA\nquality benchmark score of 2.2 out of 4. Our study found that on average,\nonline patient resources on Achilles tendinosis was written at a 9th grade\nreading level with a FKGL score of 9.8. Academic websites were found to be\nwritten at a significantly lower reading level of 8.8. The average FRE score\nwas 53.28 which translates to the 'fairly difficult' category. Utilizing our\nown content scoring system to assess the accuracy of online resources, the\nwebsites had an average accuracy of 11.5 points out of 20 possible\npoints.",
                "Conclusion:": "The average online patient resource on Achilles tendinosis is too complex for\nthe average patient, inaccurate, and is of low quality. This study found\nthat the average website had low quality as measured by the DISCERN and JAMA\nbenchmark scores. The average accuracy was also low as measured by our novel\ncontent specific score. The websites were also difficult to read as measured\nby the FRE and FKGL scores. More emphasis on improving the readability,\nquality, and accuracy of online patient resources on Achilles tendinosis\nshould be undertaken in order to improve patient healthcare."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1177/2473011421S01017",
                "pmid": null,
                "pmc": "PMC9703492",
                "pub_year": "2022",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC9703492/"
            }
        },
        {
            "title": "Content and Readability of Online Recommendations for Breast Implant Size Selection",
            "abstract": {
                "title": "Background:",
                "title_content_0": "Breast augmentation is one of the most frequently performed aesthetic procedures in the United States. Online information is often used by patients to make decisions when undergoing cosmetic procedures. It is vital that online medical information includes relevant decision-making factors and uses language that is understandable to broad patient audiences. Ideally, online resources should aid patient decisions in aesthetic surgical planning, especially implant size selection for breast augmentation. We describe patient decision-making factors and readability of breast implant size selection recommended by private practice plastic surgery webpages.",
                "Methods:": "Using a depersonalized, anonymous query to Google search engine, the terms \u201cbreast implant size factors\u201d and \u201cbreast implant size decision\u201d were searched, and 52 plastic surgery private practice webpages were identified. Webpages were analyzed for reported decision-making factors of implant size selection. Readability analyses of webpages were performed with Readability Studio and Hemingway Editor.",
                "Results:": "The two major decision-making factors for implant size selection reported by webpages were body/tissue-based measurements and surgeon input. Ten factors related to patient lifestyle, surgical goals, and procedural options were also identified. Average webpage scores for five readability measures exceeded recommended levels for medical information.",
                "Conclusions:": "Reported decision-making factors for implant size selection emphasize a plastic surgeon\u2019s expertise but may enhance the patient\u2019s role in preoperative planning. Webpages describing breast implant size selection exceed the sixth and eighth grade reading levels recommended by the AMA and NIH, respectively. Improving the readability of webpages will refine the role of online medical information in preoperative planning of breast augmentation."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1097/GOX.0000000000004787",
                "pmid": null,
                "pmc": "PMC9872969",
                "pub_year": "2023",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC9872969/"
            }
        },
        {
            "title": "How understandable are the patient education materials about flat foot on the Internet for parents?",
            "abstract": {
                "title_content_0": "Flat foot is a common reason for parents to visit orthopedic clinics. As the Internet has become an easy-search platform, parents often seek online educational materials before seeking out a professional. The aim of this study was to investigate the quality, readability, and understandability of such online materials for parents. An Internet search was performed for \u201cflat foot\u201d and \u201cpes planus\u201d using the Google search engine. The readability was evaluated using 6 different grading systems: Flesch Reading Ease Score, Flesch\u2013Kincaid Grade Level, Simple Measure of Gobbledygook, Fry Readability score, Gunning Fog Index tests, and Automated Readability Index. The Patient Education Materials Assessment Tool test was used to assess the understandability. For quality assessment, the Journal of American Medical Association benchmark criteria and Health on the Net code were applied. One hundred nine websites were included and evaluated for readability, understandability, and quality. The mean readability grade for all websites was 10.5\u2005\u00b1\u20052.0. The mean Gunning Fog Index tests and Flesch\u2013Kincaid Grade Level scores for all websites were 12.4\u2005\u00b1\u20052.2 and 9.7\u2005\u00b1\u20052.1 sequentially. The mean Coleman\u2013Liau index score was 10.0\u2005\u00b1\u20051.5, and the average Fry Readability score was 9.9\u2005\u00b1\u20052.0. The automated readability index for all websites was 10.3\u2005\u00b1\u20052.5. The average Flesch Reading Ease score for all educational materials was 59.3\u2005\u00b1\u200510.1. The average Patient Education Materials Assessment Tool score for all educational materials was 81% (range, 70\u201387%). The mean Journal of American Medical Association benchmark criterion for all websites was 1.0, with a range from 1.0 and 2.0. Eighteen (16.5%) websites had Health on the Net certificates. Readability, understandability, and quality of patient education materials about flat feet on the Internet vary and are often worse than professional recommendations."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1097/MD.0000000000032791",
                "pmid": null,
                "pmc": "PMC9907911",
                "pub_year": "2023",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC9907911/"
            }
        },
        {
            "title": "Challenges encountered by pharmacy staff in using prescription medication labels during medication counselling with older adults and solutions employed: A mixed-methods study",
            "abstract": {
                "Background": "Prescription medication labels (PMLs) predominantly dispensed in English, are an important adjunct to medication counselling. PMLs are routinely used by pharmacy staff to counsel older adults about their medications. This study sought to identify challenges that pharmacy staff observe older adults face in using their PMLs, and to identify and quantify solutions employed by pharmacy staff during medication counselling to address such challenges.",
                "Methods": "Ten in-depth interviews were done with primary care pharmacy staff to gather the range of challenges and solutions. Subsequently, a quantitative survey, informed by the qualitative findings, was administered to 121 pharmacy staff to assess if the reported solutions were commonly used.",
                "Results": "The two main challenges were incongruity between PML language (English) and older adults' language proficiency, and poor PML legibility. The solutions, classified under three themes, were simplifying medication information on PMLs, supplementing PMLs with additional medication information and mitigating poor readability.",
                "Conclusions": "Pharmacy staff observed challenges faced by older adults in using PMLs during medication counselling. Ad-hoc improvisations by pharmacy staff to PMLs were pervasive. System-level PML improvements, such as provision of legible bilingual medication instructions, pharmaceutical pictograms and additional medication information, through patient information leaflets or using quick response (QR) codes on PMLs, should be considered. This will facilitate patient-provider communication, especially in settings with language dissonance between PMLs and patients."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1016/j.rcsop.2023.100226",
                "pmid": null,
                "pmc": "PMC9918413",
                "pub_year": "2023",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC9918413/"
            }
        },
        {
            "title": "Designing clinical guidelines that improve access and satisfaction in the emergency department",
            "abstract": {
                "title": "Abstract",
                "title_content_0": "Clinical guidelines are evidence\u2010based clinician decision\u2010support tools that improve health outcomes, reduce patient harm, and decrease healthcare costs, but are often underused in emergency departments (EDs). This article describes a replicable, evidence\u2010based design\u2010thinking approach to developing best practices for guideline design that improves clinical satisfaction and usage. We used a 5\u2010step process to enhance guideline usability in our ED. First, we conducted end\u2010user interviews to identify barriers to guideline usage. Second, we reviewed the literature to identify key principles in guideline design. Third, we applied our findings to create a standardized guideline format, incorporating rapid cycle learning and iterative improvements. Fourth, we ensured the clinical validity of our updated guidelines by using a rigorous process for peer review. Lastly, we evaluated the impact of our guideline conversion process by tracking clinical guidelines access per day from October 2020 to January 2022. Our end\u2010user interviews and review of the design literature revealed several barriers to guideline use, including lack of readability, design inconsistencies, and guideline complexity. Although our previous clinical guideline system averaged 0.13 users per day, >43 users per day accessed the clinical guidelines on our new digital platform in January 2022, representing an increase in access and use exceeding 33,000%. Our replicable process using open\u2010access resources increased clinician access to and satisfaction with clinical guidelines in our ED. Design\u2010thinking and use of low\u2010cost technology can significantly improve clinical guideline visibility and has the potential to increase guideline use."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1002/emp2.12919",
                "pmid": null,
                "pmc": "PMC9990158",
                "pub_year": "2023",
                "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC9990158/"
            }
        },
        {
            "title": "Evaluating the Spanish readability of American Society for Metabolic and Bariatric Surgery (ASMBS) Centers of Excellence (COE) websites",
            "abstract": {
                "Background": "Healthcare disparities continue to be an ongoing struggle in Bariatrics. Limited availability of Spanish online material may be a correctible barrier for accessibility to Hispanic patients. We sought to evaluate accredited Bariatric Centers of Excellence (COE) for Spanish readability via their websites to determine accessibility for Spanish speakers.",
                "Methods": "This was an internet research study. 103 COE accredited by American Society for Metabolic and Bariatric Surgery (ASMBS) and the American College of Surgeons (ACS) were evaluated and assigned one of five Spanish Visibility Categories.The United States was divided into 4 regions. Regional Spanish visibility was calculated by dividing each category count by the number of institutions in each region.County Spanish-speaking populations were obtained from the US Census Bureau\u2019s 2009\u20132013 American Community Survey. Differences in their distributions across the Spanish Visibility Categories were investigated using the Mann\u2013Whitney U test.",
                "Results": "25% of websites were translatable to Spanish, and a regional discrepancy was found with 61% translatable in the West, 19% in Northeast, 19% in Midwest, and 15% in South. Median Spanish-speaking population was higher in counties where websites were translatable to Spanish than where websites were not translatable.",
                "Conclusion": "Healthcare disparities in Bariatrics continue to be an ongoing struggle. We suggest that Spanish readability for ASMBS ACS COE websites should be improved regardless of geographic differences in Spanish-speaking populations. We believe it would be valuable for these websites to have standards for readability of Spanish and other languages."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1007/s00464-023-09978-9",
                "pmid": "36914781",
                "pmc": "PMC10010650",
                "pub_year": "2023",
                "url": "https://pubmed.ncbi.nlm.nih.gov/36914781/"
            }
        },
        {
            "title": "Chest CT in COVID-19 patients: Structured vs conventional reporting",
            "abstract": {
                "Purpose": "To assess clinician satisfaction with structured (SR) and conventional (CR) radiological reports for chest CT exams in coronavirus disease 2019 (COVID-19) patients, objectively comparing both reporting strategies.",
                "Method": "We retrospectively included 68 CTs (61 patients) with COVID-19. CRs were collected from the digital database while corresponding SRs were written by an expert radiologist, including a sign checklist, severity score index and final impressions. New CRs were prepared for a random subset (n = 10) of cases, to allow comparisons in reporting time and word count. CRs were analyzed to record severity score and final impressions inclusion. A random subset of 40 paired CRs and SRs was evaluated by two clinicians to assess, using a Likert scale, readability, comprehensiveness, comprehensibility, conciseness, clinical impact, and overall quality.",
                "Results": "Overall, 19/68 (28 %) and 9/68 (13 %) of CRs included final impressions and severity score, respectively. SR writing required significantly (p < 0.001) less time (mean = 308 s; SD \u00b1 60 s) compared to CRs (mean = 458 s; SD \u00b1 72 s). On the other hand, word count was not significantly different (p = 0.059, median = 100 and 106, range = 106\u2013139 and 88\u2013131 for SRs and CRs, respectively). Both clinicians expressed significantly (all p < 0.01) higher scores for SRs compared to CRs in all categories.",
                "Conclusions": "Our study supports the use of chest CT SRs in COVID-19 patients to improve referring physician satisfaction, optimizing reporting time and provide a greater amount and quality of information within the report."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1016/j.ejrad.2021.109621",
                "pmid": "33677417",
                "pmc": "PMC7917443",
                "pub_year": "2021",
                "url": "https://pubmed.ncbi.nlm.nih.gov/33677417/"
            }
        },
        {
            "title": "Burnout and the challenges facing pharmacists during COVID-19: results of a national survey",
            "abstract": {
                "title_content_0": "Background COVID-19 has impacted the psychological wellbeing of healthcare workers and has forced pharmacists to adapt their services. Objective To measure burnout and describe the work and psychosocial factors affecting pharmacists during COVID-19, and to compare males and females. Setting An online survey was distributed to a convenience sample of pharmacists practicing in any setting in Australia during April and June 2020. Method The survey collected demographic data, burnout scores using the validated Maslach Burnout Inventory (MBI), psychosocial and work-related variables using questions adapted from previous surveys. It was tested for readability by a group of pharmacists and academic clinicians before distribution via social media and professional organisations. Main outcome measure Burnout was calculated using mean MBI scores, descriptive statistics were used to report work and psychosocial variables and Pearson\u2019s chi-square compared males and females. Results Overall, 647 responses were analysed. Most participants were female n\u2009=\u2009487 (75.7%) with hospital n\u2009=\u2009269 (42.2%) and community n\u2009=\u2009253 (39.9%) pharmacists well represented. Mean (SD) for emotional exhaustion (possible range 0\u201354) and depersonalisation (possible range 0\u201330) were 28.5 (13.39) and 7.98 (5.64), which were higher (increased burnout) than reported pre-COVID-19. Personal accomplishment (range 0\u201348, lower scores associated with burnout) mean (SD) 36.58 (7.56), was similar to previously reported. Males reported higher depersonalisation indicating more withdrawal and cynicism. Working overtime, medication supply and patient incivility were reported to affect work. Conclusion Pharmacists are experiencing burnout, with work and psychosocial factors affecting them during COVID-19. Knowledge of this and that males experience more depersonalisation is valuable to inform advocacy and interventions to support pharmacists."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1007/s11096-021-01268-5",
                "pmid": "33851288",
                "pmc": "PMC8043093",
                "pub_year": "2021",
                "url": "https://pubmed.ncbi.nlm.nih.gov/33851288/"
            }
        },
        {
            "title": "Readability of Participant Informed Consent Forms and Informational Documents",
            "abstract": {
                "Objective": "To assess the readability of the informed consent forms from the phase 3 COVID-19 vaccine trials conducted in the United States.",
                "Patients and Methods": "English consent forms were used for patients in phase 3 COVID-19 vaccine clinical trials. Consent forms were obtained in October 2020. Using Microsoft Word tools, we analyzed the readability (ie, the ease of reading) of written consent forms and informational documents from phase 3 COVID-19 vaccine clinical trials in the United States from the following manufacturers: AstraZeneca, Moderna, Pfizer, Johnson & Johnson, and Novavax.",
                "Results": "Owing to low readability and several format factors, this study determined that none of the consent forms or informational documents from the recent phase 3 COVID-19 vaccine clinical trials conducted in the United States met readability standards at the recommended 7th grade readability level for the average vaccine research volunteer in any readability category. The average English-speaking vaccine trial volunteer would have great difficulty comprehending the information provided in the consent forms and informational documents. To ensure that study subjects receive and fully comprehend information regarding a clinical study and can provide reliable consent, greater attention should be given to the development and use of simplified consent forms, multimedia formatting, personal discussion, and comprehension assessments."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1016/j.mayocp.2021.05.025",
                "pmid": "34226027",
                "pmc": "PMC8173482",
                "pub_year": "2021",
                "url": "https://pubmed.ncbi.nlm.nih.gov/34226027/"
            }
        },
        {
            "title": "Information on the Internet about clear aligner treatment\u2014an assessment of content, quality, and readability",
            "abstract": {
                "Purpose": "The goal was to evaluate the content, quality, and readability of the information available about clear aligner treatment on the Internet.",
                "Materials and methods": "The search terms \u201caligner,\u201d \u201cclear aligner,\u201d and \u201cInvisalign\u201d (Align Technology, Tempe, AZ, USA) were analyzed in three search engines (Google [Google LLC, Mountain View, CA, USA], Bing [Microsoft, Redmond, WA, USA], and Yahoo [Yahoo, Sunnyvale, CA, USA]). The first 50\u00a0websites for each keyword in each search engine were screened. Duplicate websites, advertisements, links to scientific articles, videos, and other irrelevant websites were excluded. The quality of the remaining websites was analyzed using the DISCERN and Journal of American Medical Association (JAMA) benchmark instruments together with the Health on the Net code (HONcode, Health On the Net Foundation, Geneva, Switzerland). The readability of the websites was evaluated by the Flesch Reading Ease Score (FRES) and Flesch\u2013Kincaid Grade Level (FKGL). Statistical analyses were performed by one-way analysis of variance, Kruskal\u2013Wallis and Fischer\u2019s exact tests, with p\u202f<\u20090.05 accepted to be statistically significant.",
                "Results": "Among 111 evaluated websites, most belonged to multidisciplinary dental clinics (n\u202f=\u200949; 44.2%), followed by aligner companies (n\u202f=\u200926; 23.4%), orthodontists (n\u202f=\u200926; 23.4%), and professional organizations (n\u202f=\u200910; 9%). The mean DISCERN score (sections\u00a01 and\u00a02) for all websites was 29.95/75. The average FRES and FKGL were 55.77 and 9.74, respectively. Professional organization websites had significantly higher DISCERN scores than others (p\u202f<\u20090.001), and together with multidisciplinary dental clinic websites, they showed better compliance with JAMA benchmark criteria. Professional organization websites\u2019 FRES and FKGL were also higher than other websites (p\u202f>\u20090.05).",
                "Conclusions": "Overall, the quality of web-based information about clear aligners was poor and the readability of the data was insufficient. Websites presenting high-quality data with better readability are needed for potential aligner patients."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1007/s00056-021-00331-0",
                "pmid": "34269827",
                "pmc": "PMC8284032",
                "pub_year": "2021",
                "url": "https://pubmed.ncbi.nlm.nih.gov/34269827/"
            }
        },
        {
            "title": "Recognition efficiency of atypical cardiovascular readings on ECG devices through fogged goggles\u2606",
            "abstract": {
                "title_content_0": "In their continuing battle against the COVID-19 pandemic, medical workers in hospitals worldwide need to wear safety glasses and goggles to protect their eyes from the possible transmission of the virus. However, they work for long hours and need to wear a mask and other personal protective equipment, which causes their protective eye wear to fog up. This fogging up of eye wear, in turn, has a substantial impact in the speed and accuracy of reading information on the interface of electrocardiogram (ECG) machines. To gain a better understanding of the extent of the impact, this study experimentally simulates the fogging of protective goggles when viewing the interface with three variables: the degree of fogging of the goggles, brightness of the screen, and color of the font of the cardiovascular readings. This experimental study on the target recognition of digital font is carried out by simulating the interface of an ECG machine and readability of the ECG machine with fogged eye wear. The experimental results indicate that the fogging of the lenses has a significant impact on the recognition speed and the degree of fogging has a significant correlation with the font color and brightness of the screen. With a reduction in screen brightness, its influence on recognition speed shows a v-shaped trend, and the response time is the shortest when the screen brightness is 150\u00a0cd/m2. When eyewear is fogged, yellow and green font colors allow a quicker response with a higher accuracy. On the whole, the subjects show a better performance with the use of green font, but there are inconsistencies. In terms of the interaction among the three variables, the same results are also found and the same conclusion can be made accordingly. This research study can act as a reference for the interface design of medical equipment in events where medical staff wear protective eyewear for a long period of time."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1016/j.displa.2021.102148",
                "pmid": "35013628",
                "pmc": "PMC8730785",
                "pub_year": "2022",
                "url": "https://pubmed.ncbi.nlm.nih.gov/35013628/"
            }
        },
        {
            "title": "A Multimetric Readability Analysis of Online Patient Educational Materials for Submental Fat Reduction",
            "abstract": {
                "Background": "Patients often utilize the Internet to seek information related to their care. This study assesses the readability of online patient educational materials for submental fat reduction.",
                "Methods": "Patient educational materials from the 12 most popular websites related to submental fat reduction were downloaded and assessed for readability grade level using 10 unique scales.",
                "Results": "Analysis of the 12 most popular websites (and corresponding 47 articles) revealed that patient educational materials were written, on average, at an 11th grade reading level. The Flesch Reading Ease score was 48.9 (range 39.8\u201359.2), representing a \u201cdifficult\u201d level of reading. Mean readability grade levels (range 9\u201313th grade for individual websites) were as follows: Coleman-Liau, 11.1; Flesch-Kincaid, 10.8; FORCAST, 10.8; Fry Graph, 10.1; Gunning Fog, 12.7; New Dale-Chall, 10.1; New Fog Count, 11.8; Simple Measure of Gobbledygook, 11.7; Raygor, 6.7. No website was at the 6th grade reading level for patient educational materials recommended by the American Medical Association and National Institutes of Health.",
                "Conclusions": "Online patient educational materials for submental fat reduction are written well above the recommended reading level. Recognition of disparities in health literacy is necessary to enable patients to make informed decisions and become active participants in their own care.",
                "Level of Evidence V": "This journal requires that authors assign a level of evidence to each article. For a full description of these Evidence-Based Medicine ratings, please refer to the Table of Contents or the online Instructions to Authors \u00a0www.springer.com/00266"
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1007/s00266-021-02675-9",
                "pmid": "35037081",
                "pmc": "PMC8761512",
                "pub_year": "2022",
                "url": "https://pubmed.ncbi.nlm.nih.gov/35037081/"
            }
        },
        {
            "title": "Perpetuating Health Disparities of Minority Groups: The Role of U.S. Newspapers in the COVID-19 Pandemic",
            "abstract": {
                "title_content_0": "During the COVID-19 pandemic, news media are expected to play a critical role in reducing health disparities. However, we know little about whether and how disparities in COVID-19 have been covered in national and local U.S. newspapers. This study examined whether minority health gained news attention and whether partisan bias affected related coverage in the early stages of the pandemic. Results indicate that minority groups have been underrepresented in COVID-19 news articles. Left-leaning newspapers were more likely to discuss minorities in COVID-19 news than least biased media. Left-leaning and right-leaning newspapers did not differ in the number of articles mentioning racial/ethnic minorities. COVID-19 news exceeded the average U.S. reading comprehension level and require some college education to understand but did not differ in readability levels among partisan newspapers. Left-leaning newspapers used significantly more medical terms and affiliated scientific facts to describe COVID-19 than right-leaning newspapers. Implications include avoiding potential failures in informing the public (especially the racial/ethnic minorities) essential scientific facts about disease prevention and increasing public trust in health news coverage."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1007/s12552-021-09354-z",
                "pmid": "35079295",
                "pmc": "PMC8777407",
                "pub_year": "2022",
                "url": "https://pubmed.ncbi.nlm.nih.gov/35079295/"
            }
        },
        {
            "title": "Understanding the landscape of web-based medical misinformation about vaccination",
            "abstract": {
                "title_content_0": "Given the high rates of vaccine hesitancy, web-based medical misinformation about vaccination is a serious issue. We sought to understand the nature of Google searches leading to medical misinformation about vaccination, and guided by fuzzy-trace theory, the characteristics of misinformation pages related to comprehension, inference-making, and medical decision-making. We collected data from web pages presenting vaccination information. We assessed whether web pages presented medical misinformation, had an overarching gist, used narrative, and employed emotional appeals. We used Search Engine Optimization tools to determine the number of backlinks from other web pages, monthly Google traffic, and Google Keywords. We used Coh-Metrix to measure readability and Gist Inference Scores (GIS). For medical misinformation web pages, Google traffic and backlinks were heavily skewed with means of 138.8 visitors/month and 805 backlinks per page. Medical misinformation pages were significantly more likely than other vaccine pages to have backlinks from other pages, and significantly less likely to receive at least one visitor from Google searches per month. The top Google searches leading to medical misinformation were \u201cthe truth about vaccinations,\u201d \u201cdangers of vaccination,\u201d and \u201cpro con vaccines.\u201d Most frequently, pages challenged vaccine safety, with 32.7% having an overarching gist, 7.7% presenting narratives, and 17.3% making emotional appeals. Emotional appeals were significantly more common with medical misinformation than other high-traffic vaccination pages. Misinformation pages had a mean readability grade level of 11.5, and a mean GIS of \u2013 0.234. Low GIS scores are a likely barrier to understanding gist, and are the \u201cAchilles\u2019 heel\u201d of misinformation pages."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.3758/s13428-022-01840-5",
                "pmid": "35380412",
                "pmc": "PMC8981888",
                "pub_year": "2022",
                "url": "https://pubmed.ncbi.nlm.nih.gov/35380412/"
            }
        },
        {
            "title": "Investigation of the Readability and Reliability of Online Health Information for Cancer Patients During the Coronavirus Pandemic",
            "abstract": {
                "title_content_0": "For cancer patients undergoing treatment who may be at higher risk of COVID-19, access to high-quality online health information (OHI) may be of particular importance amidst a plethora of harmful medical misinformation online. Therefore, we assessed the readability and quality of OHI available for various cancer types and treatment modalities. Search phrases included \u201ccancer radiation COVID,\u201d \u201ccancer surgery COVID,\u201d \u201ccancer chemotherapy COVID,\u201d and \u201ccancer type COVID,\u201d for the fourteen most common cancer types (e.g., \u201cprostate cancer COVID\u201d and \u201cbreast cancer COVID\u201d), yielding a total of 17 search phrases. The first 20 sources were recorded and analyzed for each keyword, yielding a total of 340 unique sources. For each of these sources, the approximate grade level required to comprehend the text was calculated as a mean of five validated readability scores; subsequently, for the first ten results of each search, the DISCERN tool was manually used to assess quality. Search terms were translated into Spanish and French, and a quality assessment using the Health on the Net Code (HONcode) accreditation was conducted. The median grade level readability for all sources was 13 (IQR 11\u201314). Median DISCERN scores for the 170 sources assessed were 55 out of 75, suggesting good quality. OHI with quality scores below the median DISCERN score had a median readability of 12.5 (IQR 11\u201314) grade reading level vs 14 (IQR 12\u201317) for those above the median DISCERN score (T-test P\u2009<\u20090.0001). Percentages of HONcode-accredited websites were 34.9%, 39.9%, and 38.6% for English, Spanish, and French OHI, respectively. We conclude that efforts are needed to make high-quality OHI available at the appropriate reading level for patients with cancer; such efforts may contribute to the alleviation of disparities in access to healthcare information."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1007/s13187-022-02140-4",
                "pmid": "35469115",
                "pmc": "PMC9038169",
                "pub_year": "2022",
                "url": "https://pubmed.ncbi.nlm.nih.gov/35469115/"
            }
        },
        {
            "title": "An Evaluation of the Quality of COVID-19 Internet Resources for Cancer Patients",
            "abstract": {
                "title_content_0": "Cancer patients may face difficulty evaluating web-based COVID-19 resources in context with their cancer diagnosis. The purpose of this study is to systematically evaluate educational resources available for cancer patients seeking online information on COVID-19 and cancer. The term \u201cCOVID-19 and Cancer\u201d was searched in Google and metasearch engines Yippy and Dogpile. After applying inclusion and exclusion criteria, the results from the 3 lists were systematically combined for a final ranked list. This list was analyzed using a validated structured rating tool with respect to accountability, interactivity, organization, readability, and content coverage and accuracy. Three hundred ninety-eight websites were identified, and 37 websites were included for analysis. Only 43% of sites disclosed authorship, 24% cited sources, and 32% were updated within 3\u00a0months of the search date. Fifty-four percent of websites had high school readability (8.0\u201312.0), 43% were at university level or above, and no websites demonstrated the recommended reading level for health information for the public (<\u20096.0). Topics most discussed were special considerations for cancer patients during COVID-19 (84%) and COVID-19 risk factors (73%). Topics least covered were COVID-19 incidence/prevalence (5%) and prognosis (8%). There is some COVID-19 information for cancer patients available online, but quality is variable. Healthcare professionals may direct cancer patients to the most reliable COVID-19 and cancer websites shown in this study and results may be helpful when designing future online health information resources.",
                "Supplementary Information": "The online version contains supplementary material available at 10.1007/s13187-022-02182-8."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1007/s13187-022-02182-8",
                "pmid": "35726079",
                "pmc": "PMC9208828",
                "pub_year": "2022",
                "url": "https://pubmed.ncbi.nlm.nih.gov/35726079/"
            }
        },
        {
            "title": "COVID-19 vaccinations: An overview of the Italian national health system's online communication from a citizen perspective",
            "abstract": {
                "title_content_0": "COVID-19 vaccine hesitancy is still widespread. During the pandemic, the internet has been the preferred channel for health-related information, especially for less-educated citizens who tend to be the most hesitant about vaccination. A well-structured web communication strategy could help both to overcome vaccine hesitancy and to ensure equity in healthcare service access. This study investigated how the various regional and local health authorities in Italy used their institutional websites to inform users about COVID-19 vaccinations between March and April 2021. We browsed 129 institutional websites, checking the availability, quality and quantity, actionability and readability of information using a literature-based common grid. Descriptive statistics and statistical tests were performed. The online public dissemination of COVID-19 vaccination information in Italy was fragmented, both across and within regions. The side effects of vaccinations, were often not reported on the websites, thus missing an opportunity to enhance vaccination uptake. More focus should also be placed on readability, since readability indexes showed that they were difficult to understand. Our research revealed that several actions could be implemented to enhance online communication on COVID-19 vaccination. For instance, simplifying texts can make them more understandable and the information reported actionable."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1016/j.healthpol.2022.08.001",
                "pmid": "35987784",
                "pmc": "PMC9349029",
                "pub_year": "2022",
                "url": "https://pubmed.ncbi.nlm.nih.gov/35987784/"
            }
        },
        {
            "title": "Readability of online monkeypox patient education materials: Improved recognition of health literacy is needed for dissemination of infectious disease information",
            "abstract": {
                "Background": "Health literacy is key to navigating the current global epidemic of misinformation and inaccuracy relating to healthcare. The American Medical Association (AMA) suggests health information should be written at the level of American sixth grade. With the monkeypox outbreak being declared a Public Health Emergency of International Concern (PHEIC) in July 2022, we sought to assess the readability of online patient education materials (PEMs) relating to monkeypox to see if they are at the target level of readability.",
                "Methods": "A search was conducted on Google.com using the search term \u2018Monkeypox\u2019. The top 50 English language webpages with patient education materials (PEMs) relating to monkeypox were compiled and categorised by country of publication and URL domain. Readability was assessed using five readability tools: Flesch Reading Ease Score (FRES), Flesch-Kincaid Grade Level (FKGL), Gunning Fog Index (GFI), Coleman-Liau Index (CLI), and, Simple Measure of Gobbledygook Index (SMOG). Unpaired t-test for URL domain, and one-way ANOVA for country were performed to determine influence on readability.",
                "Results": "Three of the five tools (FRES, GFI, CLI) identified no webpages that met the target readability score. The FKGL and SMOG tools identified one (2%) and two (4%) webpages respectively that met the target level. County and URL domain demonstrated no influence on readability.",
                "Conclusion": "Online PEMs relating to monkeypox are written above the recommended reading level. Based on the previously established effect of health literacy, this is likely exacerbating health inequalities. This study highlights the need for readability to be considered when publishing online PEMs."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "doi": "10.1016/j.idh.2022.11.002",
                "pmid": "36564245",
                "pmc": "PMC9770025",
                "pub_year": "2022",
                "url": "https://pubmed.ncbi.nlm.nih.gov/36564245/"
            }
        },
        {
            "title": "The Fewer Splits are Better: Deconstructing Readability in Sentence Splitting",
            "abstract": {
                "abstract": "In this work, we focus on sentence splitting, a subfield of text simplification, primarily motivated by an unproven idea that if you divide a sentence into pieces, it should become easier to understand. Our primary goal in this paper is to determine whether this is true. In particular, we ask, does it matter whether we break a sentence into two or three? We report on our findings based on Amazon Mechanical Turk. More specifically, we introduce a Bayesian modeling framework to further investigate to what degree a particular way of splitting the complex sentence affects readability, along with a number of other parameters adopted from diverse perspectives, including clinical linguistics, and cognitive linguistics. The Bayesian modeling experiment provides clear evidence that bisecting the sentence leads to enhanced readability to a degree greater than when we create simplification by trisection."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "bibtext": "@inproceedings{nomoto-2022-fewer,\n    title = \"The Fewer Splits are Better: Deconstructing Readability in Sentence Splitting\",\n    author = \"Nomoto, Tadashi\",\n    editor = \"\\v Stajner, Sanja  and\n      Saggion, Horacio  and\n      Ferr\\'es, Daniel  and\n      Shardlow, Matthew  and\n      Sheang, Kim Cheng  and\n      North, Kai  and\n      Zampieri, Marcos  and\n      Xu, Wei\",\n    booktitle = \"Proceedings of the Workshop on Text Simplification, Accessibility, and Readability (TSAR-2022)\",\n    month = dec,\n    year = \"2022\",\n    address = \"Abu Dhabi, United Arab Emirates (Virtual)\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.tsar-1.1/\",\n    doi = \"10.18653/v1/2022.tsar-1.1\",\n    pages = \"1--11\"\n}",
                "anthology_id": "2022.tsar-1.1",
                "url": "https://aclanthology.org/2022.tsar-1.1",
                "pub_year": "2022"
            }
        },
        {
            "title": "Patient-friendly Clinical Notes: Towards a new Text Simplification Dataset",
            "abstract": {
                "abstract": "Automatic text simplification can help patients to better understand their own clinical notes. A major hurdle for the development of clinical text simplification methods is the lack of high quality resources. We report ongoing efforts in creating a parallel dataset of professionally simplified clinical notes. Currently, this corpus consists of 851 document-level simplifications of German pathology reports. We highlight characteristics of this dataset and establish first baselines for paragraph-level simplification."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "bibtext": "@inproceedings{trienes-etal-2022-patient,\n    title = \"Patient-friendly Clinical Notes: Towards a new Text Simplification Dataset\",\n    author = {Trienes, Jan  and\n      Schl\\\"otterer, J\\\"org  and\n      Schildhaus, Hans-Ulrich  and\n      Seifert, Christin},\n    editor = \"\\v Stajner, Sanja  and\n      Saggion, Horacio  and\n      Ferr\\'es, Daniel  and\n      Shardlow, Matthew  and\n      Sheang, Kim Cheng  and\n      North, Kai  and\n      Zampieri, Marcos  and\n      Xu, Wei\",\n    booktitle = \"Proceedings of the Workshop on Text Simplification, Accessibility, and Readability (TSAR-2022)\",\n    month = dec,\n    year = \"2022\",\n    address = \"Abu Dhabi, United Arab Emirates (Virtual)\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.tsar-1.3/\",\n    doi = \"10.18653/v1/2022.tsar-1.3\",\n    pages = \"19--27\"\n}",
                "anthology_id": "2022.tsar-1.3",
                "url": "https://aclanthology.org/2022.tsar-1.3",
                "pub_year": "2022"
            }
        },
        {
            "title": "Benchmarking the Simplification of Dutch Municipal Text",
            "abstract": {
                "abstract": "Text simplification (TS) makes written information more accessible to all people, especially those with cognitive or language impairments. Despite much progress in TS due to advances in NLP technology, the bottleneck issue of lack of data for low-resource languages persists. Dutch is one of these languages that lack a monolingual simplification corpus. In this paper, we use English as a pivot language for the simplification of Dutch medical and municipal text. We experiment with augmenting training data and corpus choice for this pivot-based approach. We compare the results to a baseline and an end-to-end LLM approach using the GPT 3.5 Turbo model. Our evaluation shows that, while we can substantially improve the results of the pivot pipeline, the zero-shot end-to-end GPT-based simplification performs better on all metrics. Our work shows how an existing pivot-based pipeline can be improved for simplifying Dutch medical text. Moreover, we provide baselines for the comparison in the domain of Dutch municipal text and make our corresponding evaluation dataset publicly available."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "bibtext": "@inproceedings{vlantis-etal-2024-benchmarking,\n    title = \"Benchmarking the Simplification of {D}utch Municipal Text\",\n    author = \"Vlantis, Daniel  and\n      Gornishka, Iva  and\n      Wang, Shuai\",\n    editor = \"Calzolari, Nicoletta  and\n      Kan, Min-Yen  and\n      Hoste, Veronique  and\n      Lenci, Alessandro  and\n      Sakti, Sakriani  and\n      Xue, Nianwen\",\n    booktitle = \"Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)\",\n    month = may,\n    year = \"2024\",\n    address = \"Torino, Italia\",\n    publisher = \"ELRA and ICCL\",\n    url = \"https://aclanthology.org/2024.lrec-main.199/\",\n    pages = \"2217--2226\"\n}",
                "anthology_id": "2024.lrec-main.199",
                "url": "https://aclanthology.org/2024.lrec-main.199",
                "pub_year": "2024"
            }
        },
        {
            "title": "Replace, Paraphrase or Fine-tune? Evaluating Automatic Simplification for Medical Texts in Spanish",
            "abstract": {
                "abstract": "Patients can not always completely understand medical documents given the myriad of technical terms they contain. Automatic text simplification techniques can help, but they must guarantee that the content is transmitted rigorously and not creating wrong information. In this work, we tested: 1) lexicon-based simplification approaches, using a Spanish lexicon of technical and laymen terms collected for this task (SimpMedLexSp); 2) deep-learning (DL) based methods, with BART-based and prompt-learning-based models; and 3) a combination of both techniques. As a test set, we used 5000 parallel (technical and laymen) sentence pairs: 3800 manually aligned sentences from the CLARA-MeD corpus; and 1200 sentences from clinical trials simplified by linguists. We conducted a quantitative evaluation with standard measures (BLEU, ROUGE and SARI) and a human evaluation, in which eleven subjects scored the simplification output of several methods. In our experiments, the lexicon improved the quantitative results when combined with the DL models. The simplified sentences using only the lexicon were assessed with the highest scores regarding semantic adequacy; however, their fluency needs to be improved. The prompt-method had similar ratings in this aspect and in simplification. We make available the models and the data to reproduce our results."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "bibtext": "@inproceedings{campillos-llanos-etal-2024-replace,\n    title = \"Replace, Paraphrase or Fine-tune? Evaluating Automatic Simplification for Medical Texts in {S}panish\",\n    author = \"Campillos-Llanos, Leonardo  and\n      Terroba, Ana Rosa  and\n      Bartolom\\'e, Roc\\'\\i o  and\n      Valverde-Mateos, Ana  and\n      Gonz\\'alez, Cristina  and\n      Capllonch-Carri\\'on, Adri\\'an  and\n      Heras, Jonathan\",\n    editor = \"Calzolari, Nicoletta  and\n      Kan, Min-Yen  and\n      Hoste, Veronique  and\n      Lenci, Alessandro  and\n      Sakti, Sakriani  and\n      Xue, Nianwen\",\n    booktitle = \"Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)\",\n    month = may,\n    year = \"2024\",\n    address = \"Torino, Italia\",\n    publisher = \"ELRA and ICCL\",\n    url = \"https://aclanthology.org/2024.lrec-main.1216/\",\n    pages = \"13929--13945\"\n}",
                "anthology_id": "2024.lrec-main.1216",
                "url": "https://aclanthology.org/2024.lrec-main.1216",
                "pub_year": "2024"
            }
        },
        {
            "title": "ELiRF-VRAIN at BioLaySumm: Boosting Lay Summarization Systems Performance with Ranking Models",
            "abstract": {
                "abstract": "This paper presents our contribution to the BioLaySumm 2024 shared task of the 23rd BioNLP Workshop. The task is to create a lay summary, given a biomedical research article and its technical summary. As the input to the system could be large, a Longformer Encoder-Decoder (LED) has been used. We continuously pre-trained a general domain LED model with biomedical data to adapt it to this specific domain. In the pre-training phase, several pre-training tasks were aggregated to inject linguistic knowledge and increase the abstractivity of the generated summaries. Since the distribution of samples between the two datasets, eLife and PLOS, is unbalanced, we fine-tuned two models: one for eLife and another for PLOS. To increase the quality of the lay summaries of the system, we developed a regression model that helps us rank the summaries generated by the summarization models. This regression model predicts the quality of the summary in three different aspects: Relevance, Readability, and Factuality. We present the results of our models and a study to measure the ranking capabilities of the regression model."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "bibtext": "@inproceedings{ahuir-etal-2024-elirf,\n    title = \"{EL}i{RF}-{VRAIN} at {B}io{L}ay{S}umm: Boosting Lay Summarization Systems Performance with Ranking Models\",\n    author = \"Ahuir, Vicent  and\n      Torres, Diego  and\n      Segarra, Encarna  and\n      Hurtado, Llu\\'\\i s-F.\",\n    editor = \"Demner-Fushman, Dina  and\n      Ananiadou, Sophia  and\n      Miwa, Makoto  and\n      Roberts, Kirk  and\n      Tsujii, Junichi\",\n    booktitle = \"Proceedings of the 23rd Workshop on Biomedical Natural Language Processing\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.bionlp-1.68/\",\n    doi = \"10.18653/v1/2024.bionlp-1.68\",\n    pages = \"755--761\"\n}",
                "anthology_id": "2024.bionlp-1.68",
                "url": "https://aclanthology.org/2024.bionlp-1.68",
                "pub_year": "2024"
            }
        },
        {
            "title": "BioLay_AK_SS at BioLaySumm: Domain Adaptation by Two-Stage Fine-Tuning of Large Language Models used for Biomedical Lay Summary Generation",
            "abstract": {
                "abstract": "Lay summarization is essential but challenging, as it simplifies scientific information for non-experts and keeps them updated with the latest scientific knowledge. In our participation in the Shared Task: Lay Summarization of Biomedical Research Articles @ BioNLP Workshop (Goldsack et al., 2024), ACL 2024, we conducted a comprehensive evaluation on abstractive summarization of biomedical literature using Large Language Models (LLMs) and assessed the performance using ten metrics across three categories: relevance, readability, and factuality, using eLife and PLOS datasets provided by the organizers. We developed a two-stage framework for lay summarization of biomedical scientific articles. In the first stage, we generated summaries using BART and PEGASUS LLMs by fine-tuning them on the given datasets. In the second stage, we combined the generated summaries and input them to BioBART, and then fine-tuned it on the same datasets. Our findings show that combining general and domain-specific LLMs enhances performance."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "bibtext": "@inproceedings{karotia-susan-2024-biolay,\n    title = \"{B}io{L}ay\\_{AK}\\_{SS} at {B}io{L}ay{S}umm: Domain Adaptation by Two-Stage Fine-Tuning of Large Language Models used for Biomedical Lay Summary Generation\",\n    author = \"Karotia, Akanksha  and\n      Susan, Seba\",\n    editor = \"Demner-Fushman, Dina  and\n      Ananiadou, Sophia  and\n      Miwa, Makoto  and\n      Roberts, Kirk  and\n      Tsujii, Junichi\",\n    booktitle = \"Proceedings of the 23rd Workshop on Biomedical Natural Language Processing\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.bionlp-1.69/\",\n    doi = \"10.18653/v1/2024.bionlp-1.69\",\n    pages = \"762--768\"\n}",
                "anthology_id": "2024.bionlp-1.69",
                "url": "https://aclanthology.org/2024.bionlp-1.69",
                "pub_year": "2024"
            }
        },
        {
            "title": "WisPerMed at BioLaySumm: Adapting Autoregressive Large Language Models for Lay Summarization of Scientific Articles",
            "abstract": {
                "abstract": "This paper details the efforts of the WisPerMed team in the BioLaySumm2024 Shared Task on automatic lay summarization in the biomedical domain, aimed at making scientific publications accessible to non-specialists. Large language models (LLMs), specifically the BioMistral and Llama3 models, were fine-tuned and employed to create lay summaries from complex scientific texts. The summarization performance was enhanced through various approaches, including instruction tuning, few-shot learning, and prompt variations tailored to incorporate specific context information. The experiments demonstrated that fine-tuning generally led to the best performance across most evaluated metrics. Few-shot learning notably improved the models\u2019 ability to generate relevant and factually accurate texts, particularly when using a well-crafted prompt. Additionally, a Dynamic Expert Selection (DES) mechanism to optimize the selection of text outputs based on readability and factuality metrics was developed. Out of 54 participants, the WisPerMed team reached the 4th place, measured by readability, factuality, and relevance. Determined by the overall score, our approach improved upon the baseline by approx. 5.5 percentage points and was only approx. 1.5 percentage points behind the first place."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "bibtext": "@inproceedings{pakull-etal-2024-wispermed,\n    title = \"{W}is{P}er{M}ed at {B}io{L}ay{S}umm: Adapting Autoregressive Large Language Models for Lay Summarization of Scientific Articles\",\n    author = {Pakull, Tabea Margareta Grace  and\n      Damm, Hendrik  and\n      Idrissi-Yaghir, Ahmad  and\n      Sch\\\"afer, Henning  and\n      Horn, Peter A.  and\n      Friedrich, Christoph M.},\n    editor = \"Demner-Fushman, Dina  and\n      Ananiadou, Sophia  and\n      Miwa, Makoto  and\n      Roberts, Kirk  and\n      Tsujii, Junichi\",\n    booktitle = \"Proceedings of the 23rd Workshop on Biomedical Natural Language Processing\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.bionlp-1.70/\",\n    doi = \"10.18653/v1/2024.bionlp-1.70\",\n    pages = \"769--779\"\n}",
                "anthology_id": "2024.bionlp-1.70",
                "url": "https://aclanthology.org/2024.bionlp-1.70",
                "pub_year": "2024"
            }
        },
        {
            "title": "RAG-RLRC-LaySum at BioLaySumm: Integrating Retrieval-Augmented Generation and Readability Control for Layman Summarization of Biomedical Texts",
            "abstract": {
                "abstract": "This paper introduces the RAG-RLRC-LaySum framework, designed to make complex biomedical research accessible to laymen through advanced Natural Language Processing (NLP) techniques. Our innovative Retrieval Augmentation Generation (RAG) solution, enhanced by a reranking method, utilizes multiple knowledge sources to ensure the precision and pertinence of lay summaries. Additionally, our Reinforcement Learning for Readability Control (RLRC) strategy improves readability, making scientific content comprehensible to non-specialists. Evaluations using the publicly accessible PLOS and eLife datasets show that our methods surpass Plain Gemini model, demonstrating a 20% increase in readability scores, a 15% improvement in ROUGE-2 relevance scores, and a 10% enhancement in factual accuracy. The RAG-RLRC-LaySum framework effectively democratizes scientific knowledge, enhancing public engagement with biomedical discoveries."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "bibtext": "@inproceedings{ji-etal-2024-rag,\n    title = \"{RAG}-{RLRC}-{L}ay{S}um at {B}io{L}ay{S}umm: Integrating Retrieval-Augmented Generation and Readability Control for Layman Summarization of Biomedical Texts\",\n    author = \"Ji, Yuelyu  and\n      Li, Zhuochun  and\n      Meng, Rui  and\n      Sivarajkumar, Sonish  and\n      Wang, Yanshan  and\n      Yu, Zeshui  and\n      Ji, Hui  and\n      Han, Yushui  and\n      Zeng, Hanyu  and\n      He, Daqing\",\n    editor = \"Demner-Fushman, Dina  and\n      Ananiadou, Sophia  and\n      Miwa, Makoto  and\n      Roberts, Kirk  and\n      Tsujii, Junichi\",\n    booktitle = \"Proceedings of the 23rd Workshop on Biomedical Natural Language Processing\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.bionlp-1.75/\",\n    doi = \"10.18653/v1/2024.bionlp-1.75\",\n    pages = \"810--817\"\n}",
                "anthology_id": "2024.bionlp-1.75",
                "url": "https://aclanthology.org/2024.bionlp-1.75",
                "pub_year": "2024"
            }
        },
        {
            "title": "Team YXZ at BioLaySumm: Adapting Large Language Models for Biomedical Lay Summarization",
            "abstract": {
                "abstract": "Biomedical literature are crucial for disseminating new scientific findings. However, the complexity of these research articles often leads to misinterpretations by the public. To address this urgent issue, we participated in the BioLaySumm task at the 2024 ACL BioNLP workshop, which focuses on automatically simplifying technical biomedical articles for non-technical audiences. We conduct a systematic evaluation of the SOTA large language models (LLMs) in 2024 and found that LLMs can generally achieve better readability scores than smaller models like Bart. Then we iteratively developed techniques of title infusing, K-shot prompting , LLM rewriting and instruction finetuning to further boost readability while balancing factuality and relevance. Notably, our submission achieved the first place in readability at the workshop, and among the top-3 teams with the highest readability scores, we have the best overall rank. Here, we present our experiments and findings on how to effectively adapt LLMs for automatic lay summarization. Our code is available at https://github.com/zhoujieli/biolaysumm."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "bibtext": "@inproceedings{zhou-etal-2024-team,\n    title = \"Team {YXZ} at {B}io{L}ay{S}umm: Adapting Large Language Models for Biomedical Lay Summarization\",\n    author = \"Zhou, Jieli  and\n      Ye, Cheng  and\n      Xu, Pengcheng  and\n      Xin, Hongyi\",\n    editor = \"Demner-Fushman, Dina  and\n      Ananiadou, Sophia  and\n      Miwa, Makoto  and\n      Roberts, Kirk  and\n      Tsujii, Junichi\",\n    booktitle = \"Proceedings of the 23rd Workshop on Biomedical Natural Language Processing\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.bionlp-1.76/\",\n    doi = \"10.18653/v1/2024.bionlp-1.76\",\n    pages = \"818--825\"\n}",
                "anthology_id": "2024.bionlp-1.76",
                "url": "https://aclanthology.org/2024.bionlp-1.76",
                "pub_year": "2024"
            }
        },
        {
            "title": "ReproHum #0892-01: The painful route to consistent results: A reproduction study of human evaluation in NLG",
            "abstract": {
                "abstract": "In spite of the core role human judgement plays in evaluating the performance of NLP systems, the way human assessments are elicited in NLP experiments, and to some extent the nature of human judgement itself, pose challenges to the reliability and validity of human evaluation. In the context of the larger ReproHum project, aimed at running large scale multi-lab reproductions of human judgement, we replicated the understandability assessment by humans on several generated outputs of simplified text described in the paper \u201cNeural Text Simplification of Clinical Letters with a Domain Specific Phrase Table\u201d by Shardlow and Nawaz, appeared in the Proceedings of ACL 2019. Although we had to implement a series of modifications compared to the original study, which were necessary to run our human evaluation on exactly the same data, we managed to collect assessments and compare results with the original study. We obtained results consistent with those of the reference study, confirming their findings. The paper is complete with as much information as possible to foster and facilitate future reproduction."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "bibtext": "@inproceedings{mondella-etal-2024-reprohum,\n    title = \"{R}epro{H}um \\#0892-01: The painful route to consistent results: A reproduction study of human evaluation in {NLG}\",\n    author = \"Mondella, Irene  and\n      Lai, Huiyuan  and\n      Nissim, Malvina\",\n    editor = \"Balloccu, Simone  and\n      Belz, Anya  and\n      Huidrom, Rudali  and\n      Reiter, Ehud  and\n      Sedoc, Joao  and\n      Thomson, Craig\",\n    booktitle = \"Proceedings of the Fourth Workshop on Human Evaluation of NLP Systems (HumEval) @ LREC-COLING 2024\",\n    month = may,\n    year = \"2024\",\n    address = \"Torino, Italia\",\n    publisher = \"ELRA and ICCL\",\n    url = \"https://aclanthology.org/2024.humeval-1.24/\",\n    pages = \"261--268\"\n}",
                "anthology_id": "2024.humeval-1.24",
                "url": "https://aclanthology.org/2024.humeval-1.24",
                "pub_year": "2024"
            }
        },
        {
            "title": "Paragraph-level Simplification of Medical Texts",
            "abstract": {
                "abstract": "We consider the problem of learning to simplify medical texts. This is important because most reliable, up-to-date information in biomedicine is dense with jargon and thus practically inaccessible to the lay audience. Furthermore, manual simplification does not scale to the rapidly growing body of biomedical literature, motivating the need for automated approaches. Unfortunately, there are no large-scale resources available for this task. In this work we introduce a new corpus of parallel texts in English comprising technical and lay summaries of all published evidence pertaining to different clinical topics. We then propose a new metric based on likelihood scores from a masked language model pretrained on scientific texts. We show that this automated measure better differentiates between technical and lay summaries than existing heuristics. We introduce and evaluate baseline encoder-decoder Transformer models for simplification and propose a novel augmentation to these in which we explicitly penalize the decoder for producing \u201cjargon\u201d terms; we find that this yields improvements over baselines in terms of readability."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "bibtext": "@inproceedings{devaraj-etal-2021-paragraph,\n    title = \"Paragraph-level Simplification of Medical Texts\",\n    author = \"Devaraj, Ashwin  and\n      Marshall, Iain  and\n      Wallace, Byron  and\n      Li, Junyi Jessy\",\n    editor = \"Toutanova, Kristina  and\n      Rumshisky, Anna  and\n      Zettlemoyer, Luke  and\n      Hakkani-Tur, Dilek  and\n      Beltagy, Iz  and\n      Bethard, Steven  and\n      Cotterell, Ryan  and\n      Chakraborty, Tanmoy  and\n      Zhou, Yichao\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.395/\",\n    doi = \"10.18653/v1/2021.naacl-main.395\",\n    pages = \"4972--4984\"\n}",
                "anthology_id": "2021.naacl-main.395",
                "url": "https://aclanthology.org/2021.naacl-main.395",
                "pub_year": "2021"
            }
        },
        {
            "title": "Semi-automatic Construction of a Word Complexity Lexicon for Japanese Medical Terminology",
            "abstract": {
                "abstract": "We construct a word complexity lexicon for medical terms in Japanese.To facilitate communication between medical practitioners and patients, medical text simplification is being studied.Medical text simplification is a natural language processing task that paraphrases complex technical terms into expressions that patients can understand.However, in contrast to English, where this task is being actively studied, there are insufficient language resources in Japanese.As a first step in advancing research on medical text simplification in Japanese, we annotate the 370,000 words from a large-scale medical terminology lexicon with a five-point scale of complexity for patients."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "bibtext": "@inproceedings{sugihara-etal-2024-semi,\n    title = \"Semi-automatic Construction of a Word Complexity Lexicon for {J}apanese Medical Terminology\",\n    author = \"Sugihara, Soichiro  and\n      Kajiwara, Tomoyuki  and\n      Ninomiya, Takashi  and\n      Wakamiya, Shoko  and\n      Aramaki, Eiji\",\n    editor = \"Naumann, Tristan  and\n      Ben Abacha, Asma  and\n      Bethard, Steven  and\n      Roberts, Kirk  and\n      Bitterman, Danielle\",\n    booktitle = \"Proceedings of the 6th Clinical Natural Language Processing Workshop\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.clinicalnlp-1.29/\",\n    doi = \"10.18653/v1/2024.clinicalnlp-1.29\",\n    pages = \"329--333\"\n}",
                "anthology_id": "2024.clinicalnlp-1.29",
                "url": "https://aclanthology.org/2024.clinicalnlp-1.29",
                "pub_year": "2024"
            }
        },
        {
            "title": "Benchmarking Automated Clinical Language Simplification: Dataset, Algorithm, and Evaluation",
            "abstract": {
                "abstract": "Patients with low health literacy usually have difficulty understanding medical jargon and the complex structure of professional medical language. Although some studies are proposed to automatically translate expert language into layperson-understandable language, only a few of them focus on both accuracy and readability aspects simultaneously in the clinical domain. Thus, simplification of the clinical language is still a challenging task, but unfortunately, it is not yet fully addressed in previous work. To benchmark this task, we construct a new dataset named MedLane to support the development and evaluation of automated clinical language simplification approaches. Besides, we propose a new model called DECLARE that follows the human annotation procedure and achieves state-of-the-art performance compared with eight strong baselines. To fairly evaluate the performance, we also propose three specific evaluation metrics. Experimental results demonstrate the utility of the annotated MedLane dataset and the effectiveness of the proposed model DECLARE."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "bibtext": "@inproceedings{luo-etal-2022-benchmarking,\n    title = \"Benchmarking Automated Clinical Language Simplification: Dataset, Algorithm, and Evaluation\",\n    author = \"Luo, Junyu  and\n      Lin, Junxian  and\n      Lin, Chi  and\n      Xiao, Cao  and\n      Gui, Xinning  and\n      Ma, Fenglong\",\n    editor = \"Calzolari, Nicoletta  and\n      Huang, Chu-Ren  and\n      Kim, Hansaem  and\n      Pustejovsky, James  and\n      Wanner, Leo  and\n      Choi, Key-Sun  and\n      Ryu, Pum-Mo  and\n      Chen, Hsin-Hsi  and\n      Donatelli, Lucia  and\n      Ji, Heng  and\n      Kurohashi, Sadao  and\n      Paggio, Patrizia  and\n      Xue, Nianwen  and\n      Kim, Seokhwan  and\n      Hahm, Younggyun  and\n      He, Zhong  and\n      Lee, Tony Kyungil  and\n      Santus, Enrico  and\n      Bond, Francis  and\n      Na, Seung-Hoon\",\n    booktitle = \"Proceedings of the 29th International Conference on Computational Linguistics\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"International Committee on Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.coling-1.313/\",\n    pages = \"3550--3562\"\n}",
                "anthology_id": "2022.coling-1.313",
                "url": "https://aclanthology.org/2022.coling-1.313",
                "pub_year": "2022"
            }
        },
        {
            "title": "LARGEMED: A Resource for Identifying and Generating Paraphrases for French Medical Terms",
            "abstract": {
                "abstract": "This article presents a method extending an existing French corpus of paraphrases of medical terms ANONYMOUS with new data from Web archives created during the Covid-19 pandemic. Our method semi-automatically detects new terms and paraphrase markers introducing paraphrases from these Web archives, followed by a manual annotation step to identify paraphrases and their lexical and semantic properties. The extended large corpus LARGEMED could be used for automatic medical text simplification for patients and their families. To automatise data collection, we propose two experiments. The first experiment uses the new LARGEMED dataset to train a binary classifier aiming to detect new sentences containing possible paraphrases. The second experiment aims to use correct paraphrases to train a model for paraphrase generation, by adapting T5 Language Model to the paraphrase generation task using an adversarial algorithm."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "bibtext": "@inproceedings{buhnila-todirascu-2024-largemed,\n    title = \"{LARGEMED}: A Resource for Identifying and Generating Paraphrases for {F}rench Medical Terms\",\n    author = \"Buhnila, Ioana  and\n      Todirascu, Amalia\",\n    editor = \"Nunzio, Giorgio Maria Di  and\n      Vezzani, Federica  and\n      Ermakova, Liana  and\n      Azarbonyad, Hosein  and\n      Kamps, Jaap\",\n    booktitle = \"Proceedings of the Workshop on DeTermIt! Evaluating Text Difficulty in a Multilingual Context @ LREC-COLING 2024\",\n    month = may,\n    year = \"2024\",\n    address = \"Torino, Italia\",\n    publisher = \"ELRA and ICCL\",\n    url = \"https://aclanthology.org/2024.determit-1.14/\",\n    pages = \"141--151\"\n}",
                "anthology_id": "2024.determit-1.14",
                "url": "https://aclanthology.org/2024.determit-1.14",
                "pub_year": "2024"
            }
        },
        {
            "title": "Making Science Simple: Corpora for the Lay Summarisation of Scientific Literature",
            "abstract": {
                "abstract": "Lay summarisation aims to jointly summarise and simplify a given text, thus making its content more comprehensible to non-experts.Automatic approaches for lay summarisation can provide significant value in broadening access to scientific literature, enabling a greater degree of both interdisciplinary knowledge sharing and public understanding when it comes to research findings. However, current corpora for this task are limited in their size and scope, hindering the development of broadly applicable data-driven approaches. Aiming to rectify these issues, we present two novel lay summarisation datasets, PLOS (large-scale) and eLife (medium-scale), each of which contains biomedical journal articles alongside expert-written lay summaries.We provide a thorough characterisation of our lay summaries, highlighting differing levels of readability and abstractivenessbetween datasets that can be leveraged to support the needs of different applications.Finally, we benchmark our datasets using mainstream summarisation approaches and perform a manual evaluation with domain experts, demonstrating their utility and casting light on the key challenges of this task."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "bibtext": "@inproceedings{goldsack-etal-2022-making,\n    title = \"Making Science Simple: Corpora for the Lay Summarisation of Scientific Literature\",\n    author = \"Goldsack, Tomas  and\n      Zhang, Zhihao  and\n      Lin, Chenghua  and\n      Scarton, Carolina\",\n    editor = \"Goldberg, Yoav  and\n      Kozareva, Zornitsa  and\n      Zhang, Yue\",\n    booktitle = \"Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing\",\n    month = dec,\n    year = \"2022\",\n    address = \"Abu Dhabi, United Arab Emirates\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.emnlp-main.724/\",\n    doi = \"10.18653/v1/2022.emnlp-main.724\",\n    pages = \"10589--10604\"\n}",
                "anthology_id": "2022.emnlp-main.724",
                "url": "https://aclanthology.org/2022.emnlp-main.724",
                "pub_year": "2022"
            }
        },
        {
            "title": "Enhancing Biomedical Lay Summarisation with External Knowledge Graphs",
            "abstract": {
                "abstract": "Previous approaches for automatic lay summarisation are exclusively reliant on the source article that, given it is written for a technical audience (e.g., researchers), is unlikely to explicitly define all technical concepts or state all of the background information that is relevant for a lay audience. We address this issue by augmenting eLife, an existing biomedical lay summarisation dataset, with article-specific knowledge graphs, each containing detailed information on relevant biomedical concepts. Using both automatic and human evaluations, we systematically investigate the effectiveness of three different approaches for incorporating knowledge graphs within lay summarisation models, with each method targeting a distinct area of the encoder-decoder model architecture. Our results confirm that integrating graph-based domain knowledge can significantly benefit lay summarisation by substantially increasing the readability of generated text and improving the explanation of technical concepts."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "bibtext": "@inproceedings{goldsack-etal-2023-enhancing,\n    title = \"Enhancing Biomedical Lay Summarisation with External Knowledge Graphs\",\n    author = \"Goldsack, Tomas  and\n      Zhang, Zhihao  and\n      Tang, Chen  and\n      Scarton, Carolina  and\n      Lin, Chenghua\",\n    editor = \"Bouamor, Houda  and\n      Pino, Juan  and\n      Bali, Kalika\",\n    booktitle = \"Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing\",\n    month = dec,\n    year = \"2023\",\n    address = \"Singapore\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.emnlp-main.498/\",\n    doi = \"10.18653/v1/2023.emnlp-main.498\",\n    pages = \"8016--8032\"\n}",
                "anthology_id": "2023.emnlp-main.498",
                "url": "https://aclanthology.org/2023.emnlp-main.498",
                "pub_year": "2023"
            }
        },
        {
            "title": "BLESS: Benchmarking Large Language Models on Sentence Simplification",
            "abstract": {
                "abstract": "We present BLESS, a comprehensive performance benchmark of the most recent state-of-the-art Large Language Models (LLMs) on the task of text simplification (TS). We examine how well off-the-shelf LLMs can solve this challenging task, assessing a total of 44 models, differing in size, architecture, pre-training methods, and accessibility, on three test sets from different domains (Wikipedia, news, and medical) under a few-shot setting. Our analysis considers a suite of automatic metrics, as well as a large-scale quantitative investigation into the types of common edit operations performed by the different models. Furthermore, we perform a manual qualitative analysis on a subset of model outputs to better gauge the quality of the generated simplifications. Our evaluation indicates that the best LLMs, despite not being trained on TS perform comparably with state-of-the-art TS baselines. Additionally, we find that certain LLMs demonstrate a greater range and diversity of edit operations. Our performance benchmark will be available as a resource for the development of future TS methods and evaluation metrics."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "bibtext": "@inproceedings{kew-etal-2023-bless,\n    title = \"{BLESS}: Benchmarking Large Language Models on Sentence Simplification\",\n    author = \"Kew, Tannon  and\n      Chi, Alison  and\n      V\\'asquez-Rodr\\'\\i guez, Laura  and\n      Agrawal, Sweta  and\n      Aumiller, Dennis  and\n      Alva-Manchego, Fernando  and\n      Shardlow, Matthew\",\n    editor = \"Bouamor, Houda  and\n      Pino, Juan  and\n      Bali, Kalika\",\n    booktitle = \"Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing\",\n    month = dec,\n    year = \"2023\",\n    address = \"Singapore\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.emnlp-main.821/\",\n    doi = \"10.18653/v1/2023.emnlp-main.821\",\n    pages = \"13291--13309\"\n}",
                "anthology_id": "2023.emnlp-main.821",
                "url": "https://aclanthology.org/2023.emnlp-main.821",
                "pub_year": "2023"
            }
        },
        {
            "title": "Multilingual Simplification of Medical Texts",
            "abstract": {
                "abstract": "Automated text simplification aims to produce simple versions of complex texts. This task is especially useful in the medical domain, where the latest medical findings are typically communicated via complex and technical articles. This creates barriers for laypeople seeking access to up-to-date medical findings, consequently impeding progress on health literacy. Most existing work on medical text simplification has focused on monolingual settings, with the result that such evidence would be available only in just one language (most often, English). This work addresses this limitation via multilingual simplification, i.e., directly simplifying complex texts into simplified texts in multiple languages. We introduce MultiCochrane, the first sentence-aligned multilingual text simplification dataset for the medical domain in four languages: English, Spanish, French, and Farsi. We evaluate fine-tuned and zero-shot models across these languages with extensive human assessments and analyses. Although models can generate viable simplified texts, we identify several outstanding challenges that this dataset might be used to address."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "bibtext": "@inproceedings{joseph-etal-2023-multilingual,\n    title = \"Multilingual Simplification of Medical Texts\",\n    author = \"Joseph, Sebastian  and\n      Kazanas, Kathryn  and\n      Reina, Keziah  and\n      Ramanathan, Vishnesh  and\n      Xu, Wei  and\n      Wallace, Byron  and\n      Li, Junyi Jessy\",\n    editor = \"Bouamor, Houda  and\n      Pino, Juan  and\n      Bali, Kalika\",\n    booktitle = \"Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing\",\n    month = dec,\n    year = \"2023\",\n    address = \"Singapore\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.emnlp-main.1037/\",\n    doi = \"10.18653/v1/2023.emnlp-main.1037\",\n    pages = \"16662--16692\"\n}",
                "anthology_id": "2023.emnlp-main.1037",
                "url": "https://aclanthology.org/2023.emnlp-main.1037",
                "pub_year": "2023"
            }
        },
        {
            "title": "Chinese Medical Speech Recognition with Punctuated Hypothesis",
            "abstract": {
                "abstract": "Automatic Speech Recognition (ASR) technology presents the possibility for medical professionals to document patient record, diagnosis, postoperative care, patrol records, and etc. that are now done manually. However, earlier research aimed on Chinese medical speech corpus (ChiMeS) has two shortcomings: first is the lack of punctuation, resulting in reduced readability of the output transcript, and second is the poor recognition error rate, affecting its application put to the fields. Accordingly, the contributions of this paper consist of: (1) A punctuated Chinese medical corpus psChiMeS-14 newly annotated from ChiMeS-14, which is the collection of 516 anonymized medical record readouts of 867 minutes long, recorded by 15 professional nursing staff from Taipei Hospital of the Ministry of Health and Welfare. psChiMeS-14 is manually punctuated with: colons, commas, and periods, ready for general end-to-end ASR models. (2) A self-attention based speech recognition solution by conformer networks. Trained by and tested on psChiMeS-14 corpus, the solutions deliver state-of-the-art recognition performance: CER (character error rate) 10.5%, and KER (Keyword error rate) of 13.10%, respectively, which is contrasted to the 15.70% CER and the 22.50% KER by an earlier reported Joint CTC/Attention architecture."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "bibtext": "@inproceedings{chung-etal-2021-chinese,\n    title = \"{C}hinese Medical Speech Recognition with Punctuated Hypothesis\",\n    author = \"Chung, Sheng-Luen  and\n      Fan, Jin-Huan  and\n      Ting, Hsien-Wei\",\n    editor = \"Lee, Lung-Hao  and\n      Chang, Chia-Hui  and\n      Chen, Kuan-Yu\",\n    booktitle = \"Proceedings of the 33rd Conference on Computational Linguistics and Speech Processing (ROCLING 2021)\",\n    month = oct,\n    year = \"2021\",\n    address = \"Taoyuan, Taiwan\",\n    publisher = \"The Association for Computational Linguistics and Chinese Language Processing (ACLCLP)\",\n    url = \"https://aclanthology.org/2021.rocling-1.9/\",\n    pages = \"63--71\"\n}",
                "anthology_id": "2021.rocling-1.9",
                "url": "https://aclanthology.org/2021.rocling-1.9",
                "pub_year": "2021"
            }
        },
        {
            "title": "Evaluation Dataset for Japanese Medical Text Simplification",
            "abstract": {
                "abstract": "We create a parallel corpus for medical text simplification in Japanese, which simplifies medical terms into expressions that patients can understand without effort.While text simplification in the medial domain is strongly desired by society, it is less explored in Japanese because of the lack of language resources.In this study, we build a parallel corpus for Japanese text simplification evaluation in the medical domain using patients\u2019 weblogs.This corpus consists of 1,425 pairs of complex and simple sentences with or without medical terms.To tackle medical text simplification without a training corpus of the corresponding domain, we repurpose a Japanese text simplification model of other domains.Furthermore, we propose a lexically constrained reranking method that allows to avoid technical terms to be output.Experimental results show that our method contributes to achieving higher simplification performance in the medical domain."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "bibtext": "@inproceedings{horiguchi-etal-2024-evaluation,\n    title = \"Evaluation Dataset for {J}apanese Medical Text Simplification\",\n    author = \"Horiguchi, Koki  and\n      Kajiwara, Tomoyuki  and\n      Arase, Yuki  and\n      Ninomiya, Takashi\",\n    editor = \"Cao, Yang (Trista)  and\n      Papadimitriou, Isabel  and\n      Ovalle, Anaelia  and\n      Zampieri, Marcos  and\n      Ferraro, Francis  and\n      Swayamdipta, Swabha\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 4: Student Research Workshop)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-srw.23/\",\n    doi = \"10.18653/v1/2024.naacl-srw.23\",\n    pages = \"219--225\"\n}",
                "anthology_id": "2024.naacl-srw.23",
                "url": "https://aclanthology.org/2024.naacl-srw.23",
                "pub_year": "2024"
            }
        },
        {
            "title": "Towards AI-supported Health Communication in Plain Language: Evaluating Intralingual Machine Translation of Medical Texts",
            "abstract": {
                "abstract": "In this paper, we describe results of a study on evaluation of intralingual machine translation. The study focuses on machine translations of medical texts into Plain German. The automatically simplified texts were compared with manually simplified texts (i.e., simplified by human experts) as well as with the underlying, unsimplified source texts. We analyse the quality of the translations based on different criteria, such as correctness, readability, and syntactic complexity. The study revealed that the machine translations were easier to read than the source texts, but contained a higher number of complex syntactic relations than the human translations. Furthermore, we identified various types of mistakes. These included not only grammatical mistakes but also content-related mistakes that resulted, for example, from mistranslations of grammatical structures, ambiguous words or numbers, omissions of relevant prefixes or negation, and incorrect explanations of technical terms."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "bibtext": "@inproceedings{deilen-etal-2024-towards,\n    title = \"Towards {AI}-supported Health Communication in Plain Language: Evaluating Intralingual Machine Translation of Medical Texts\",\n    author = {Deilen, Silvana  and\n      Lapshinova-Koltunski, Ekaterina  and\n      Hern\\'andez Garrido, Sergio  and\n      Maa\\ss , Christiane  and\n      H\\\"orner, Julian  and\n      Theel, Vanessa  and\n      Ziemer, Sophie},\n    editor = \"Demner-Fushman, Dina  and\n      Ananiadou, Sophia  and\n      Thompson, Paul  and\n      Ondov, Brian\",\n    booktitle = \"Proceedings of the First Workshop on Patient-Oriented Language Processing (CL4Health) @ LREC-COLING 2024\",\n    month = may,\n    year = \"2024\",\n    address = \"Torino, Italia\",\n    publisher = \"ELRA and ICCL\",\n    url = \"https://aclanthology.org/2024.cl4health-1.6/\",\n    pages = \"44--53\"\n}",
                "anthology_id": "2024.cl4health-1.6",
                "url": "https://aclanthology.org/2024.cl4health-1.6",
                "pub_year": "2024"
            }
        },
        {
            "title": "Large Language Models as Drug Information Providers for Patients",
            "abstract": {
                "abstract": "Recently, a significant interest has arisen about the application of Large Language Models (LLMs) in medical settings to enhance various aspects of healthcare. Particularly, the application of such models to improve knowledge access for both clinicians and patients seems very promising but still far from perfect. In this paper, we present a preliminary evaluation of LLMs as drug information providers to support patients in drug administration. We focus on posology, namely dosage quantity and prescription, contraindications and adverse drug reactions and run an experiment on the Italian language to assess both the trustworthiness of the outputs and their readability. The results show that different types of errors affect the LLM answers. In some cases, the model does not recognize the drug name, due to the presence of synonymous words, or it provides untrustworthy information, caused by intrinsic hallucinations. Overall, the complexity of the language is lower and this could contribute to make medical information more accessible to lay people."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "bibtext": "@inproceedings{giordano-di-buono-2024-large,\n    title = \"Large Language Models as Drug Information Providers for Patients\",\n    author = \"Giordano, Luca  and\n      di Buono, Maria Pia\",\n    editor = \"Demner-Fushman, Dina  and\n      Ananiadou, Sophia  and\n      Thompson, Paul  and\n      Ondov, Brian\",\n    booktitle = \"Proceedings of the First Workshop on Patient-Oriented Language Processing (CL4Health) @ LREC-COLING 2024\",\n    month = may,\n    year = \"2024\",\n    address = \"Torino, Italia\",\n    publisher = \"ELRA and ICCL\",\n    url = \"https://aclanthology.org/2024.cl4health-1.7/\",\n    pages = \"54--63\"\n}",
                "anthology_id": "2024.cl4health-1.7",
                "url": "https://aclanthology.org/2024.cl4health-1.7",
                "pub_year": "2024"
            }
        },
        {
            "title": "MedReadMe: A Systematic Study for Fine-grained Sentence Readability in Medical Domain",
            "abstract": {
                "abstract": "Medical texts are notoriously challenging to read. Properly measuring their readability is the first step towards making them more accessible. Here, we present the first systematic study on fine-grained readability measurements in the medical domain, at both sentence-level and span-level. We first introduce a new dataset MedReadMe, which consists of manually annotated readability ratings and fine-grained complex span annotation for 4,520 sentences, featuring two novel \u201cGoogle-Easy\u201d and \u201cGoogle-Hard\u201d categories. It supports our quantitative analysis, which covers 650 linguistic features and additional complex span features, to answer \u201cwhy medical sentences are so hard.\u201d Enabled by our high-quality annotation, we benchmark several state-of-the-art sentence-level readability metrics, including unsupervised, supervised, and prompting-based methods using recently developed large language models (LLMs). Informed by our fine-grained complex span annotation, we find that adding a single feature, capturing the number of jargon spans, into existing readability formulas can significantly improve their correlation with human judgments, and also make them more stable. We will publicly release data and code."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "bibtext": "@inproceedings{jiang-xu-2024-medreadme,\n    title = \"{M}ed{R}ead{M}e: A Systematic Study for Fine-grained Sentence Readability in Medical Domain\",\n    author = \"Jiang, Chao  and\n      Xu, Wei\",\n    editor = \"Al-Onaizan, Yaser  and\n      Bansal, Mohit  and\n      Chen, Yun-Nung\",\n    booktitle = \"Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2024\",\n    address = \"Miami, Florida, USA\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.emnlp-main.958/\",\n    doi = \"10.18653/v1/2024.emnlp-main.958\",\n    pages = \"17293--17319\"\n}",
                "anthology_id": "2024.emnlp-main.958",
                "url": "https://aclanthology.org/2024.emnlp-main.958",
                "pub_year": "2024"
            }
        },
        {
            "title": "Readability Controllable Biomedical Document Summarization",
            "abstract": {
                "abstract": "Different from general documents, it is recognised that the ease with which people can understand a biomedical text is eminently varied, owing to the highly technical nature of biomedical documents and the variance of readers\u2019 domain knowledge. However, existing biomedical document summarization systems have paid little attention to readability control, leaving users with summaries that are incompatible with their levels of expertise.In recognition of this urgent demand, we introduce a new task of readability controllable summarization for biomedical documents, which aims to recognise users\u2019 readability demands and generate summaries that better suit their needs: technical summaries for experts and plain language summaries (PLS) for laymen.To establish this task, we construct a corpus consisting of biomedical papers with technical summaries and PLSs written by the authors, and benchmark multiple advanced controllable abstractive and extractive summarization models based on pre-trained language models (PLMs) with prevalent controlling and generation techniques.Moreover, we propose a novel masked language model (MLM) based metric and its variant to effectively evaluate the readability discrepancy between lay and technical summaries.Experimental results from automated and human evaluations show that though current control techniques allow for a certain degree of readability adjustment during generation, the performance of existing controllable summarization methods is far from desirable in this task."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "bibtext": "@inproceedings{luo-etal-2022-readability,\n    title = \"Readability Controllable Biomedical Document Summarization\",\n    author = \"Luo, Zheheng  and\n      Xie, Qianqian  and\n      Ananiadou, Sophia\",\n    editor = \"Goldberg, Yoav  and\n      Kozareva, Zornitsa  and\n      Zhang, Yue\",\n    booktitle = \"Findings of the Association for Computational Linguistics: EMNLP 2022\",\n    month = dec,\n    year = \"2022\",\n    address = \"Abu Dhabi, United Arab Emirates\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.findings-emnlp.343/\",\n    doi = \"10.18653/v1/2022.findings-emnlp.343\",\n    pages = \"4667--4680\"\n}",
                "anthology_id": "2022.findings-emnlp.343",
                "url": "https://aclanthology.org/2022.findings-emnlp.343",
                "pub_year": "2022"
            }
        },
        {
            "title": "Document-level Text Simplification with Coherence Evaluation",
            "abstract": {
                "abstract": "We present a coherence-aware evaluation of document-level Text Simplification (TS), an approach that has not been considered in TS so far. We improve current TS sentence-based models to support a multi-sentence setting and the implementation of a state-of-the-art neural coherence model for simplification quality assessment. We enhanced English sentence simplification neural models for document-level simplification using 136,113 paragraph-level samples from both the general and medical domains to generate multiple sentences. Additionally, we use document-level simplification, readability and coherence metrics for evaluation. Our contributions include the introduction of coherence assessment into simplification evaluation with the automatic evaluation of 34,052 simplifications, a fine-tuned state-of-the-art model for document-level simplification, a coherence-based analysis of our results and a human evaluation of 300 samples that demonstrates the challenges encountered when moving towards document-level simplification."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "bibtext": "@inproceedings{vasquez-rodriguez-etal-2023-document,\n    title = \"Document-level Text Simplification with Coherence Evaluation\",\n    author = \"V\\'asquez-Rodr\\'\\i guez, Laura  and\n      Shardlow, Matthew  and\n      Przyby\\l a, Piotr  and\n      Ananiadou, Sophia\",\n    editor = \"\\v Stajner, Sanja  and\n      Saggio, Horacio  and\n      Shardlow, Matthew  and\n      Alva-Manchego, Fernando\",\n    booktitle = \"Proceedings of the Second Workshop on Text Simplification, Accessibility and Readability\",\n    month = sep,\n    year = \"2023\",\n    address = \"Varna, Bulgaria\",\n    publisher = \"INCOMA Ltd., Shoumen, Bulgaria\",\n    url = \"https://aclanthology.org/2023.tsar-1.9/\",\n    pages = \"85--101\"\n}",
                "anthology_id": "2023.tsar-1.9",
                "url": "https://aclanthology.org/2023.tsar-1.9",
                "pub_year": "2023"
            }
        },
        {
            "title": "InfoLossQA: Characterizing and Recovering Information Loss in Text Simplification",
            "abstract": {
                "abstract": "Text simplification aims to make technical texts more accessible to laypeople but often results in deletion of information and vagueness. This work proposes InfoLossQA, a framework to characterize and recover simplification-induced information loss in form of question-and-answer (QA) pairs. Building on the theory of Questions Under Discussion, the QA pairs are designed to help readers deepen their knowledge of a text. First, we collect a dataset of 1,000 linguist-curated QA pairs derived from 104 LLM simplifications of English medical study abstracts. Our analyses of this data reveal that information loss occurs frequently, and that the QA pairs give a high-level overview of what information was lost. Second, we devise two methods for this task: end-to-end prompting of open-source and commercial language models, and a natural language inference pipeline. With a novel evaluation framework considering the correctness of QA pairs and their linguistic suitability, our expert evaluation reveals that models struggle to reliably identify information loss and applying similar standards as humans at what constitutes information loss."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "bibtext": "@inproceedings{trienes-etal-2024-infolossqa,\n    title = \"{I}nfo{L}oss{QA}: Characterizing and Recovering Information Loss in Text Simplification\",\n    author = {Trienes, Jan  and\n      Joseph, Sebastian  and\n      Schl\\\"otterer, J\\\"org  and\n      Seifert, Christin  and\n      Lo, Kyle  and\n      Xu, Wei  and\n      Wallace, Byron  and\n      Li, Junyi Jessy},\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.234/\",\n    doi = \"10.18653/v1/2024.acl-long.234\",\n    pages = \"4263--4294\"\n}",
                "anthology_id": "2024.acl-long.234",
                "url": "https://aclanthology.org/2024.acl-long.234",
                "pub_year": "2024"
            }
        },
        {
            "title": "Identifying Medical Paraphrases in Scientific versus Popularization Texts in French for Laypeople Understanding",
            "abstract": {
                "abstract": "Scientific medical terms are difficult to understand for laypeople due to their technical formulas and etymology. Understanding medical concepts is important for laypeople as personal and public health is a lifelong concern. In this study, we present our methodology for building a French lexical resource annotated with paraphrases for the simplification of monolexical and multiword medical terms. In order to find medical paraphrases, we automatically searched for medical terms and specific lexical markers that help to paraphrase them. We annotated the medical terms, the paraphrase markers, and the paraphrase. We analysed the lexical relations and semantico-pragmatic functions that exists between the term and its paraphrase. We computed statistics for the medical paraphrase corpus, and we evaluated the readability of the medical paraphrases for a non-specialist coder. Our results show that medical paraphrases from popularization texts are easier to understand (62.66%) than paraphrases extracted from scientific texts (50%)."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "bibtext": "@inproceedings{buhnila-2022-identifying,\n    title = \"Identifying Medical Paraphrases in Scientific versus Popularization Texts in {F}rench for Laypeople Understanding\",\n    author = \"Buhnila, Ioana\",\n    editor = \"Cohan, Arman  and\n      Feigenblat, Guy  and\n      Freitag, Dayne  and\n      Ghosal, Tirthankar  and\n      Herrmannova, Drahomira  and\n      Knoth, Petr  and\n      Lo, Kyle  and\n      Mayr, Philipp  and\n      Shmueli-Scheuer, Michal  and\n      de Waard, Anita  and\n      Wang, Lucy Lu\",\n    booktitle = \"Proceedings of the Third Workshop on Scholarly Document Processing\",\n    month = oct,\n    year = \"2022\",\n    address = \"Gyeongju, Republic of Korea\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.sdp-1.8/\",\n    pages = \"69--79\"\n}",
                "anthology_id": "2022.sdp-1.8",
                "url": "https://aclanthology.org/2022.sdp-1.8",
                "pub_year": "2022"
            }
        },
        {
            "title": "GRASUM at BioLaySumm Task 1: Background Knowledge Grounding for Readable, Relevant, and Factual Biomedical Lay Summaries",
            "abstract": {
                "abstract": "Communication of scientific findings to the public is important for keeping non-experts informed of developments such as life-saving medical treatments. However, generating readable lay summaries from scientific documents is challenging, and currently, these summaries suffer from critical factual errors. One popular intervention for improving factuality is using additional external knowledge to provide factual grounding. However, it is unclear how these grounding sources should be retrieved, selected, or integrated, and how supplementary grounding documents might affect the readability or relevance of the generated summaries. We develop a simple method for selecting grounding sources and integrating them with source documents. We then use the BioLaySum summarization dataset to evaluate the effects of different grounding sources on summary quality. We found that grounding source documents improves the relevance and readability of lay summaries but does not improve factuality of lay summaries. This continues to be true in zero-shot summarization settings where we hypothesized that grounding might be even more important for factual lay summaries."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "bibtext": "@inproceedings{rosati-2023-grasum,\n    title = \"{GRASUM} at {B}io{L}ay{S}umm Task 1: Background Knowledge Grounding for Readable, Relevant, and Factual Biomedical Lay Summaries\",\n    author = \"Rosati, Domenic\",\n    editor = \"Demner-fushman, Dina  and\n      Ananiadou, Sophia  and\n      Cohen, Kevin\",\n    booktitle = \"The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.bionlp-1.46/\",\n    doi = \"10.18653/v1/2023.bionlp-1.46\",\n    pages = \"483--490\"\n}",
                "anthology_id": "2023.bionlp-1.46",
                "url": "https://aclanthology.org/2023.bionlp-1.46",
                "pub_year": "2023"
            }
        },
        {
            "title": "APTSumm at BioLaySumm Task 1: Biomedical Breakdown, Improving Readability by Relevancy Based Selection",
            "abstract": {
                "abstract": "In this paper we tackle a lay summarization task which aims to produce lay-summary of biomedical articles. BioLaySumm in the BioNLP Workshop at ACL 2023 (Goldsack et al., 2023), has presented us with this lay summarization task for biomedical articles. Our proposed models provide a three-step abstractive approach for summarizing biomedical articles. Our methodology involves breaking down the original document into distinct sections, generating candidate summaries for each subsection, then finally re-ranking and selecting the top performing paragraph for each section. We run ablation studies to establish that each step in our pipeline is critical for improvement in the quality of lay summary. This model achieved the second-highest rank in terms of readability scores (Luo et al., 2022). Our work distinguishes itself from previous studies by not only considering the content of the paper but also its structure, resulting in more coherent and comprehensible lay summaries. We hope that our model for generating lay summaries of biomedical articles will be a useful resource for individuals across various domains, including academia, industry, and healthcare, who require rapid comprehension of key scientific research."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "bibtext": "@inproceedings{poornash-etal-2023-aptsumm,\n    title = \"{APTS}umm at {B}io{L}ay{S}umm Task 1: Biomedical Breakdown, Improving Readability by Relevancy Based Selection\",\n    author = \"Poornash, A.s.  and\n      Deshmukh, Atharva  and\n      Sharma, Archit  and\n      Saha, Sriparna\",\n    editor = \"Demner-fushman, Dina  and\n      Ananiadou, Sophia  and\n      Cohen, Kevin\",\n    booktitle = \"The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.bionlp-1.61/\",\n    doi = \"10.18653/v1/2023.bionlp-1.61\",\n    pages = \"579--585\"\n}",
                "anthology_id": "2023.bionlp-1.61",
                "url": "https://aclanthology.org/2023.bionlp-1.61",
                "pub_year": "2023"
            }
        },
        {
            "title": "NCUEE-NLP at BioLaySumm Task 2: Readability-Controlled Summarization of Biomedical Articles Using the PRIMERA Models",
            "abstract": {
                "abstract": "This study describes the model design of the NCUEE-NLP system for BioLaySumm Task 2 at the BioNLP 2023 workshop. We separately fine-tune pretrained PRIMERA models to independently generate technical abstracts and lay summaries of biomedical articles. A total of seven evaluation metrics across three criteria were used to compare system performance. Our best submission was ranked first for relevance, second for readability, and fourth for factuality, tying first for overall performance."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "bibtext": "@inproceedings{chen-etal-2023-ncuee,\n    title = \"{NCUEE}-{NLP} at {B}io{L}ay{S}umm Task 2: Readability-Controlled Summarization of Biomedical Articles Using the {PRIMERA} Models\",\n    author = \"Chen, Chao-Yi  and\n      Yang, Jen-Hao  and\n      Lee, Lung-Hao\",\n    editor = \"Demner-fushman, Dina  and\n      Ananiadou, Sophia  and\n      Cohen, Kevin\",\n    booktitle = \"The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.bionlp-1.62/\",\n    doi = \"10.18653/v1/2023.bionlp-1.62\",\n    pages = \"586--591\"\n}",
                "anthology_id": "2023.bionlp-1.62",
                "url": "https://aclanthology.org/2023.bionlp-1.62",
                "pub_year": "2023"
            }
        },
        {
            "title": "Pathology Dynamics at BioLaySumm: the trade-off between Readability, Relevance, and Factuality in Lay Summarization",
            "abstract": {
                "abstract": "Lay summarization aims to simplify complex scientific information for non-expert audiences. This paper investigates the trade-off between readability and relevance in the lay summarization of long biomedical documents. We introduce a two-stage framework that attains the best readability metrics in the first subtask of BioLaySumm 2023, with 8.924 FleschKincaid Grade Level and 9.188 DaleChall Readability Score. However, this comes at the cost of reduced relevance and factuality, emphasizing the inherent challenges of balancing readability and content preservation in lay summarization. The first stage generates summaries using a large language model, such as BART with LSG attention. The second stage uses a zero-shot sentence simplification method to improve the readability of the summaries. In the second subtask, a hybrid dataset is employed to train a model capable of generating both lay summaries and abstracts. This approach achieves the best readability score and shares the top overall rank with other leading methods. Our study underscores the importance of developing effective methods for creating accessible lay summaries while maintaining information integrity. Future work will integrate simplification and summary generation within a joint optimization framework that generates high-quality lay summaries that effectively communicate scientific content to a broader audience."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "bibtext": "@inproceedings{al-hussaini-etal-2023-pathology,\n    title = \"Pathology Dynamics at {B}io{L}ay{S}umm: the trade-off between Readability, Relevance, and Factuality in Lay Summarization\",\n    author = \"Al-Hussaini, Irfan  and\n      Wu, Austin  and\n      Mitchell, Cassie\",\n    editor = \"Demner-fushman, Dina  and\n      Ananiadou, Sophia  and\n      Cohen, Kevin\",\n    booktitle = \"The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.bionlp-1.63/\",\n    doi = \"10.18653/v1/2023.bionlp-1.63\",\n    pages = \"592--601\"\n}",
                "anthology_id": "2023.bionlp-1.63",
                "url": "https://aclanthology.org/2023.bionlp-1.63",
                "pub_year": "2023"
            }
        },
        {
            "title": "IKM_Lab at BioLaySumm Task 1: Longformer-based Prompt Tuning for Biomedical Lay Summary Generation",
            "abstract": {
                "abstract": "This paper describes the entry by the Intelligent Knowledge Management (IKM) Laboratory in the BioLaySumm 2023 task1. We aim to transform lengthy biomedical articles into concise, reader-friendly summaries that can be easily comprehended by the general public. We utilized a long-text abstractive summarization longformer model and experimented with several prompt methods for this task. Our entry placed 10th overall, but we were particularly proud to achieve a 3rd place score in the readability evaluation metric."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "bibtext": "@inproceedings{wu-etal-2023-ikm,\n    title = \"{IKM}\\_{L}ab at {B}io{L}ay{S}umm Task 1: Longformer-based Prompt Tuning for Biomedical Lay Summary Generation\",\n    author = \"Wu, Yu-Hsuan  and\n      Lin, Ying-Jia  and\n      Kao, Hung-Yu\",\n    editor = \"Demner-fushman, Dina  and\n      Ananiadou, Sophia  and\n      Cohen, Kevin\",\n    booktitle = \"The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.bionlp-1.64/\",\n    doi = \"10.18653/v1/2023.bionlp-1.64\",\n    pages = \"602--610\"\n}",
                "anthology_id": "2023.bionlp-1.64",
                "url": "https://aclanthology.org/2023.bionlp-1.64",
                "pub_year": "2023"
            }
        },
        {
            "title": "MDC at BioLaySumm Task 1: Evaluating GPT Models for Biomedical Lay Summarization",
            "abstract": {
                "abstract": "This paper presents our approach to the BioLaySumm Task 1 shared task, held at the BioNLP 2023 Workshop. The effective communication of scientific knowledge to the general public is often limited by the technical language used in research, making it difficult for non-experts to comprehend. To address this issue, lay summaries can be used to explain research findings to non-experts in an accessible form. We conduct an evaluation of autoregressive language models, both general and specialized for the biomedical domain, to generate lay summaries from biomedical research article abstracts. Our findings demonstrate that a GPT-3.5 model combined with a straightforward few-shot prompt produces lay summaries that achieve significantly relevance and factuality compared to those generated by a fine-tuned BioGPT model. However, the summaries generated by the BioGPT model exhibit better readability. Notably, our submission for the shared task achieved 1st place in the competition."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "bibtext": "@inproceedings{turbitt-etal-2023-mdc,\n    title = \"{MDC} at {B}io{L}ay{S}umm Task 1: Evaluating {GPT} Models for Biomedical Lay Summarization\",\n    author = \"Turbitt, Ois\\'\\i n  and\n      Bevan, Robert  and\n      Aboshokor, Mouhamad\",\n    editor = \"Demner-fushman, Dina  and\n      Ananiadou, Sophia  and\n      Cohen, Kevin\",\n    booktitle = \"The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.bionlp-1.65/\",\n    doi = \"10.18653/v1/2023.bionlp-1.65\",\n    pages = \"611--619\"\n}",
                "anthology_id": "2023.bionlp-1.65",
                "url": "https://aclanthology.org/2023.bionlp-1.65",
                "pub_year": "2023"
            }
        },
        {
            "title": "Simplification automatique de textes biom\u00e9dicaux en fran\u00e7ais: lorsque des donn\u00e9es pr\u00e9cises de petite taille aident (French Biomedical Text Simplification : When Small and Precise Helps )",
            "abstract": {
                "abstract": "Nous pr\u00e9sentons un r\u00e9sum\u00e9 en fran\u00e7ais et un r\u00e9sum\u00e9 en anglais de l\u2019article (Cardon & Grabar, 2020), publi\u00e9 dans les actes de la conf\u00e9rence 28th International Conference on Computational Linguistics (COLING 2020)."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "bibtext": "@inproceedings{cardon-grabar-2021-simplification,\n    title = \"Simplification automatique de textes biom\\'edicaux en fran\\c cais: lorsque des donn\\'ees pr\\'ecises de petite taille aident ({F}rench Biomedical Text Simplification : When Small and Precise Helps )\",\n    author = \"Cardon, Remi  and\n      Grabar, Natalia\",\n    editor = \"Denis, Pascal  and\n      Grabar, Natalia  and\n      Fraisse, Amel  and\n      Cardon, R\\'emi  and\n      Jacquemin, Bernard  and\n      Kergosien, Eric  and\n      Balvet, Antonio\",\n    booktitle = \"Actes de la 28e Conf\\'erence sur le Traitement Automatique des Langues Naturelles. Volume 1 : conf\\'erence principale\",\n    month = \"6\",\n    year = \"2021\",\n    address = \"Lille, France\",\n    publisher = \"ATALA\",\n    url = \"https://aclanthology.org/2021.jeptalnrecital-taln.30/\",\n    pages = \"275--277\",\n    language = \"fra\"\n}",
                "anthology_id": "2021.jeptalnrecital-taln.30",
                "url": "https://aclanthology.org/2021.jeptalnrecital-taln.30",
                "pub_year": "2021"
            }
        },
        {
            "title": "Cochrane-auto: An Aligned Dataset for the Simplification of Biomedical Abstracts",
            "abstract": {
                "abstract": "The most reliable and up-to-date information on health questions is in the biomedical literature, but inaccessible due to the complex language full of jargon. Domain specific scientific text simplification holds the promise to make this literature accessible to a lay audience. Therefore, we create Cochrane-auto: a large corpus of pairs of aligned sentences, paragraphs, and abstracts from biomedical abstracts and lay summaries. Experiments demonstrate that a plan-guided simplification system trained on Cochrane-auto is able to outperform a strong baseline trained on unaligned abstracts and lay summaries. More generally, our freely available corpus complementing Newsela-auto and Wiki-auto facilitates text simplification research beyond the sentence-level and direct lexical and grammatical revisions."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "bibtext": "@inproceedings{bakker-kamps-2024-cochrane,\n    title = \"Cochrane-auto: An Aligned Dataset for the Simplification of Biomedical Abstracts\",\n    author = \"Bakker, Jan  and\n      Kamps, Jaap\",\n    editor = \"Shardlow, Matthew  and\n      Saggion, Horacio  and\n      Alva-Manchego, Fernando  and\n      Zampieri, Marcos  and\n      North, Kai  and\n      \\v Stajner, Sanja  and\n      Stodden, Regina\",\n    booktitle = \"Proceedings of the Third Workshop on Text Simplification, Accessibility and Readability (TSAR 2024)\",\n    month = nov,\n    year = \"2024\",\n    address = \"Miami, Florida, USA\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.tsar-1.5/\",\n    doi = \"10.18653/v1/2024.tsar-1.5\",\n    pages = \"41--51\"\n}",
                "anthology_id": "2024.tsar-1.5",
                "url": "https://aclanthology.org/2024.tsar-1.5",
                "pub_year": "2024"
            }
        },
        {
            "title": "Society of Medical Simplifiers",
            "abstract": {
                "abstract": "Medical text simplification is crucial for making complex biomedical literature more accessible to non-experts. Traditional methods struggle with the specialized terms and jargon of medical texts, lacking the flexibility to adapt the simplification process dynamically. In contrast, recent advancements in large language models (LLMs) present unique opportunities by offering enhanced control over text simplification through iterative refinement and collaboration between specialized agents. In this work, we introduce the Society of Medical Simplifiers, a novel LLM-based framework inspired by the \u201cSociety of Mind\u201d (SOM) philosophy. Our approach leverages the strengths of LLMs by assigning five distinct roles, i.e., Layperson, Simplifier, Medical Expert, Language Clarifier, and Redundancy Checker, organized into interaction loops. This structure allows the agents to progressively improve text simplification while maintaining the complexity and accuracy of the original content. Evaluations on the Cochrane text simplification dataset demonstrate that our framework is on par with or outperforms state-of-the-art methods, achieving superior readability and content preservation through controlled simplification processes."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "bibtext": "@inproceedings{lyu-pergola-2024-society,\n    title = \"Society of Medical Simplifiers\",\n    author = \"Lyu, Chen  and\n      Pergola, Gabriele\",\n    editor = \"Shardlow, Matthew  and\n      Saggion, Horacio  and\n      Alva-Manchego, Fernando  and\n      Zampieri, Marcos  and\n      North, Kai  and\n      \\v Stajner, Sanja  and\n      Stodden, Regina\",\n    booktitle = \"Proceedings of the Third Workshop on Text Simplification, Accessibility and Readability (TSAR 2024)\",\n    month = nov,\n    year = \"2024\",\n    address = \"Miami, Florida, USA\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.tsar-1.7/\",\n    doi = \"10.18653/v1/2024.tsar-1.7\",\n    pages = \"61--68\"\n}",
                "anthology_id": "2024.tsar-1.7",
                "url": "https://aclanthology.org/2024.tsar-1.7",
                "pub_year": "2024"
            }
        },
        {
            "title": "SciGisPy: a Novel Metric for Biomedical Text Simplification via Gist Inference Score",
            "abstract": {
                "abstract": "Biomedical literature is often written in highly specialized language, posing significant comprehension challenges for non-experts. Automatic text simplification (ATS) offers a solution by making such texts more accessible while preserving critical information. However, evaluating ATS for biomedical texts is still challenging due to the limitations of existing evaluation metrics. General-domain metrics like SARI, BLEU, and ROUGE focus on surface-level text features, and readability metrics like FKGL and ARI fail to account for domain-specific terminology or assess how well the simplified text conveys core meanings (gist). To address this, we introduce SciGisPy, a novel evaluation metric inspired by Gist Inference Score (GIS) from Fuzzy-Trace Theory (FTT). SciGisPy measures how well a simplified text facilitates the formation of abstract inferences (gist) necessary for comprehension, especially in the biomedical domain. We revise GIS for this purpose by introducing domain-specific enhancements, including semantic chunking, Information Content (IC) theory, and specialized embeddings, while removing unsuitable indexes. Our experimental evaluation on the Cochrane biomedical text simplification dataset demonstrates that SciGisPy outperforms the original GIS formulation, with a significant increase in correctly identified simplified texts (84% versus 44.8%). The results and a thorough ablation study confirm that SciGisPy better captures the essential meaning of biomedical content, outperforming existing approaches."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "bibtext": "@inproceedings{lyu-pergola-2024-scigispy,\n    title = \"{S}ci{G}is{P}y: a Novel Metric for Biomedical Text Simplification via Gist Inference Score\",\n    author = \"Lyu, Chen  and\n      Pergola, Gabriele\",\n    editor = \"Shardlow, Matthew  and\n      Saggion, Horacio  and\n      Alva-Manchego, Fernando  and\n      Zampieri, Marcos  and\n      North, Kai  and\n      \\v Stajner, Sanja  and\n      Stodden, Regina\",\n    booktitle = \"Proceedings of the Third Workshop on Text Simplification, Accessibility and Readability (TSAR 2024)\",\n    month = nov,\n    year = \"2024\",\n    address = \"Miami, Florida, USA\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.tsar-1.10/\",\n    doi = \"10.18653/v1/2024.tsar-1.10\",\n    pages = \"95--106\"\n}",
                "anthology_id": "2024.tsar-1.10",
                "url": "https://aclanthology.org/2024.tsar-1.10",
                "pub_year": "2024"
            }
        },
        {
            "title": "Evaluating Factuality in Text Simplification",
            "abstract": {
                "abstract": "Automated simplification models aim to make input texts more readable. Such methods have the potential to make complex information accessible to a wider audience, e.g., providing access to recent medical literature which might otherwise be impenetrable for a lay reader. However, such models risk introducing errors into automatically simplified texts, for instance by inserting statements unsupported by the corresponding original text, or by omitting key information. Providing more readable but inaccurate versions of texts may in many cases be worse than providing no such access at all. The problem of factual accuracy (and the lack thereof) has received heightened attention in the context of summarization models, but the factuality of automatically simplified texts has not been investigated. We introduce a taxonomy of errors that we use to analyze both references drawn from standard simplification datasets and state-of-the-art model outputs. We find that errors often appear in both that are not captured by existing evaluation metrics, motivating a need for research into ensuring the factual accuracy of automated simplification models."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "bibtext": "@inproceedings{devaraj-etal-2022-evaluating,\n    title = \"Evaluating Factuality in Text Simplification\",\n    author = \"Devaraj, Ashwin  and\n      Sheffield, William  and\n      Wallace, Byron  and\n      Li, Junyi Jessy\",\n    editor = \"Muresan, Smaranda  and\n      Nakov, Preslav  and\n      Villavicencio, Aline\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-long.506/\",\n    doi = \"10.18653/v1/2022.acl-long.506\",\n    pages = \"7331--7345\"\n}",
                "anthology_id": "2022.acl-long.506",
                "url": "https://aclanthology.org/2022.acl-long.506",
                "pub_year": "2022"
            }
        },
        {
            "title": "Evaluation of intralingual machine translation for health communication",
            "abstract": {
                "abstract": "In this paper, we describe results of a study on evaluation of intralingual machine translation. The study focuses on machine translations of medical texts into Plain German. The automatically simplified texts were compared with manually simplified texts (i.e., simplified by human experts) as well as with the underlying, unsimplified source texts. We analyse the quality of outputs from three models based on different criteria, such as correctness, readability, and syntactic complexity. We compare the outputs of the three models under analysis between each other, as well as with the existing human translations. The study revealed that system performance depends on the evaluation criteria used and that only one of the three models showed strong similarities to the human translations. Furthermore, we identified various types of errors in all three models. These included not only grammatical mistakes and misspellings, but also incorrect explanations of technical terms and false statements, which in turn led to serious content-related mistakes."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "bibtext": "@inproceedings{deilen-etal-2024-evaluation,\n    title = \"Evaluation of intralingual machine translation for health communication\",\n    author = {Deilen, Silvana  and\n      Lapshinova-Koltunski, Ekaterina  and\n      Garrido, Sergio  and\n      H\\\"orner, Julian  and\n      Maa\\ss , Christiane  and\n      Theel, Vanessa  and\n      Ziemer, Sophie},\n    editor = \"Scarton, Carolina  and\n      Prescott, Charlotte  and\n      Bayliss, Chris  and\n      Oakley, Chris  and\n      Wright, Joanna  and\n      Wrigley, Stuart  and\n      Song, Xingyi  and\n      Gow-Smith, Edward  and\n      Bawden, Rachel  and\n      S\\'anchez-Cartagena, V\\'\\i ctor M  and\n      Cadwell, Patrick  and\n      Lapshinova-Koltunski, Ekaterina  and\n      Cabarr\\~ao, Vera  and\n      Chatzitheodorou, Konstantinos  and\n      Nurminen, Mary  and\n      Kanojia, Diptesh  and\n      Moniz, Helena\",\n    booktitle = \"Proceedings of the 25th Annual Conference of the European Association for Machine Translation (Volume 1)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Sheffield, UK\",\n    publisher = \"European Association for Machine Translation (EAMT)\",\n    url = \"https://aclanthology.org/2024.eamt-1.39/\",\n    pages = \"469--479\"\n}",
                "anthology_id": "2024.eamt-1.39",
                "url": "https://aclanthology.org/2024.eamt-1.39",
                "pub_year": "2024"
            }
        },
        {
            "title": "NapSS: Paragraph-level Medical Text Simplification via Narrative Prompting and Sentence-matching Summarization",
            "abstract": {
                "abstract": "Accessing medical literature is difficult for laypeople as the content is written for specialists and contains medical jargon. Automated text simplification methods offer a potential means to address this issue. In this work, we propose a summarize-then-simplify two-stage strategy, which we call NapSS, identifying the relevant content to simplify while ensuring that the original narrative flow is preserved. In this approach, we first generate reference summaries via sentence matching between the original and the simplified abstracts. These summaries are then used to train an extractive summarizer, learning the most relevant content to be simplified. Then, to ensure the narrative consistency of the simplified text, we synthesize auxiliary narrative prompts combining key phrases derived from the syntactical analyses of the original text. Our model achieves results significantly better than the seq2seq baseline on an English medical corpus, yielding 3% 4% absolute improvements in terms of lexical similarity, and providing a further 1.1% improvement of SARI score when combined with the baseline. We also highlight shortcomings of existing evaluation methods, and introduce new metrics that take into account both lexical and high-level semantic similarity. A human evaluation conducted on a random sample of the test set further establishes the effectiveness of the proposed approach. Codes and models are released here: https://github.com/LuJunru/NapSS."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "bibtext": "@inproceedings{lu-etal-2023-napss,\n    title = \"{N}ap{SS}: Paragraph-level Medical Text Simplification via Narrative Prompting and Sentence-matching Summarization\",\n    author = \"Lu, Junru  and\n      Li, Jiazheng  and\n      Wallace, Byron  and\n      He, Yulan  and\n      Pergola, Gabriele\",\n    editor = \"Vlachos, Andreas  and\n      Augenstein, Isabelle\",\n    booktitle = \"Findings of the Association for Computational Linguistics: EACL 2023\",\n    month = may,\n    year = \"2023\",\n    address = \"Dubrovnik, Croatia\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-eacl.80/\",\n    doi = \"10.18653/v1/2023.findings-eacl.80\",\n    pages = \"1079--1091\"\n}",
                "anthology_id": "2023.findings-eacl.80",
                "url": "https://aclanthology.org/2023.findings-eacl.80",
                "pub_year": "2023"
            }
        },
        {
            "title": "Medical Text Simplification: Optimizing for Readability with Unlikelihood Training and Reranked Beam Search Decoding",
            "abstract": {
                "abstract": "Text simplification has emerged as an increasingly useful application of AI for bridging the communication gap in specialized fields such as medicine, where the lexicon is often dominated by technical jargon and complex constructs. Despite notable progress, methods in medical simplification sometimes result in the generated text having lower quality and diversity. In this work, we explore ways to further improve the readability of text simplification in the medical domain. We propose (1) a new unlikelihood loss that encourages generation of simpler terms and (2) a reranked beam search decoding method that optimizes for simplicity, which achieve better performance on readability metrics on three datasets. This study\u2019s findings offer promising avenues for improving text simplification in the medical field."
            },
            "is_peer_reviewed": true,
            "metadata": {
                "bibtext": "@inproceedings{flores-etal-2023-medical,\n    title = \"Medical Text Simplification: Optimizing for Readability with Unlikelihood Training and Reranked Beam Search Decoding\",\n    author = \"Flores, Lorenzo Jaime  and\n      Huang, Heyuan  and\n      Shi, Kejian  and\n      Chheang, Sophie  and\n      Cohan, Arman\",\n    editor = \"Bouamor, Houda  and\n      Pino, Juan  and\n      Bali, Kalika\",\n    booktitle = \"Findings of the Association for Computational Linguistics: EMNLP 2023\",\n    month = dec,\n    year = \"2023\",\n    address = \"Singapore\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-emnlp.322/\",\n    doi = \"10.18653/v1/2023.findings-emnlp.322\",\n    pages = \"4859--4873\"\n}",
                "anthology_id": "2023.findings-emnlp.322",
                "url": "https://aclanthology.org/2023.findings-emnlp.322",
                "pub_year": "2023"
            }
        }
    ],
    "last_updated": "2024-12-22 10:14:49"
}